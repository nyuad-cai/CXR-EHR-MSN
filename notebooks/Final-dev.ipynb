{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c8fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple, Union\n",
    "from types import FunctionType\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c784be",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee17ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvStemConfig(NamedTuple):\n",
    "    out_channels: int = 64\n",
    "    kernel_size: int = 3\n",
    "    stride: int = 2\n",
    "    norm_layer: Callable[..., nn.Module] = nn.BatchNorm2d\n",
    "    activation_layer: Callable[..., nn.Module] = nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733d4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_api_usage_once(obj: Any) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Logs API usage(module and name) within an organization.\n",
    "    In a large ecosystem, it's often useful to track the PyTorch and\n",
    "    TorchVision APIs usage. This API provides the similar functionality to the\n",
    "    logging module in the Python stdlib. It can be used for debugging purpose\n",
    "    to log which methods are used and by default it is inactive, unless the user\n",
    "    manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_.\n",
    "    Please note it is triggered only once for the same API call within a process.\n",
    "    It does not collect any data from open-source users since it is no-op by default.\n",
    "    For more information, please refer to\n",
    "    * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging;\n",
    "    * Logging policy: https://github.com/pytorch/vision/issues/5052;\n",
    "\n",
    "    Args:\n",
    "        obj (class instance or method): an object to extract info from.\n",
    "    \"\"\"\n",
    "    module = obj.__module__\n",
    "    if not module.startswith(\"torchvision\"):\n",
    "        module = f\"torchvision.internal.{module}\"\n",
    "    name = obj.__class__.__name__\n",
    "    if isinstance(obj, FunctionType):\n",
    "        name = obj.__name__\n",
    "    torch._C._log_api_usage_once(f\"{module}.{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec34018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ntuple(x: Any, n: int) -> Tuple[Any, ...]:\n",
    "    \"\"\"\n",
    "    Make n-tuple from input x. If x is an iterable, then we just convert it to tuple.\n",
    "    Otherwise, we will make a tuple of length n, all with value of x.\n",
    "    reference: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/utils.py#L8\n",
    "\n",
    "    Args:\n",
    "        x (Any): input value\n",
    "        n (int): length of the resulting tuple\n",
    "    \"\"\"\n",
    "    if isinstance(x, collections.abc.Iterable):\n",
    "        return tuple(x)\n",
    "    return tuple(repeat(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bb0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_index_like(index: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Expands the index along the last dimension of the input tokens.\n",
    "\n",
    "    Args:\n",
    "        index:\n",
    "            Index tensor with shape (batch_size, idx_length) where each entry is\n",
    "            an index in [0, sequence_length).\n",
    "        tokens:\n",
    "            Tokens tensor with shape (batch_size, sequence_length, dim).\n",
    "\n",
    "    Returns:\n",
    "        Index tensor with shape (batch_size, idx_length, dim) where the original\n",
    "        indices are repeated dim times along the last dimension.\n",
    "\n",
    "    \"\"\"\n",
    "    dim = tokens.shape[-1]\n",
    "    index = index.unsqueeze(-1).expand(-1, -1, dim)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11eeb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_at_index(tokens: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Selects tokens at index.\n",
    "\n",
    "    Args:\n",
    "        tokens:\n",
    "            Token tensor with shape (batch_size, sequence_length, dim).\n",
    "        index:\n",
    "            Index tensor with shape (batch_size, index_length) where each entry is\n",
    "            an index in [0, sequence_length).\n",
    "\n",
    "    Returns:\n",
    "        Token tensor with shape (batch_size, index_length, dim) containing the\n",
    "        selected tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    index = expand_index_like(index, tokens)\n",
    "    return torch.gather(tokens, 1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc30b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNormActivation(torch.nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, ...]] = 3,\n",
    "        stride: Union[int, Tuple[int, ...]] = 1,\n",
    "        padding: Optional[Union[int, Tuple[int, ...], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        dilation: Union[int, Tuple[int, ...]] = 1,\n",
    "        inplace: Optional[bool] = True,\n",
    "        bias: Optional[bool] = None,\n",
    "        conv_layer: Callable[..., torch.nn.Module] = torch.nn.Conv2d,\n",
    "    ) -> None:\n",
    "\n",
    "        if padding is None:\n",
    "            if isinstance(kernel_size, int) and isinstance(dilation, int):\n",
    "                padding = (kernel_size - 1) // 2 * dilation\n",
    "            else:\n",
    "                _conv_dim = len(kernel_size) if isinstance(kernel_size, Sequence) else len(dilation)\n",
    "                kernel_size = make_ntuple(kernel_size, _conv_dim)\n",
    "                dilation = make_ntuple(dilation, _conv_dim)\n",
    "                padding = tuple((kernel_size[i] - 1) // 2 * dilation[i] for i in range(_conv_dim))\n",
    "        if bias is None:\n",
    "            bias = norm_layer is None\n",
    "\n",
    "        layers = [\n",
    "            conv_layer(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                dilation=dilation,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            layers.append(norm_layer(out_channels))\n",
    "\n",
    "        if activation_layer is not None:\n",
    "            params = {} if inplace is None else {\"inplace\": inplace}\n",
    "            layers.append(activation_layer(**params))\n",
    "        super().__init__(*layers)\n",
    "        log_api_usage_once(self)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if self.__class__ == ConvNormActivation:\n",
    "            warnings.warn(\n",
    "                \"Don't use ConvNormActivation directly, please use Conv2dNormActivation and Conv3dNormActivation instead.\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce7b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dNormActivation(ConvNormActivation):\n",
    "    \"\"\"\n",
    "    Configurable block used for Convolution2d-Normalization-Activation blocks.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input image\n",
    "        out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block\n",
    "        kernel_size: (int, optional): Size of the convolving kernel. Default: 3\n",
    "        stride (int, optional): Stride of the convolution. Default: 1\n",
    "        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation``\n",
    "        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        dilation (int): Spacing between kernel elements. Default: 1\n",
    "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
    "        bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "        stride: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: Optional[Union[int, Tuple[int, int], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        dilation: Union[int, Tuple[int, int]] = 1,\n",
    "        inplace: Optional[bool] = True,\n",
    "        bias: Optional[bool] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups,\n",
    "            norm_layer,\n",
    "            activation_layer,\n",
    "            dilation,\n",
    "            inplace,\n",
    "            bias,\n",
    "            torch.nn.Conv2d,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fcfa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place.\n",
    "            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = None,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "        log_api_usage_once(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe3f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d38cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaac1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        num_cls_tokens: int,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "        self.seq_length = seq_length\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                input: torch.Tensor,\n",
    "                idx_keep: Optional[torch.Tensor] = None\n",
    "               ):\n",
    "\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "\n",
    "        input = input + self._interpolate_pos_encoding(input,self.num_cls_tokens)\n",
    "        if idx_keep is not None:\n",
    "            input = get_at_index(input, idx_keep)\n",
    "        return self.ln(self.layers(self.dropout(input)))\n",
    "\n",
    "    def _interpolate_pos_encoding(self,\n",
    "                                  input: torch.Tensor,\n",
    "                                  num_cls_tokens: int = 3):\n",
    "        \"\"\"Returns the interpolated positional embedding for the given input.\n",
    "\n",
    "        This function interpolates self.pos_embedding for all tokens in the input,\n",
    "        ignoring the class token. This allows encoding variable sized images.\n",
    "\n",
    "        Args:\n",
    "            input:\n",
    "               Input tensor with shape (batch_size, num_sequences).\n",
    "            num_cls_tokens:\n",
    "               number of classification tokens prepended to the\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = input.shape[1] - num_cls_tokens\n",
    "        N = self.pos_embedding.shape[1] - num_cls_tokens\n",
    "        diff = num_cls_tokens -1\n",
    "        if npatch == N:\n",
    "            return self.pos_embedding\n",
    "\n",
    "        else:\n",
    "            npatch += diff\n",
    "            class_emb = self.pos_embedding[:, 1]\n",
    "            pos_embedding = self.pos_embedding[:, 1+diff:]\n",
    "        dim = input.shape[-1]\n",
    "        pos_embedding = nn.functional.interpolate(\n",
    "            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode=\"bicubic\",\n",
    "        )\n",
    "        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c0dca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "    # defualt settings vits16\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 3,\n",
    "        hidden_dim: int = 192,\n",
    "        mlp_dim: int = 768,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 1000,\n",
    "        num_cls_tokens: int = 3,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        log_api_usage_once(self)\n",
    "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "\n",
    "        if conv_stem_configs is not None:\n",
    "            # As per https://arxiv.org/abs/2106.14881\n",
    "            seq_proj = nn.Sequential()\n",
    "            prev_channels = 3\n",
    "            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n",
    "                seq_proj.add_module(\n",
    "                    f\"conv_bn_relu_{i}\",\n",
    "                    Conv2dNormActivation(\n",
    "                        in_channels=prev_channels,\n",
    "                        out_channels=conv_stem_layer_config.out_channels,\n",
    "                        kernel_size=conv_stem_layer_config.kernel_size,\n",
    "                        stride=conv_stem_layer_config.stride,\n",
    "                        norm_layer=conv_stem_layer_config.norm_layer,\n",
    "                        activation_layer=conv_stem_layer_config.activation_layer,\n",
    "                    ),\n",
    "                )\n",
    "                prev_channels = conv_stem_layer_config.out_channels\n",
    "            seq_proj.add_module(\n",
    "                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n",
    "            )\n",
    "            self.conv_proj: nn.Module = seq_proj\n",
    "        else:\n",
    "            self.conv_proj = nn.Conv2d(\n",
    "                in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "            )\n",
    "\n",
    "        self.seq_length = (image_size // patch_size) ** 2\n",
    "        # Add a class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, self.num_cls_tokens, hidden_dim))\n",
    "        self.seq_length += self.num_cls_tokens\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            self.seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            num_cls_tokens,\n",
    "            norm_layer)\n",
    "        #self.seq_length = seq_length #+ self.num_cls_tokens\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        #torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        #torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                branch: str = 'target',\n",
    "                idx_keep: Optional[torch.Tensor] = None):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        if branch == 'target':\n",
    "            batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "            x = torch.cat([batch_class_token, x], dim=1)\n",
    "            x = self.encoder(x)\n",
    "            return x[:,0:self.num_cls_tokens]\n",
    "        elif branch == 'anchor':\n",
    "            batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "            x = torch.cat([batch_class_token, x], dim=1)\n",
    "            x = torch.cat((x[:,:1],x[:,self.num_cls_tokens:]),dim = 1)\n",
    "            x = self.encoder(x,idx_keep=idx_keep)\n",
    "            return x[:,0]\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        # else:\n",
    "        #x = x[:, 0]\n",
    "        #x = self.heads(x)\n",
    "\n",
    "        # return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6815dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemconfig = [ConvStemConfig(out_channels = 64, kernel_size = 3 , stride = 2) for i in range(4)]\n",
    "transformer = VisionTransformer(image_size=224,\n",
    "                                patch_size=16,\n",
    "                                num_layers=12,\n",
    "                                num_heads=3,\n",
    "                                hidden_dim=192,\n",
    "                                mlp_dim=768,\n",
    "                                dropout=0.0,\n",
    "                                attention_dropout=0.0,\n",
    "                                num_cls_tokens=3,\n",
    "                                conv_stem_configs=stemconfig\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "078ad151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 192])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.randn(2,3,224,224)\n",
    "transformer(x,branch='anchor').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba721e",
   "metadata": {},
   "source": [
    "# ChexMSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c6c4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "320b0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model:nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11f6200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deactivate_requires_grad(model: nn.Module):\n",
    "    \"\"\"Deactivates the requires_grad flag for all parameters of a model.\n",
    "\n",
    "    This has the same effect as permanently executing the model within a `torch.no_grad()`\n",
    "    context. Use this method to disable gradient computation and therefore\n",
    "    training for a model.\n",
    "\n",
    "    Examples:\n",
    "        >>> backbone = resnet18()\n",
    "        >>> deactivate_requires_grad(backbone)\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53b45a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def update_momentum(model: nn.Module, model_ema: nn.Module, m: float):\n",
    "    \"\"\"Updates parameters of `model_ema` with Exponential Moving Average of `model`\n",
    "\n",
    "    Momentum encoders are a crucial component fo models such as MoCo or BYOL.\n",
    "\n",
    "    Examples:\n",
    "        >>> backbone = resnet18()\n",
    "        >>> projection_head = MoCoProjectionHead()\n",
    "        >>> backbone_momentum = copy.deepcopy(moco)\n",
    "        >>> projection_head_momentum = copy.deepcopy(projection_head)\n",
    "        >>>\n",
    "        >>> # update momentum\n",
    "        >>> update_momentum(moco, moco_momentum, m=0.999)\n",
    "        >>> update_momentum(projection_head, projection_head_momentum, m=0.999)\n",
    "    \"\"\"\n",
    "    for model_ema, model in zip(model_ema.parameters(), model.parameters()):\n",
    "        model_ema.data = model_ema.data * m + model.data * (1.0 - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03c91045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_token_mask(size: Tuple[int, int],\n",
    "                      mask_ratio: float = 0.6,\n",
    "                      mask_class_token: bool = False,\n",
    "                     ) -> torch.Tensor:\n",
    "    \"\"\"Creates random token masks.\n",
    "\n",
    "    Args:\n",
    "        size:\n",
    "            Size of the token batch for which to generate masks.\n",
    "            Should be (batch_size, sequence_length).\n",
    "        mask_ratio:\n",
    "            Percentage of tokens to mask.\n",
    "        mask_class_token:\n",
    "            If False the class token is never masked. If True the class token\n",
    "            might be masked.\n",
    "        device:\n",
    "            Device on which to create the index masks.\n",
    "\n",
    "    Returns:\n",
    "        A (index_keep, index_mask) tuple where each index is a tensor.\n",
    "        index_keep contains the indices of the unmasked tokens and has shape\n",
    "        (batch_size, num_keep). index_mask contains the indices of the masked\n",
    "        tokens and has shape (batch_size, sequence_length - num_keep).\n",
    "        num_keep is equal to sequence_length * (1- mask_ratio).\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, sequence_length = size\n",
    "    num_keep = int(sequence_length * (1 - mask_ratio))\n",
    "\n",
    "    noise = torch.rand(batch_size, sequence_length)\n",
    "    if not mask_class_token and sequence_length > 0:\n",
    "        # make sure that class token is not masked\n",
    "        noise[:, 0] = -1\n",
    "        num_keep = max(1, num_keep)\n",
    "\n",
    "    # get indices of tokens to keep\n",
    "    indices = torch.argsort(noise, dim=1)\n",
    "    idx_keep = indices[:, :num_keep]\n",
    "    idx_mask = indices[:, num_keep:]\n",
    "\n",
    "    return idx_keep, idx_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a353047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int = 768,\n",
    "                 out_features: int = 2048,\n",
    "               bias: bool = False,\n",
    "              ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_block = nn.Sequential(nn.Linear(in_features= in_features,\n",
    "                                                   out_features= out_features,\n",
    "                                               bias=bias),\n",
    "                                         nn.LayerNorm(normalized_shape= out_features),\n",
    "                                         nn.GELU()\n",
    "                                        )\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.tensor:\n",
    "        x = self.dense_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4fa07df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int = 192,\n",
    "                 hidden_features: int = 768,\n",
    "                 out_features: int = 192,\n",
    "                 bias : bool = False\n",
    "                 ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.projection_head = nn.Sequential(DenseBlock(in_features= self.in_features,\n",
    "                                                    out_features= self.hidden_features,\n",
    "                                                    bias= bias\n",
    "                                                    ),\n",
    "                                         DenseBlock(in_features= self.hidden_features,\n",
    "                                                    out_features= self.hidden_features,\n",
    "                                                    bias= bias\n",
    "                                                    ),\n",
    "                                         nn.Linear(in_features= self.hidden_features,\n",
    "                                                   out_features= self.out_features,\n",
    "                                                   bias= bias\n",
    "                                                   )\n",
    "                                         )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        x = self.projection_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35b22143",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_head = ProjectionHead(in_features=192,\n",
    "                                 hidden_features=768,\n",
    "                                 out_features=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8d262b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexMSN(nn.Module):\n",
    "    def __init__(self,\n",
    "               backbone: nn.Module,\n",
    "               projection_head: nn.Module,\n",
    "               masking_ratio: float = 0.15,\n",
    "               ema_p: float = 0.996,\n",
    "               focal: bool = True\n",
    "              ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.ema_p = ema_p\n",
    "        self.focal = focal\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = projection_head\n",
    "\n",
    "        self.target_backbone = copy.deepcopy(self.backbone)\n",
    "        self.target_projection_head = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        deactivate_requires_grad(self.target_backbone)\n",
    "        deactivate_requires_grad(self.target_projection_head)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                views:list[torch.tensor],\n",
    "                focal: bool = True\n",
    "                ) -> tuple[torch.Tensor]:\n",
    "\n",
    "        update_momentum(model= self.backbone,\n",
    "                        model_ema= self.target_backbone,\n",
    "                        m = self.ema_p,\n",
    "                       )\n",
    "        update_momentum(model= self.projection_head,\n",
    "                        model_ema= self.target_projection_head,\n",
    "                        m = self.ema_p,\n",
    "                       )\n",
    "        projections = self._forward_all(batch=views,focal=self.focal)\n",
    "        \n",
    "\n",
    "        return projections\n",
    "\n",
    "    def _target_forward(self,\n",
    "                        view: torch.tensor\n",
    "                        ) -> torch.Tensor:\n",
    "\n",
    "        target_encodings = self.target_backbone(x= view,\n",
    "                                                branch='target'\n",
    "                                                )\n",
    "        target_projections = self.target_projection_head(x= target_encodings)\n",
    "\n",
    "        return target_projections\n",
    "\n",
    "\n",
    "    def _anchor_forward(self,\n",
    "                        view: torch.tensor\n",
    "                        ) -> torch.Tensor:\n",
    "\n",
    "        batch_size, _, _, width = view.shape\n",
    "        seq_length = (width // self.backbone.patch_size) ** 2\n",
    "        idx_keep, idx_mask = random_token_mask(size= (view.shape[0],seq_length),\n",
    "                                               mask_ratio= self.masking_ratio\n",
    "                                          )\n",
    "\n",
    "        anchor_encodings = self.backbone(x= view,\n",
    "                                                branch= 'anchor',\n",
    "                                                idx_keep= idx_keep)\n",
    "        anchor_projections = self.projection_head(x= anchor_encodings)\n",
    "\n",
    "        return anchor_projections\n",
    "    \n",
    "    \n",
    "    def _forward_all(self,\n",
    "                     batch: list,\n",
    "                     focal: bool = True\n",
    "                     ) -> torch.tensor:\n",
    "        \n",
    "        target_view = batch[0][0]\n",
    "        anchor_view_sim = batch[0][1]\n",
    "        focal_views_sim = torch.concat(batch[0][2:],dim=0)\n",
    "        anchor_view_age = batch[1][1]\n",
    "        focal_views_age = torch.concat(batch[1][2:],dim=0)\n",
    "        anchor_view_gender = batch[2][1]\n",
    "        focal_views_gender = torch.concat(batch[2][2:],dim=0)\n",
    "        \n",
    "        \n",
    "        target_projections = self._target_forward(target_view)\n",
    "        \n",
    "        anchor_projections_sim = self._anchor_forward(anchor_view_sim)\n",
    "        if focal:\n",
    "            anchor_focal_projections_sim = self._anchor_forward(focal_views_sim)\n",
    "            similarity_projections = self._arrange_tokens(anchor_projections_sim,\n",
    "                                                          anchor_focal_projections_sim,\n",
    "                                                          num_focal = 10)\n",
    "                                                          \n",
    "        \n",
    "        anchor_projections_age = self._anchor_forward(anchor_view_age)\n",
    "        if focal:\n",
    "            anchor_focal_projections_age =  self._anchor_forward(focal_views_age)\n",
    "            age_projections = self._arrange_tokens(anchor_projections_age,\n",
    "                                                   anchor_focal_projections_age,\n",
    "                                                   num_focal = 10)\n",
    "                                                    \n",
    "        \n",
    "        \n",
    "        anchor_projections_gender = self._anchor_forward(anchor_view_gender)\n",
    "        if focal:\n",
    "            anchor_focal_projections_gender = self._anchor_forward(focal_views_gender)  \n",
    "            gender_projections = self._arrange_tokens(anchor_projections_gender,\n",
    "                                                      anchor_focal_projections_gender,\n",
    "                                                      num_focal = 10)\n",
    "                                                          \n",
    "        if focal:\n",
    "            anchor_projections = torch.stack((similarity_projections,\n",
    "                                              age_projections,\n",
    "                                              gender_projections\n",
    "                                              ),\n",
    "                                         dim= 0)\n",
    "        else:\n",
    "            anchor_projections = torch.stack((anchor_projections_sim,\n",
    "                                              anchor_projections_age,\n",
    "                                              anchor_projections_gender\n",
    "                                              ),\n",
    "                                             dim= 1)       \n",
    "\n",
    "        return (anchor_projections,\n",
    "                target_projections)\n",
    "    \n",
    "    def _arrange_tokens(self,\n",
    "                        tensor1: torch.tensor,\n",
    "                        tensor2:torch.tensor,\n",
    "                        num_focal: int = 10\n",
    "                        ) ->torch.tensor:\n",
    "\n",
    "        a = torch.stack(torch.split(tensor1,1),0)\n",
    "        b = torch.stack(torch.split(tensor2,num_focal),0)\n",
    "        c = torch.cat((a.expand(-1,num_focal,-1),b),dim=1)[:,num_focal-1:]\n",
    "        arranged_tokens = torch.cat(c.split(1),1).squeeze(0)\n",
    "        return arranged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8aa1e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "msn = ChexMSN(backbone=transformer,\n",
    "             projection_head=projection_head,\n",
    "             masking_ratio=0.15,\n",
    "             ema_p=0.996,\n",
    "             focal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d43943ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# msn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ea49e",
   "metadata": {},
   "source": [
    "# MSN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "12ac5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5cb83dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototype_probabilities(queries: torch.tensor,\n",
    "                            prototypes: torch.tensor,\n",
    "                            temperature: float,\n",
    "                            ) -> torch.tensor:\n",
    "    \"\"\"Returns probability for each query to belong to each prototype.\n",
    "\n",
    "    Args:\n",
    "        queries:\n",
    "            Tensor with shape (batch_size, dim), projection head output\n",
    "        prototypes:\n",
    "            Tensor with shape (num_prototypes, dim)\n",
    "        temperature:\n",
    "            Inverse scaling factor for the similarity.\n",
    "\n",
    "    Returns:\n",
    "        Probability tensor with shape (batch_size, num_prototypes) which sums to 1 along\n",
    "        the num_prototypes dimension.\n",
    "\n",
    "    \"\"\"                           \n",
    "    return F.softmax(torch.matmul(queries, prototypes.T) / temperature, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "94abce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(probabilities: torch.tensor, \n",
    "            temperature: float\n",
    "           ) -> torch.tensor:\n",
    "    \"\"\"Sharpens the probabilities with the given temperature.\n",
    "\n",
    "    Args:\n",
    "        probabilities:\n",
    "            Tensor with shape (batch_size, dim)\n",
    "        temperature:\n",
    "            Temperature in (0, 1]. Lower temperature results in stronger sharpening (\n",
    "            output probabilities are less uniform).\n",
    "    Returns:\n",
    "        Probabilities tensor with shape (batch_size, num_prototypes).\n",
    "\n",
    "    \"\"\"\n",
    "    probabilities = probabilities ** (1.0 / temperature)\n",
    "    probabilities /= torch.sum(probabilities, dim=1, keepdim=True)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0e201450",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sinkhorn(probabilities: torch.tensor,\n",
    "             iterations: int = 3,\n",
    "             gather_distributed: bool = False,\n",
    "            ) -> torch.tensor:\n",
    "    \"\"\"Runs sinkhorn normalization on the probabilities as described in [0].\n",
    "\n",
    "    Code inspired by [1].\n",
    "\n",
    "    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141\n",
    "    - [1]: https://github.com/facebookresearch/msn\n",
    "\n",
    "    Args:\n",
    "        probabilities:\n",
    "            Probabilities tensor with shape (batch_size, num_prototypes).\n",
    "        iterations:\n",
    "            Number of iterations of the sinkhorn algorithms. Set to 0 to disable.\n",
    "        gather_distributed:\n",
    "            If True then features from all gpus are gathered during normalization.\n",
    "    Returns:\n",
    "        A normalized probabilities tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    if iterations <= 0:\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    num_targets, num_prototypes = probabilities.shape\n",
    "    probabilities = probabilities.T\n",
    "    sum_probabilities = torch.sum(probabilities)\n",
    "\n",
    "    probabilities = probabilities / sum_probabilities\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # normalize rows\n",
    "        row_sum = torch.sum(probabilities, dim=1, keepdim=True)\n",
    "\n",
    "        probabilities /= row_sum\n",
    "        probabilities /= num_prototypes\n",
    "\n",
    "        # normalize columns\n",
    "        probabilities /= torch.sum(probabilities, dim=0, keepdim=True)\n",
    "        probabilities /= num_targets\n",
    "\n",
    "    probabilities *= num_targets\n",
    "    return probabilities.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6121ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_loss(mean_anchor_probs: torch.tensor\n",
    "                       ) -> torch.tensor:\n",
    "    \"\"\"Calculates mean entropy regularization loss.\"\"\"\n",
    "    loss = -torch.sum(torch.log(mean_anchor_probs ** (-mean_anchor_probs)))\n",
    "    loss += math.log(float(len(mean_anchor_probs)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ea816eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 temperature: float = 0.1,\n",
    "                 sinkhorn_iterations: int = 3,\n",
    "                 similarity_weight: float = 1.0,\n",
    "                 age_weight: float = 1.0,\n",
    "                 gender_weight: float = 1.0,\n",
    "                 regularization_weight: float = 1.0,\n",
    "               ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.sinkhorn_iterations = sinkhorn_iterations\n",
    "        self.similarity_weight = similarity_weight\n",
    "        self.age_weight = age_weight\n",
    "        self.gender_weight = gender_weight\n",
    "        self.regularization_weight = regularization_weight\n",
    "    \n",
    "    \n",
    "    def forward(self,\n",
    "                anchors: torch.tensor,\n",
    "                targets: torch.tensor,\n",
    "                prototypes: torch.tensor,\n",
    "                target_sharpen_temperature: float = 0.25,\n",
    "                focal: bool = True\n",
    "                      ) -> torch.tensor:\n",
    "        \n",
    "        similarity_loss = self.similarity_weight * self._forward_loss(anchors=anchors[0] if focal else anchors[:,0],\n",
    "                                                                      targets=targets[:,0],\n",
    "                                                                      prototypes=prototypes[0].weight\n",
    "                                                                     )\n",
    "        age_loss = self.age_weight * self._forward_loss(anchors=anchors[1] if focal else anchors[:,1],\n",
    "                                                           targets=targets[:,1],\n",
    "                                                           prototypes=prototypes[1].weight\n",
    "                                                          )\n",
    "        gender_loss = self.gender_weight * self._forward_loss(anchors=anchors[2] if focal else anchors[:,2],\n",
    "                                                              targets=targets[:,2],\n",
    "                                                              prototypes=prototypes[2].weight\n",
    "                                                             )\n",
    "        \n",
    "        loss = similarity_loss + gender_loss + gender_loss\n",
    "        return loss\n",
    "    \n",
    "    def _forward_loss(self,\n",
    "                anchors: torch.tensor,\n",
    "                targets: torch.tensor,\n",
    "                prototypes: torch.tensor,\n",
    "                target_sharpen_temperature: float = 0.25,\n",
    "               ) -> torch.tensor:\n",
    "\n",
    "        num_views = anchors.shape[0] // targets.shape[0]\n",
    "        anchors = F.normalize(anchors, dim=1)\n",
    "        targets = F.normalize(targets, dim=1)\n",
    "        prototypes = F.normalize(prototypes, dim=1)\n",
    "\n",
    "        anchor_probs = prototype_probabilities(anchors, \n",
    "                                               prototypes, \n",
    "                                               temperature=self.temperature\n",
    "                                              )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_probs = prototype_probabilities(targets, \n",
    "                                                   prototypes, \n",
    "                                                   temperature=self.temperature\n",
    "                                                   )\n",
    "            target_probs = sharpen(target_probs, temperature=target_sharpen_temperature)\n",
    "            if self.sinkhorn_iterations > 0:\n",
    "                target_probs = sinkhorn(probabilities=target_probs,\n",
    "                                        iterations=self.sinkhorn_iterations,\n",
    "                                        )\n",
    "            target_probs = torch.repeat_interleave(target_probs, repeats=num_views, dim=0)\n",
    "\n",
    "        loss = torch.mean(torch.sum(torch.log(anchor_probs ** (-target_probs)), dim=1))\n",
    "\n",
    "        # regularization loss\n",
    "        if self.regularization_weight > 0:\n",
    "            mean_anchor_probs = torch.mean(anchor_probs, dim=0)\n",
    "            reg_loss = regularization_loss(mean_anchor_probs=mean_anchor_probs)\n",
    "            loss += self.regularization_weight * reg_loss\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "34ccd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterterion = MSNLoss(temperature=0.1,\n",
    "                       sinkhorn_iterations=3,\n",
    "                       similarity_weight=1.0,\n",
    "                       age_weight=1.0,\n",
    "                       gender_weight=1.0,\n",
    "                       regularization_weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2203ee",
   "metadata": {},
   "source": [
    "# MSN transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "33a8d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL.Image import Image\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f0d354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNTransform(MultiViewTransform):\n",
    "    \"\"\"Implements the transformations for MSN [0].\n",
    "\n",
    "    Input to this transform:\n",
    "        PIL Image or Tensor.\n",
    "\n",
    "    Output of this transform:\n",
    "        List of Tensor of length 2 * random_views + focal_views. (12 by default)\n",
    "\n",
    "    Applies the following augmentations by default:\n",
    "        - Random resized crop\n",
    "        - Random horizontal flip\n",
    "        - ImageNet normalization\n",
    "\n",
    "    Generates a set of random and focal views for each input image. The generated output\n",
    "    is (views, target, filenames) where views is list with the following entries:\n",
    "    [random_views_0, random_views_1, ..., focal_views_0, focal_views_1, ...].\n",
    "\n",
    "    - [0]: Masked Siamese Networks, 2022: https://arxiv.org/abs/2204.07141\n",
    "\n",
    "    Attributes:\n",
    "        random_size:\n",
    "            Size of the random image views in pixels.\n",
    "        focal_size:\n",
    "            Size of the focal image views in pixels.\n",
    "        random_views:\n",
    "            Number of random views to generate.\n",
    "        focal_views:\n",
    "            Number of focal views to generate.\n",
    "        random_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops for the relative to random_size.\n",
    "        focal_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops relative to focal_size.\n",
    "        hf_prob:\n",
    "            Probability that horizontal flip is applied.\n",
    "        vf_prob:\n",
    "            Probability that vertical flip is applied.\n",
    "        normalize:\n",
    "            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_size: int = 224,\n",
    "        focal_size: int = 96,\n",
    "        random_views: int = 2,\n",
    "        focal_views: int = 10,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        random_crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        focal_crop_scale: Tuple[float, float] = (0.05, 0.3),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.0,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,\n",
    "    ):\n",
    "        random_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=random_size,\n",
    "            crop_scale=random_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        focal_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=focal_size,\n",
    "            crop_scale=focal_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        transforms = [random_view_transform] * random_views\n",
    "        transforms += [focal_view_transform] * focal_views\n",
    "        super().__init__(transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7c6ed8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNViewTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        crop_size: int = 224,\n",
    "        crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.2,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,\n",
    "    ):\n",
    "\n",
    "        transform = [\n",
    "            T.RandomAffine(degrees=affine_dgrees, \n",
    "                           scale=affine_scale, \n",
    "                           shear=affine_shear, \n",
    "                           translate=affine_translate),\n",
    "            T.RandomResizedCrop(size=crop_size, scale=crop_scale),\n",
    "            T.RandomHorizontalFlip(p=hf_prob),\n",
    "            T.RandomVerticalFlip(p=vf_prob),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=normalize[\"mean\"], std=normalize[\"std\"]),\n",
    "        ]\n",
    "\n",
    "        self.transform = T.Compose(transform)\n",
    "\n",
    "    def __call__(self, image: Union[Tensor, Image]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Applies the transforms to the input image.\n",
    "\n",
    "        Args:\n",
    "            image:\n",
    "                The input image to apply the transforms to.\n",
    "\n",
    "        Returns:\n",
    "            The transformed image.\n",
    "\n",
    "        \"\"\"\n",
    "        transformed: Tensor = self.transform(image)\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4064df",
   "metadata": {},
   "source": [
    "# MSN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a3690144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import Tuple, Optional\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightly.transforms import MSNTransform\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7156ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexMSNDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str,\n",
    "                 transforms: nn.Module,\n",
    "                 same = True\n",
    "                 ) -> None:\n",
    "      \n",
    "        self.meta = pd.read_csv(data_dir)\n",
    "        self.all_images = list(self.meta.path)\n",
    "        self.transform = transforms\n",
    "        self.same = same\n",
    "        \n",
    "    def __len__(self\n",
    "                ) -> int:\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self,\n",
    "                    index: int\n",
    "                    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        \n",
    "        target_path = self.all_images[index]\n",
    "        image_id = target_path.split('/')[-1][:-4]\n",
    "        img_age_path, img_gender_path = self._retrieve_anchors(image_id=image_id,\n",
    "                                                               meta = self.meta,\n",
    "                                                               same=self.same)\n",
    "\n",
    "        img_target = Image.open(fp=target_path).convert('RGB')\n",
    "        img_target = self.transform(img_target)\n",
    "        \n",
    "        img_age = Image.open(fp=img_age_path).convert('RGB')\n",
    "        img_age = self.transform(img_age)\n",
    "\n",
    "        img_gender = Image.open(fp=img_gender_path).convert('RGB')\n",
    "        img_gender = self.transform(img_gender)\n",
    "\n",
    "        return (img_target,img_age,img_gender)\n",
    "    \n",
    "    \n",
    "    def _retrieve_anchors(self,\n",
    "                          image_id: str,\n",
    "                          meta: pd.DataFrame,\n",
    "                          same: bool = False) -> Tuple[str]:\n",
    "        record = meta[meta.dicom_id == image_id]\n",
    "    \n",
    "        subject_id = list(record.subject_id)[0]\n",
    "        age_groub =list(record.ageR5)[0] \n",
    "        gender = list(record.gender)[0]\n",
    "    \n",
    "        group = meta[meta.ageR5 == age_groub]\n",
    "    \n",
    "        if same:\n",
    "            candidate_anchors = group[group.gender == gender]\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.subject_id != subject_id]\n",
    "            images= list(candidate_anchors.path)\n",
    "            sampled_images = random.sample(images,k=2)\n",
    "            image_age, image_gender = sampled_images[0],sampled_images[1]\n",
    "            return image_age, image_gender\n",
    "        else:\n",
    "            candidate_anchors = group\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.subject_id != subject_id]\n",
    "            images= list(candidate_anchors.path)\n",
    "            image_age = random.sample(images,k=1)[0]\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.gender == gender]\n",
    "            images= list(candidate_anchors.path)\n",
    "            image_gender = random.sample(images,k=1)[0]\n",
    "            return image_age, image_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d485f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = MSNTransform()\n",
    "dataset = ChexMSNDataset(data_dir='../data/meta.csv',transforms= transforms,same=True)\n",
    "dataloder = DataLoader(dataset=dataset,batch_size=2,num_workers=8,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2570c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexMSNModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 criterion: nn.Module,\n",
    "                 num_prototypes: int = 1024,\n",
    "                 learning_rate: float =  1e-3,\n",
    "                 weight_decay: float = 0.0,\n",
    "                 max_epochs: int = 50,\n",
    "\n",
    "\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.num_prototypes = num_prototypes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.in_prototypes = self.model.projection_head.out_features\n",
    "        self.prototypes = nn.ModuleList([nn.Linear(in_features=self.in_prototypes,out_features=num_prototypes),\n",
    "                                         nn.Linear(in_features=self.in_prototypes,out_features=num_prototypes),\n",
    "                                         nn.Linear(in_features=self.in_prototypes,out_features=num_prototypes)])\n",
    "    \n",
    "    \n",
    "\n",
    "    def training_step(self, \n",
    "                      batch: List[Tensor], \n",
    "                      batch_idx: int\n",
    "                     ) -> float:\n",
    "        \n",
    "\n",
    "        anchors, target = self.model(batch) \n",
    "        loss = self.criterion(anchors,target,self.prototypes)\n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=True , logger=True)      \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        params = [\n",
    "            *list(self.model.backbone.parameters()),\n",
    "            *list(self.model.projection_head.parameters()),\n",
    "            self.prototypes,\n",
    "        ]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(params, \n",
    "                                      lr=self.learning_rate,\n",
    "                                      weight_decay=self.weight_decay)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                               eta_min=0.00001,\n",
    "                                                               T_max=self.max_epochs)\n",
    "        return {'optimizer': optimizer,\n",
    "               'lr_scheduler': scheduler\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7a4fa66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.1774, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChexMSNModel(msn,criterterion)\n",
    "model.training_step(next(iter(dataloder)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f8dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chexmsn-env\n",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
