{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer implementation"
      ],
      "metadata": {
        "id": "xREDrfXYsQ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
        "from torchvision.ops import *\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Sv_KTqnFsQfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvStemConfig(NamedTuple):\n",
        "    out_channels: int = 64\n",
        "    kernel_size: int = 3\n",
        "    stride: int = 2\n",
        "    norm_layer: Callable[..., nn.Module] = nn.BatchNorm2d\n",
        "    activation_layer: Callable[..., nn.Module] = nn.ReLU"
      ],
      "metadata": {
        "id": "31q37yTHbxE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_index_like(index: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Expands the index along the last dimension of the input tokens.\n",
        "\n",
        "    Args:\n",
        "        index:\n",
        "            Index tensor with shape (batch_size, idx_length) where each entry is\n",
        "            an index in [0, sequence_length).\n",
        "        tokens:\n",
        "            Tokens tensor with shape (batch_size, sequence_length, dim).\n",
        "\n",
        "    Returns:\n",
        "        Index tensor with shape (batch_size, idx_length, dim) where the original\n",
        "        indices are repeated dim times along the last dimension.\n",
        "\n",
        "    \"\"\"\n",
        "    dim = tokens.shape[-1]\n",
        "    index = index.unsqueeze(-1).expand(-1, -1, dim)\n",
        "    return index"
      ],
      "metadata": {
        "id": "S8P8AHvJf-wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_at_index(tokens: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Selects tokens at index.\n",
        "\n",
        "    Args:\n",
        "        tokens:\n",
        "            Token tensor with shape (batch_size, sequence_length, dim).\n",
        "        index:\n",
        "            Index tensor with shape (batch_size, index_length) where each entry is\n",
        "            an index in [0, sequence_length).\n",
        "\n",
        "    Returns:\n",
        "        Token tensor with shape (batch_size, index_length, dim) containing the\n",
        "        selected tokens.\n",
        "\n",
        "    \"\"\"\n",
        "    index = expand_index_like(index, tokens)\n",
        "    return torch.gather(tokens, 1, index)"
      ],
      "metadata": {
        "id": "yhPIIxm3fsP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(MLP):\n",
        "    \"\"\"Transformer MLP block.\"\"\"\n",
        "\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
        "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, std=1e-6)\n",
        "\n",
        "    def _load_from_state_dict(\n",
        "        self,\n",
        "        state_dict,\n",
        "        prefix,\n",
        "        local_metadata,\n",
        "        strict,\n",
        "        missing_keys,\n",
        "        unexpected_keys,\n",
        "        error_msgs,\n",
        "    ):\n",
        "        version = local_metadata.get(\"version\", None)\n",
        "\n",
        "        if version is None or version < 2:\n",
        "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
        "            for i in range(2):\n",
        "                for type in [\"weight\", \"bias\"]:\n",
        "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
        "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
        "                    if old_key in state_dict:\n",
        "                        state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "        super()._load_from_state_dict(\n",
        "            state_dict,\n",
        "            prefix,\n",
        "            local_metadata,\n",
        "            strict,\n",
        "            missing_keys,\n",
        "            unexpected_keys,\n",
        "            error_msgs,\n",
        "        )"
      ],
      "metadata": {
        "id": "Kawb7bqFbydU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads: int,\n",
        "        hidden_dim: int,\n",
        "        mlp_dim: int,\n",
        "        dropout: float,\n",
        "        attention_dropout: float,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Attention block\n",
        "        self.ln_1 = norm_layer(hidden_dim)\n",
        "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # MLP block\n",
        "        self.ln_2 = norm_layer(hidden_dim)\n",
        "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
        "        x = self.ln_1(input)\n",
        "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
        "        x = self.dropout(x)\n",
        "        x = x + input\n",
        "\n",
        "        y = self.ln_2(x)\n",
        "        y = self.mlp(y)\n",
        "        return x + y"
      ],
      "metadata": {
        "id": "jag1PLOzb3_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_length: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        hidden_dim: int,\n",
        "        mlp_dim: int,\n",
        "        dropout: float,\n",
        "        attention_dropout: float,\n",
        "        num_cls_tokens: int,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_cls_tokens = num_cls_tokens\n",
        "        self.seq_length = seq_length\n",
        "        # Note that batch_size is on the first dim because\n",
        "        # we have batch_first=True in nn.MultiAttention() by default\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
        "        for i in range(num_layers):\n",
        "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
        "                num_heads,\n",
        "                hidden_dim,\n",
        "                mlp_dim,\n",
        "                dropout,\n",
        "                attention_dropout,\n",
        "                norm_layer,\n",
        "            )\n",
        "        self.layers = nn.Sequential(layers)\n",
        "        self.ln = norm_layer(hidden_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                input: torch.Tensor,\n",
        "                idx_keep: Optional[torch.Tensor] = None\n",
        "               ):\n",
        "\n",
        "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
        "\n",
        "        input = input + self._interpolate_pos_encoding(input,self.num_cls_tokens)\n",
        "        if idx_keep is not None:\n",
        "            input = utils.get_at_index(input, idx_keep)\n",
        "        return self.ln(self.layers(self.dropout(input)))\n",
        "\n",
        "    def _interpolate_pos_encoding(self,\n",
        "                                  input: torch.Tensor,\n",
        "                                  num_cls_tokens: int = 3):\n",
        "        \"\"\"Returns the interpolated positional embedding for the given input.\n",
        "\n",
        "        This function interpolates self.pos_embedding for all tokens in the input,\n",
        "        ignoring the class token. This allows encoding variable sized images.\n",
        "\n",
        "        Args:\n",
        "            input:\n",
        "               Input tensor with shape (batch_size, num_sequences).\n",
        "            num_cls_tokens:\n",
        "               number of classification tokens prepended to the\n",
        "        \"\"\"\n",
        "\n",
        "        npatch = input.shape[1] - num_cls_tokens\n",
        "        N = self.pos_embedding.shape[1] - num_cls_tokens\n",
        "        diff = num_cls_tokens -1\n",
        "        if npatch == N:\n",
        "          return self.pos_embedding\n",
        "\n",
        "        else:\n",
        "          npatch += diff\n",
        "          class_emb = self.pos_embedding[:, 1]\n",
        "          pos_embedding = self.pos_embedding[:, 1+diff:]\n",
        "        dim = input.shape[-1]\n",
        "        pos_embedding = nn.functional.interpolate(\n",
        "            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=math.sqrt(npatch / N),\n",
        "            mode=\"bicubic\",\n",
        "        )\n",
        "        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)"
      ],
      "metadata": {
        "id": "vXSrz21eb-D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: int = 224,\n",
        "        patch_size: int = 16,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 4,\n",
        "        hidden_dim: int = 512,\n",
        "        mlp_dim: int = 512,\n",
        "        dropout: float = 0.0,\n",
        "        attention_dropout: float = 0.0,\n",
        "        num_classes: int = 1000,\n",
        "        num_cls_tokens: int = 3,\n",
        "        representation_size: Optional[int] = None,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.dropout = dropout\n",
        "        self.num_classes = num_classes\n",
        "        self.representation_size = representation_size\n",
        "        self.norm_layer = norm_layer\n",
        "        self.num_cls_tokens = num_cls_tokens\n",
        "\n",
        "        if conv_stem_configs is not None:\n",
        "            # As per https://arxiv.org/abs/2106.14881\n",
        "            seq_proj = nn.Sequential()\n",
        "            prev_channels = 3\n",
        "            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n",
        "                seq_proj.add_module(\n",
        "                    f\"conv_bn_relu_{i}\",\n",
        "                    Conv2dNormActivation(\n",
        "                        in_channels=prev_channels,\n",
        "                        out_channels=conv_stem_layer_config.out_channels,\n",
        "                        kernel_size=conv_stem_layer_config.kernel_size,\n",
        "                        stride=conv_stem_layer_config.stride,\n",
        "                        norm_layer=conv_stem_layer_config.norm_layer,\n",
        "                        activation_layer=conv_stem_layer_config.activation_layer,\n",
        "                    ),\n",
        "                )\n",
        "                prev_channels = conv_stem_layer_config.out_channels\n",
        "            seq_proj.add_module(\n",
        "                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n",
        "            )\n",
        "            self.conv_proj: nn.Module = seq_proj\n",
        "        else:\n",
        "            self.conv_proj = nn.Conv2d(\n",
        "                in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
        "            )\n",
        "\n",
        "        self.seq_length = (image_size // patch_size) ** 2\n",
        "        # Add a class token\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, self.num_cls_tokens, hidden_dim))\n",
        "        self.seq_length += self.num_cls_tokens\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            self.seq_length,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            hidden_dim,\n",
        "            mlp_dim,\n",
        "            dropout,\n",
        "            attention_dropout,\n",
        "            num_cls_tokens,\n",
        "            norm_layer)\n",
        "        #self.seq_length = seq_length #+ self.num_cls_tokens\n",
        "\n",
        "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
        "        if representation_size is None:\n",
        "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
        "        else:\n",
        "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
        "            heads_layers[\"act\"] = nn.Tanh()\n",
        "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
        "\n",
        "        self.heads = nn.Sequential(heads_layers)\n",
        "\n",
        "        if isinstance(self.conv_proj, nn.Conv2d):\n",
        "            # Init the patchify stem\n",
        "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
        "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
        "            if self.conv_proj.bias is not None:\n",
        "                nn.init.zeros_(self.conv_proj.bias)\n",
        "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
        "            # Init the last 1x1 conv of the conv stem\n",
        "            nn.init.normal_(\n",
        "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
        "            )\n",
        "            if self.conv_proj.conv_last.bias is not None:\n",
        "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
        "\n",
        "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
        "            fan_in = self.heads.pre_logits.in_features\n",
        "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
        "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
        "\n",
        "        if isinstance(self.heads.head, nn.Linear):\n",
        "            nn.init.zeros_(self.heads.head.weight)\n",
        "            nn.init.zeros_(self.heads.head.bias)\n",
        "\n",
        "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        n, c, h, w = x.shape\n",
        "        p = self.patch_size\n",
        "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
        "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
        "        n_h = h // p\n",
        "        n_w = w // p\n",
        "\n",
        "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
        "        x = self.conv_proj(x)\n",
        "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
        "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
        "\n",
        "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
        "        # The self attention layer expects inputs in the format (N, S, E)\n",
        "        # where S is the source sequence length, N is the batch size, E is the\n",
        "        # embedding dimension\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor,\n",
        "                branch: str = 'target',\n",
        "                idx_keep: Optional[torch.Tensor] = None):\n",
        "        # Reshape and permute the input tensor\n",
        "        x = self._process_input(x)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        # Expand the class token to the full batch\n",
        "        if branch == 'target':\n",
        "          batch_class_token = self.class_token.expand(n, -1, -1)\n",
        "          x = torch.cat([batch_class_token, x], dim=1)\n",
        "          x = self.encoder(x)\n",
        "          return x[:,0:self.num_cls_tokens]\n",
        "        elif branch == 'anchor':\n",
        "          batch_class_token = self.class_token.expand(n, -1, -1)\n",
        "          x = torch.cat([batch_class_token, x], dim=1)\n",
        "          x = torch.cat((x[:,:1],x[:,self.num_cls_tokens:]),dim = 1)\n",
        "          x = self.encoder(x,idx_keep=idx_keep)\n",
        "          return x[:,0]\n",
        "\n",
        "        # Classifier \"token\" as used by standard language architectures\n",
        "        # else:\n",
        "        #x = x[:, 0]\n",
        "        #x = self.heads(x)\n",
        "\n",
        "        # return x\n",
        "# stemconfig = [ConvStemConfig(out_channels = 64, kernel_size = 3 , stride = 2) for i in range(4)]"
      ],
      "metadata": {
        "id": "5d2kHQDAadf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemconfig = [ConvStemConfig(out_channels = 64, kernel_size = 3 , stride = 2) for i in range(4)]\n",
        "\n",
        "keep,mask = utils.random_token_mask((2,197),mask_ratio=0.15)\n",
        "backbone = VisionTransformer(conv_stem_configs=stemconfig)\n"
      ],
      "metadata": {
        "id": "MrtSLM5XX08w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MSN implementation"
      ],
      "metadata": {
        "id": "Q97CMPsBRCOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install lightly"
      ],
      "metadata": {
        "id": "wNr1NT8n2m40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from lightly.models import utils\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "bpb07aKp2kAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "U9LhQG-8Y_QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deactivate_requires_grad(model: nn.Module):\n",
        "    \"\"\"Deactivates the requires_grad flag for all parameters of a model.\n",
        "\n",
        "    This has the same effect as permanently executing the model within a `torch.no_grad()`\n",
        "    context. Use this method to disable gradient computation and therefore\n",
        "    training for a model.\n",
        "\n",
        "    Examples:\n",
        "        >>> backbone = resnet18()\n",
        "        >>> deactivate_requires_grad(backbone)\n",
        "    \"\"\"\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "R-7dmv136JjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def update_momentum(model: nn.Module, model_ema: nn.Module, m: float):\n",
        "    \"\"\"Updates parameters of `model_ema` with Exponential Moving Average of `model`\n",
        "\n",
        "    Momentum encoders are a crucial component fo models such as MoCo or BYOL.\n",
        "\n",
        "    Examples:\n",
        "        >>> backbone = resnet18()\n",
        "        >>> projection_head = MoCoProjectionHead()\n",
        "        >>> backbone_momentum = copy.deepcopy(moco)\n",
        "        >>> projection_head_momentum = copy.deepcopy(projection_head)\n",
        "        >>>\n",
        "        >>> # update momentum\n",
        "        >>> update_momentum(moco, moco_momentum, m=0.999)\n",
        "        >>> update_momentum(projection_head, projection_head_momentum, m=0.999)\n",
        "    \"\"\"\n",
        "    for model_ema, model in zip(model_ema.parameters(), model.parameters()):\n",
        "        model_ema.data = model_ema.data * m + model.data * (1.0 - m)"
      ],
      "metadata": {
        "id": "q7dZ8GkR6ygF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_token_mask(size: Tuple[int, int],\n",
        "                      mask_ratio: float = 0.6,\n",
        "                      mask_class_token: bool = False,\n",
        "                     ) -> torch.Tensor:\n",
        "    \"\"\"Creates random token masks.\n",
        "\n",
        "    Args:\n",
        "        size:\n",
        "            Size of the token batch for which to generate masks.\n",
        "            Should be (batch_size, sequence_length).\n",
        "        mask_ratio:\n",
        "            Percentage of tokens to mask.\n",
        "        mask_class_token:\n",
        "            If False the class token is never masked. If True the class token\n",
        "            might be masked.\n",
        "        device:\n",
        "            Device on which to create the index masks.\n",
        "\n",
        "    Returns:\n",
        "        A (index_keep, index_mask) tuple where each index is a tensor.\n",
        "        index_keep contains the indices of the unmasked tokens and has shape\n",
        "        (batch_size, num_keep). index_mask contains the indices of the masked\n",
        "        tokens and has shape (batch_size, sequence_length - num_keep).\n",
        "        num_keep is equal to sequence_length * (1- mask_ratio).\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size, sequence_length = size\n",
        "    num_keep = int(sequence_length * (1 - mask_ratio))\n",
        "\n",
        "    noise = torch.rand(batch_size, sequence_length)\n",
        "    if not mask_class_token and sequence_length > 0:\n",
        "        # make sure that class token is not masked\n",
        "        noise[:, 0] = -1\n",
        "        num_keep = max(1, num_keep)\n",
        "\n",
        "    # get indices of tokens to keep\n",
        "    indices = torch.argsort(noise, dim=1)\n",
        "    idx_keep = indices[:, :num_keep]\n",
        "    idx_mask = indices[:, num_keep:]\n",
        "\n",
        "    return idx_keep, idx_mask"
      ],
      "metadata": {
        "id": "RFvM_-Jyo3_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_features: int = 768,\n",
        "               out_features: int = 2048,\n",
        "               bias: bool = False,\n",
        "              ) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense_block = nn.Sequential(nn.Linear(in_features= in_features,\n",
        "                                               out_features= out_features,\n",
        "                                               bias=bias),\n",
        "                                     nn.LayerNorm(normalized_shape= out_features),\n",
        "                                     nn.GELU()\n",
        "                                    )\n",
        "  def forward(self,\n",
        "              x: torch.Tensor\n",
        "              ) -> torch.tensor:\n",
        "\n",
        "    x = self.dense_block(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "HuPHXqOPRJMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.rand(2,2,768)\n",
        "DenseBlock(768,768*2)(y).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxHs3WwQRllw",
        "outputId": "004d5fdf-e932-4f78-f2f2-d178b47846fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_features: int = 512,\n",
        "               hidden_features: int = 2048,\n",
        "               out_features: int = 512,\n",
        "               bias : bool = False\n",
        "               ) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.projection_head = nn.Sequential(DenseBlock(in_features= in_features,\n",
        "                                                    out_features= hidden_features,\n",
        "                                                    bias= bias\n",
        "                                                    ),\n",
        "                                         DenseBlock(in_features= hidden_features,\n",
        "                                                    out_features= hidden_features,\n",
        "                                                    bias= bias\n",
        "                                                    ),\n",
        "                                         nn.Linear(in_features= hidden_features,\n",
        "                                                   out_features= out_features,\n",
        "                                                   bias= bias\n",
        "                                                   )\n",
        "                                         )\n",
        "\n",
        "  def forward(self,\n",
        "              x: torch.Tensor\n",
        "              ) -> torch.Tensor:\n",
        "    x = self.projection_head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "azM2ph3hRtfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2,3,224,224)"
      ],
      "metadata": {
        "id": "HqT_GdtHSoAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = VisionTransformer()"
      ],
      "metadata": {
        "id": "yZMPwdJFT4MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSN(nn.Module):\n",
        "  def __init__(self,\n",
        "               backbone: nn.Module,\n",
        "               masking_ratio: float = 0.15,\n",
        "               ema_p: float = 0.996\n",
        "              ) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.masking_ratio = masking_ratio\n",
        "    self.ema_p = ema_p\n",
        "\n",
        "    self.anchor_backbone = backbone\n",
        "    self.anchor_projection_head = ProjectionHead()\n",
        "\n",
        "    self.target_backbone = copy.deepcopy(self.anchor_backbone)\n",
        "    self.target_projection_head = copy.deepcopy(self.anchor_projection_head)\n",
        "\n",
        "    deactivate_requires_grad(self.target_backbone)\n",
        "    deactivate_requires_grad(self.target_projection_head)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,\n",
        "              views:list[torch.tensor]\n",
        "              ) -> tuple[torch.Tensor]:\n",
        "\n",
        "    update_momentum(model= self.anchor_backbone,\n",
        "                    model_ema= self.target_backbone,\n",
        "                    m = self.ema_p,\n",
        "                   )\n",
        "    update_momentum(model= self.anchor_projection_head,\n",
        "                    model_ema= self.target_projection_head,\n",
        "                    m = self.ema_p,\n",
        "                   )\n",
        "\n",
        "    target_projections = self._target_forward(views[0])\n",
        "    anchor_projections_sim = self._anchor_forward(views[1])\n",
        "    anchor_projections_age = self._anchor_forward(views[2])\n",
        "    anchor_projections_gender = self._anchor_forward(views[3])\n",
        "\n",
        "\n",
        "    anchor_projections = torch.stack((anchor_projections_sim,\n",
        "                                     anchor_projections_age,\n",
        "                                     anchor_projections_gender\n",
        "                                     ),\n",
        "                                     dim= 1\n",
        "                                     )\n",
        "\n",
        "    return (target_projections,\n",
        "            anchor_projections\n",
        "            )\n",
        "\n",
        "  def _target_forward(self,\n",
        "                      view: torch.tensor\n",
        "                      ) -> torch.Tensor:\n",
        "\n",
        "    target_encodings = self.target_backbone(x= view,\n",
        "                                            branch='target'\n",
        "                                            )\n",
        "    target_projections = self.target_projection_head(x= target_encodings)\n",
        "\n",
        "    return target_projections\n",
        "\n",
        "\n",
        "  def _anchor_forward(self,\n",
        "                      view: torch.tensor\n",
        "                      ) -> torch.Tensor:\n",
        "\n",
        "    batch_size, _, _, width = view.shape\n",
        "    seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
        "    idx_keep, idx_mask = random_token_mask(size= (view.shape[0],seq_length),\n",
        "                                           mask_ratio= self.masking_ratio\n",
        "                                          )\n",
        "\n",
        "    anchor_encodings = self.anchor_backbone(x= view,\n",
        "                                            branch= 'anchor',\n",
        "                                            idx_keep= None)\n",
        "    anchor_projections = self.anchor_projection_head(x= anchor_encodings)\n",
        "\n",
        "    return anchor_projections\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V7hPJG6-b1OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSN(backbone)([x,x-.01,x-.02,x-.03])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfZQ2sLoOjt4",
        "outputId": "a5674914-9f60-4fda-a2f2-f5a70a366297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.3703, -0.2485, -1.0448,  ..., -0.1906, -1.1420,  0.0873],\n",
              "          [-0.3931,  0.2382, -0.6288,  ...,  0.0809, -0.4583,  0.0217],\n",
              "          [-0.3198,  0.5406, -0.7428,  ...,  0.1183, -0.4445,  0.0309]],\n",
              " \n",
              "         [[ 0.1449,  0.0282, -0.4336,  ...,  0.2334, -0.6158, -0.0925],\n",
              "          [-0.2670,  0.1333, -0.7477,  ...,  0.3959, -0.2559,  0.4295],\n",
              "          [-0.1163,  0.4009, -0.6852,  ...,  0.2619, -0.0704,  0.0533]]]),\n",
              " tensor([[[-0.3588,  0.2480, -0.6336,  ...,  0.0374, -0.4314, -0.0163],\n",
              "          [-0.3013,  0.2253, -0.6751,  ...,  0.0693, -0.4367, -0.0513],\n",
              "          [-0.2425,  0.2062, -0.7045,  ...,  0.1065, -0.4489, -0.0964]],\n",
              " \n",
              "         [[-0.1811,  0.0526, -0.7377,  ...,  0.3825, -0.4510,  0.3992],\n",
              "          [-0.1445,  0.0192, -0.7054,  ...,  0.4038, -0.6250,  0.3425],\n",
              "          [-0.1111,  0.0163, -0.6744,  ...,  0.4076, -0.7796,  0.2513]]],\n",
              "        grad_fn=<StackBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVWxlItMo8V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQ2_RAkemq2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightly.models.modules.masked_autoencoder import MAEBackbone\n",
        "from torchvision.models import vit_b_16"
      ],
      "metadata": {
        "id": "FGozTVpeYsrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "from functools import partial\n",
        "from typing import Callable, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# vision_transformer requires torchvision >= 0.12\n",
        "from torchvision.models import vision_transformer\n",
        "from torchvision.models.vision_transformer import ConvStemConfig\n",
        "\n",
        "from lightly.models import utils"
      ],
      "metadata": {
        "id": "aoccG0BJsr0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSNEncoder(vision_transformer.Encoder):\n",
        "    \"\"\"Encoder for the Masked Autoencoder model [0].\n",
        "\n",
        "    Encodes patch embeddings. Code inspired by [1].\n",
        "\n",
        "    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377\n",
        "    - [1]: https://github.com/facebookresearch/mae\n",
        "\n",
        "    Attributes:\n",
        "        seq_length:\n",
        "            Token sequence length, including the class token.\n",
        "        num_layers:\n",
        "            Number of transformer blocks.\n",
        "        num_heads:\n",
        "            Number of attention heads.\n",
        "        hidden_dim:\n",
        "            Dimension of the input and output tokens.\n",
        "        mlp_dim:\n",
        "            Dimension of the MLP in the transformer block.\n",
        "        dropout:\n",
        "            Percentage of elements set to zero after the MLP in the transformer.\n",
        "        attention_dropout:\n",
        "            Percentage of elements set to zero after the attention head.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_length: int = 196,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 4,\n",
        "        hidden_dim: int = 512,\n",
        "        mlp_dim: int = 512,\n",
        "        dropout: float = 0.0,\n",
        "        attention_dropout: float = 0.0,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "    ):\n",
        "        super().__init__(\n",
        "            seq_length=seq_length,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            hidden_dim=hidden_dim,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            attention_dropout=attention_dropout,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_vit_encoder(cls, vit_encoder: vision_transformer.Encoder) -> MSNEncoder:\n",
        "        \"\"\"Creates a MSNEncoder from a torchvision ViT encoder.\"\"\"\n",
        "        # Create a new instance with dummy values as they will be overwritten\n",
        "        # by the copied vit_encoder attributes\n",
        "        encoder = cls(seq_length=1,\n",
        "                      num_layers=1,\n",
        "                      num_heads=1,\n",
        "                      hidden_dim=1,\n",
        "                      mlp_dim=1,\n",
        "                      dropout=0,\n",
        "                      attention_dropout=0)\n",
        "        encoder.pos_embedding = vit_encoder.pos_embedding\n",
        "        encoder.dropout = vit_encoder.dropout\n",
        "        encoder.layers = vit_encoder.layers\n",
        "        encoder.ln = vit_encoder.ln\n",
        "        return encoder\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self, input: torch.Tensor, idx_keep: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Encode input tokens.\n",
        "\n",
        "        Args:\n",
        "            input:\n",
        "                Batch of token sequences.\n",
        "            idx_keep:\n",
        "                Tensor with shape (batch_size, num_tokens_to_keep) where each\n",
        "                entry is an index of the token to keep in the respective batch.\n",
        "                If specified, only the indexed tokens will be encoded.\n",
        "\n",
        "        Returns:\n",
        "            Batch of encoded output tokens.\n",
        "        \"\"\"\n",
        "        input = input + self.interpolate_pos_encoding(input)\n",
        "        if idx_keep is not None:\n",
        "            input = utils.get_at_index(input, idx_keep)\n",
        "        return self.ln(self.layers(self.dropout(input)))\n",
        "\n",
        "\n",
        "\n",
        "    def interpolate_pos_encoding(self,\n",
        "                                 input: torch.Tensor,\n",
        "                                 num_cls_tokens: int = 3):\n",
        "        \"\"\"Returns the interpolated positional embedding for the given input.\n",
        "\n",
        "        This function interpolates self.pos_embedding for all tokens in the input,\n",
        "        ignoring the class token. This allows encoding variable sized images.\n",
        "\n",
        "        Args:\n",
        "            input:\n",
        "               Input tensor with shape (batch_size, num_sequences).\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        npatch = input.shape[1] - 1\n",
        "        N = self.pos_embedding.shape[1] - 1\n",
        "        if npatch == N:\n",
        "            return self.pos_embedding\n",
        "        class_emb = self.pos_embedding[:, 0]\n",
        "        pos_embedding = self.pos_embedding[:, 1:]\n",
        "        dim = input.shape[-1]\n",
        "        pos_embedding = nn.functional.interpolate(\n",
        "            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n",
        "                0, 3, 1, 2\n",
        "            ),\n",
        "            scale_factor=math.sqrt(npatch / N),\n",
        "            mode=\"bicubic\",\n",
        "        )\n",
        "        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)"
      ],
      "metadata": {
        "id": "-LOPY1ONsyjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAEBackbone(vision_transformer.VisionTransformer):\n",
        "    \"\"\"Backbone for the Masked Autoencoder model [0].\n",
        "\n",
        "    Converts images into patches and encodes them. Code inspired by [1].\n",
        "    Note that this implementation uses a learned positional embedding while [0]\n",
        "    uses a fixed positional embedding.\n",
        "\n",
        "    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377\n",
        "    - [1]: https://github.com/facebookresearch/mae\n",
        "    - [2]: Early Convolutions Help Transformers See Better, 2021, https://arxiv.org/abs/2106.14881.\n",
        "\n",
        "    Attributes:\n",
        "        image_size:\n",
        "            Input image size.\n",
        "        patch_size:\n",
        "            Width and height of the image patches. image_size must be a multiple\n",
        "            of patch_size.\n",
        "        num_layers:\n",
        "            Number of transformer blocks.\n",
        "        num_heads:\n",
        "            Number of attention heads.\n",
        "        hidden_dim:\n",
        "            Dimension of the input and output tokens.\n",
        "        mlp_dim:\n",
        "            Dimension of the MLP in the transformer block.\n",
        "        dropout:\n",
        "            Percentage of elements set to zero after the MLP in the transformer.\n",
        "        attention_dropout:\n",
        "            Percentage of elements set to zero after the attention head.\n",
        "        num_classes:\n",
        "            Number of classes for the classification head. Currently not used.\n",
        "        representation_size:\n",
        "            If specified, an additional linear layer is added before the\n",
        "            classification head to change the token dimension from hidden_dim\n",
        "            to representation_size. Currently not used.\n",
        "        norm_layer:\n",
        "            Callable that creates a normalization layer.\n",
        "        conv_stem_configs:\n",
        "            If specified, a convolutional stem is added at the beggining of the\n",
        "            network following [2]. Not used in the original Masked Autoencoder\n",
        "            paper [0].\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: int,\n",
        "        patch_size: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        hidden_dim: int,\n",
        "        mlp_dim: int,\n",
        "        dropout: float = 0,\n",
        "        attention_dropout: float = 0,\n",
        "        num_classes: int = 1000,\n",
        "        representation_size: Optional[int] = None,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            image_size=image_size,\n",
        "            patch_size=patch_size,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            hidden_dim=hidden_dim,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            attention_dropout=attention_dropout,\n",
        "            num_classes=num_classes,\n",
        "            representation_size=representation_size,\n",
        "            norm_layer=norm_layer,\n",
        "            conv_stem_configs=conv_stem_configs,\n",
        "        )\n",
        "        self.encoder = MSNEncoder(\n",
        "            seq_length=self.seq_length,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            hidden_dim=hidden_dim,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            attention_dropout=attention_dropout,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_vit(cls, vit: vision_transformer.VisionTransformer) -> MAEBackbone:\n",
        "        \"\"\"Creates a MAEBackbone from a torchvision ViT model.\"\"\"\n",
        "        # Create a new instance with dummy values as they will be overwritten\n",
        "        # by the copied vit_encoder attributes\n",
        "        backbone = cls(\n",
        "            image_size=vit.image_size,\n",
        "            patch_size=vit.patch_size,\n",
        "            num_layers=1,\n",
        "            num_heads=1,\n",
        "            hidden_dim=vit.hidden_dim,\n",
        "            mlp_dim=vit.mlp_dim,\n",
        "            dropout=vit.dropout,\n",
        "            attention_dropout=vit.attention_dropout,\n",
        "            num_classes=vit.num_classes,\n",
        "            representation_size=vit.representation_size,\n",
        "            norm_layer=vit.norm_layer,\n",
        "        )\n",
        "        backbone.conv_proj = vit.conv_proj\n",
        "        backbone.class_token = vit.class_token\n",
        "        backbone.seq_length = vit.seq_length\n",
        "        backbone.heads = vit.heads\n",
        "        backbone.encoder = MAEEncoder.from_vit_encoder(vit.encoder)\n",
        "        return backbone\n",
        "\n",
        "    def forward(\n",
        "        self, images: torch.Tensor, idx_keep: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Returns encoded class tokens from a batch of images.\n",
        "\n",
        "        Args:\n",
        "            images:\n",
        "                Tensor with shape (batch_size, channels, image_size, image_size).\n",
        "            idx_keep:\n",
        "                Tensor with shape (batch_size, num_tokens_to_keep) where each\n",
        "                entry is an index of the token to keep in the respective batch.\n",
        "                If specified, only the indexed tokens will be passed to the\n",
        "                encoder.\n",
        "\n",
        "        Returns:\n",
        "            Tensor with shape (batch_size, hidden_dim) containing the\n",
        "            encoded class token for every image.\n",
        "\n",
        "        \"\"\"\n",
        "        out = self.encode(images, idx_keep)\n",
        "        class_token = out[:, 0]\n",
        "        return class_token\n",
        "\n",
        "    def encode(\n",
        "        self, images: torch.Tensor, idx_keep: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Returns encoded class and patch tokens from images.\n",
        "\n",
        "        Args:\n",
        "            images:\n",
        "                Tensor with shape (batch_size, channels, image_size, image_size).\n",
        "            idx_keep:\n",
        "                Tensor with shape (batch_size, num_tokens_to_keep) where each\n",
        "                entry is an index of the token to keep in the respective batch.\n",
        "                If specified, only the indexed tokens will be passed to the\n",
        "                encoder.\n",
        "\n",
        "        Returns:\n",
        "            Tensor with shape (batch_size, sequence_length, hidden_dim)\n",
        "            containing the encoded class and patch tokens for every image.\n",
        "\n",
        "        \"\"\"\n",
        "        out = self.images_to_tokens(images, prepend_class_token=True)\n",
        "        return self.encoder(out, idx_keep)\n",
        "\n",
        "    def images_to_tokens(\n",
        "        self, images: torch.Tensor, prepend_class_token: bool\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Converts images into patch tokens.\n",
        "\n",
        "        Args:\n",
        "            images:\n",
        "                Tensor with shape (batch_size, channels, image_size, image_size).\n",
        "\n",
        "        Returns:\n",
        "            Tensor with shape (batch_size, sequence_length - 1, hidden_dim)\n",
        "            containing the patch tokens.\n",
        "        \"\"\"\n",
        "        x = self.conv_proj(images)\n",
        "        tokens = x.flatten(2).transpose(1, 2)\n",
        "        if prepend_class_token:\n",
        "            tokens = utils.prepend_class_token(tokens, self.class_token)\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "gt5sjygmYsmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAEBackbone.from_vit(backbone)(x).shape"
      ],
      "metadata": {
        "id": "SnRJFH1FYsjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53c206f-a597-4c18-b4ab-b54890ff3d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RA6Fn_RDYsg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZN3QbyoxYseC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqlbSVIiYsYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TEA6uT82YsV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NAR0JgkYsTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2SICuTJZYsQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRORG9nnYsOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ci_Rz4amYsLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXFghtHbYsGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L69HyAS6YsDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xuiXBby1YsA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGA-UEYTYr9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jqX4qsmYr4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from tqdm import tqdm, trange\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "dFl78FRPYruq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac39185-f35c-4074-bcad-96c04c2e830a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb045fec710>"
            ]
          },
          "metadata": {},
          "execution_count": 492
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    def __init__(self,\n",
        "                 images_size: int =  224,\n",
        "                 patch_size: int = 16,\n",
        "                 num_channels: int = 3,\n",
        "                 embedding_dim: int = 512\n",
        "                ) -> None:\n",
        "      super().__init__()\n",
        "      \"\"\"\n",
        "        Role: convert the input image of size = (b,c,h,w) into patches sequence\n",
        "              of size = (b,p,embedding_dim) where\n",
        "              b = batch size\n",
        "              c = number of channels\n",
        "              h = height\n",
        "              w = width\n",
        "\n",
        "        Inputs:\n",
        "            image_size - Dimensionality of the input image (width or height)\n",
        "            patch_size - Dimensionality of the image patch (width or height)\n",
        "            embedding_dim - Dimensionality of the patch embedding\n",
        "            num_channels - Number of channels in the input images (usually 3)\n",
        "      \"\"\"\n",
        "      self.image_size = images_size\n",
        "      self.patch_size = patch_size\n",
        "      self.num_channels = num_channels\n",
        "      self.hidden_size = embedding_dim\n",
        "\n",
        "      self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "\n",
        "      self.projection_layer = nn.Conv2d(in_channels= self.num_channels,\n",
        "                                        out_channels= self.hidden_size,\n",
        "                                        kernel_size= self.patch_size,\n",
        "                                        stride= self.patch_size\n",
        "                                       )\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor\n",
        "                ) -> torch.Tensor:\n",
        "      x = self.projection_layer(x)\n",
        "      x = x.flatten(2).transpose(1,2)\n",
        "      return x"
      ],
      "metadata": {
        "id": "Zo2kXn4xa8Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self,\n",
        "                 images_size: int =  224,\n",
        "                 num_channels: int = 3,\n",
        "                 patch_size: int = 16,\n",
        "                 embedding_dim: int = 512,\n",
        "                 num_cls_tokens: int = 3,\n",
        "                 dropout_p: float = 0.0,\n",
        "                 learnable_pos_encoding: bool = True\n",
        "                ) -> None:\n",
        "      super().__init__()\n",
        "      \"\"\"\n",
        "        Role: convert the input image into patches as well as adding CLS token\n",
        "        and positional encoding\n",
        "\n",
        "        Inputs:\n",
        "            image_size - Dimensionality of the input image (width or height)\n",
        "            num_channels - Number of channels in the input images (usually 3)\n",
        "            patch_size - Dimensionality of the image patch (width or height)\n",
        "            embedding_dim - Dimensionality of the patch embedding\n",
        "            num_cls_tokens - number of classification tokens to add to the sequence\n",
        "            dropout_p -  percentage of applied dropout\n",
        "            learnable_pos_encoding - specify typ of positional encoding (fixed or learnable)\n",
        "      \"\"\"\n",
        "      self.image_size = images_size\n",
        "      self.patch_size = patch_size\n",
        "      self.num_channels = num_channels\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.num_cls_tokens = num_cls_tokens\n",
        "      self.dropout_p = dropout_p\n",
        "      self.learnable_pos_encoding = learnable_pos_encoding\n",
        "\n",
        "      self.patch_embeddings = PatchEmbeddings(images_size= self.image_size,\n",
        "                                              patch_size= self.patch_size,\n",
        "                                              num_channels= self.num_channels,\n",
        "                                              embedding_dim= self.embedding_dim\n",
        "                                              )\n",
        "\n",
        "      self.cls_token = nn.Parameter(torch.randn(size= (1, self.num_cls_tokens, self.embedding_dim)))\n",
        "\n",
        "      num_patches = self.patch_embeddings.num_patches\n",
        "      if self.learnable_pos_encoding:\n",
        "        num_patches = self.patch_embeddings.num_patches\n",
        "        self.positional_embeddings = nn.Parameter(torch.empty(1, num_patches , self.embedding_dim).normal_(std=0.02))\n",
        "\n",
        "      else:\n",
        "        self.positional_embeddings = self._get_positional_embeddings(num_patches=num_patches,\n",
        "                                                                     embedding_dim=self.embedding_dim\n",
        "                                                                    )\n",
        "\n",
        "      self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor\n",
        "                ) -> torch.Tensor:\n",
        "\n",
        "      x = self.patch_embeddings(x)\n",
        "      batch_size, _, _ = x.size()\n",
        "      cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
        "      x = torch.cat([cls_token,x], dim=1)\n",
        "      x = x + self.positional_embeddings\n",
        "      x = self.dropout(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "    # def _get_positional_embeddings(self,\n",
        "    #                              num_patches: int,\n",
        "    #                              embedding_dim: int\n",
        "    #                              ) -> torch.Tensor:\n",
        "    #   poaitional_embeddings  = torch.ones(num_patches + self.num_cls_tokens, embedding_dim)\n",
        "    #   for i in range(num_patches + self.num_cls_tokens):\n",
        "    #       for j in range(embedding_dim):\n",
        "    #           poaitional_embeddings [i][j] = np.sin(i / (10000 ** (j / embedding_dim))) \\\n",
        "    #           if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / embedding_dim)))\n",
        "    #   return poaitional_embeddings.unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "QgIQjWfMbCad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNet(nn.Module):\n",
        "    def __init__(self,\n",
        "               embedding_dim: int = 512,\n",
        "               hidden_dim = 192,\n",
        "               dropout_p: float = 0.0\n",
        "               ) -> None:\n",
        "      super().__init__()\n",
        "      \"\"\"\n",
        "        Role: Feed forward netork of attention head\n",
        "\n",
        "        Inputs:\n",
        "            embedding_dim - Dimensionality of the patch embedding\n",
        "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
        "            dropout_p -  percentage of applied dropout\n",
        "            learnable_pos_encoding - specify typ of positional encoding (fixed or learnable)\n",
        "      \"\"\"\n",
        "      self.feed_forward = nn.Sequential(nn.Linear(in_features= embedding_dim,\n",
        "                                                  out_features= hidden_dim\n",
        "                                                 ),\n",
        "                                        nn.GELU(),\n",
        "                                        nn.Linear(in_features=hidden_dim,\n",
        "                                                  out_features=embedding_dim\n",
        "                                                 ),\n",
        "                                        nn.Dropout(dropout_p)\n",
        "                                       )\n",
        "    def forward(self,\n",
        "              x: torch.tensor\n",
        "              )-> torch.Tensor:\n",
        "      x = self.feed_forward(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "6RB8v9TUbHOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int = 512,\n",
        "                 hidden_dim: int = 192,\n",
        "                 num_heads: int = 4,\n",
        "                 dropout_p=0.0,\n",
        "                 pre_layer_norm: bool= True\n",
        "                 ) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      \"\"\" Role: capture the interaction between\n",
        "          Inputs:\n",
        "              embedding_dim - Dimensionality of input and attention feature vectors\n",
        "              hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
        "              num_heads - Number of heads to use in the Multi-Head Attention block\n",
        "              dropout - Amount of dropout to apply in the feed-forward network\n",
        "              pre_layer_norm - Specifies where to apply the layer norm (before or after)\n",
        "      \"\"\"\n",
        "      self.pre_layer_norm = pre_layer_norm\n",
        "      self.layer_norm_1 = nn.LayerNorm(normalized_shape= embedding_dim)\n",
        "\n",
        "      self.attention_head = nn.MultiheadAttention(embed_dim= embedding_dim,\n",
        "                                                  num_heads= num_heads,\n",
        "                                                  dropout= dropout_p)\n",
        "\n",
        "      self.layer_norm_2 = nn.LayerNorm(normalized_shape= embedding_dim)\n",
        "\n",
        "      self.feed_forward = FeedForwardNet(embedding_dim= embedding_dim,\n",
        "                                         hidden_dim= hidden_dim,\n",
        "                                         dropout_p= dropout_p\n",
        "                                        )\n",
        "\n",
        "    #attention weights from here\n",
        "    def forward(self,\n",
        "                x: torch.tensor\n",
        "                )-> torch.Tensor:\n",
        "\n",
        "        if self.pre_layer_norm:\n",
        "          normalization_out1 = self.layer_norm_1(x)\n",
        "          attention_out = self.attention_head(normalization_out1, normalization_out1, normalization_out1)[0]\n",
        "          addition_out1 = x + attention_out\n",
        "          normalization_out2 = self.layer_norm_2(addition_out1)\n",
        "          feed_forward_out = self.feed_forward(normalization_out2)\n",
        "          layer_out = addition_out1 + feed_forward_out\n",
        "\n",
        "        else:\n",
        "          attention_out = self.attention_head(x,x,x)[0]\n",
        "          addition_out1 = x + attention_out\n",
        "          normalization_out = self.layer_norm_1(addition_out1)\n",
        "          feed_forward_out = self.feed_forward(normalization_out)\n",
        "          addition_out2 = normalization_out + feed_forward_out\n",
        "          layer_out = self.layer_norm_2(addition_out2)\n",
        "\n",
        "        return attention_out"
      ],
      "metadata": {
        "id": "jWWhv6RubWr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers: int = 6,\n",
        "                 num_heads: int = 4,\n",
        "                 hidden_dim: int = 192,\n",
        "                 pre_layer_norm: bool= True,\n",
        "                 images_size: int =  224,\n",
        "                 num_channels: int = 3,\n",
        "                 patch_size: int = 16,\n",
        "                 embedding_dim: int = 512,\n",
        "                 num_cls_tokens: int = 3,\n",
        "                 dropout_p: float = 0.0,\n",
        "                 learnable_pos_encoding: bool = False\n",
        "                ) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "\n",
        "      self.image_size = images_size\n",
        "      self.patch_size = patch_size\n",
        "      self.num_channels = num_channels\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.num_cls_tokens = num_cls_tokens\n",
        "      self.dropout_p = dropout_p\n",
        "      self.learnable_pos_encoding = learnable_pos_encoding\n",
        "\n",
        "      self.input_embeddings = PatchEmbeddings(images_size= self.image_size,\n",
        "                                              num_channels= self.num_channels,\n",
        "                                              patch_size= self.patch_size,\n",
        "                                              embedding_dim= self.embedding_dim,\n",
        "                                              )\n",
        "                                              # num_cls_tokens= self.num_cls_tokens,\n",
        "                                              # dropout_p=self.dropout_p,\n",
        "                                              # learnable_pos_encoding=self.learnable_pos_encoding)\n",
        "\n",
        "\n",
        "\n",
        "      self.num_layers = num_layers\n",
        "      self.num_heads = num_heads\n",
        "      self.hidden_dim = hidden_dim\n",
        "      self.pre_layer_norm = pre_layer_norm\n",
        "\n",
        "      self.attention_block = AttentionBlock(embedding_dim= self.embedding_dim,\n",
        "                                            hidden_dim= self.hidden_dim,\n",
        "                                            num_heads= self.num_heads,\n",
        "                                            dropout_p=self.dropout_p,\n",
        "                                            pre_layer_norm=self.pre_layer_norm)\n",
        "\n",
        "\n",
        "      self.cls_token = nn.Parameter(torch.randn(size= (1, self.num_cls_tokens, self.embedding_dim)))\n",
        "      num_patches = self.input_embeddings.num_patches\n",
        "      self.positional_embeddings = nn.Parameter(torch.empty(1, num_patches + self.num_cls_tokens, self.embedding_dim).normal_(std=0.02))\n",
        "\n",
        "      self.encoder = nn.Sequential(*[self.attention_block for layer in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.tensor,\n",
        "                branch: str = 'target'\n",
        "                )-> torch.Tensor:\n",
        "\n",
        "      if branch == 'target':\n",
        "        x = self.input_embeddings(x)\n",
        "        batch_size, _, _ = x.size()\n",
        "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
        "        x = torch.cat([cls_token,x], dim=1)\n",
        "        x = x + self.positional_embeddings\n",
        "        x = self.encoder(x)\n",
        "        return x#[:,0:self.num_cls_tokens]\n",
        "\n",
        "      elif branch == 'anchor':\n",
        "        x = self.input_embeddings(x)\n",
        "        batch_size, _, _ = x.size()\n",
        "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
        "        x = torch.cat([cls_token,x], dim=1)\n",
        "        x = x + self.positional_embeddings\n",
        "        x = torch.cat((x[:,:1],x[:,self.num_cls_tokens:]),dim = 1)\n",
        "        x = self.encoder(x)\n",
        "        return x#[:,0]"
      ],
      "metadata": {
        "id": "bGe0sAbCbcSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEXComnnbk7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}