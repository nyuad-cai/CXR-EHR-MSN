{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xREDrfXYsQ2v"
   },
   "source": [
    "# Transformer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sv_KTqnFsQfM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple\n",
    "from types import FunctionType\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "31q37yTHbxE8"
   },
   "outputs": [],
   "source": [
    "class ConvStemConfig(NamedTuple):\n",
    "    out_channels: int = 64\n",
    "    kernel_size: int = 3\n",
    "    stride: int = 2\n",
    "    norm_layer: Callable[..., nn.Module] = nn.BatchNorm2d\n",
    "    activation_layer: Callable[..., nn.Module] = nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_api_usage_once(obj: Any) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Logs API usage(module and name) within an organization.\n",
    "    In a large ecosystem, it's often useful to track the PyTorch and\n",
    "    TorchVision APIs usage. This API provides the similar functionality to the\n",
    "    logging module in the Python stdlib. It can be used for debugging purpose\n",
    "    to log which methods are used and by default it is inactive, unless the user\n",
    "    manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_.\n",
    "    Please note it is triggered only once for the same API call within a process.\n",
    "    It does not collect any data from open-source users since it is no-op by default.\n",
    "    For more information, please refer to\n",
    "    * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging;\n",
    "    * Logging policy: https://github.com/pytorch/vision/issues/5052;\n",
    "\n",
    "    Args:\n",
    "        obj (class instance or method): an object to extract info from.\n",
    "    \"\"\"\n",
    "    module = obj.__module__\n",
    "    if not module.startswith(\"torchvision\"):\n",
    "        module = f\"torchvision.internal.{module}\"\n",
    "    name = obj.__class__.__name__\n",
    "    if isinstance(obj, FunctionType):\n",
    "        name = obj.__name__\n",
    "    torch._C._log_api_usage_once(f\"{module}.{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ntuple(x: Any, n: int) -> Tuple[Any, ...]:\n",
    "    \"\"\"\n",
    "    Make n-tuple from input x. If x is an iterable, then we just convert it to tuple.\n",
    "    Otherwise, we will make a tuple of length n, all with value of x.\n",
    "    reference: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/utils.py#L8\n",
    "\n",
    "    Args:\n",
    "        x (Any): input value\n",
    "        n (int): length of the resulting tuple\n",
    "    \"\"\"\n",
    "    if isinstance(x, collections.abc.Iterable):\n",
    "        return tuple(x)\n",
    "    return tuple(repeat(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S8P8AHvJf-wr"
   },
   "outputs": [],
   "source": [
    "def expand_index_like(index: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Expands the index along the last dimension of the input tokens.\n",
    "\n",
    "    Args:\n",
    "        index:\n",
    "            Index tensor with shape (batch_size, idx_length) where each entry is\n",
    "            an index in [0, sequence_length).\n",
    "        tokens:\n",
    "            Tokens tensor with shape (batch_size, sequence_length, dim).\n",
    "\n",
    "    Returns:\n",
    "        Index tensor with shape (batch_size, idx_length, dim) where the original\n",
    "        indices are repeated dim times along the last dimension.\n",
    "\n",
    "    \"\"\"\n",
    "    dim = tokens.shape[-1]\n",
    "    index = index.unsqueeze(-1).expand(-1, -1, dim)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yhPIIxm3fsP7"
   },
   "outputs": [],
   "source": [
    "def get_at_index(tokens: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Selects tokens at index.\n",
    "\n",
    "    Args:\n",
    "        tokens:\n",
    "            Token tensor with shape (batch_size, sequence_length, dim).\n",
    "        index:\n",
    "            Index tensor with shape (batch_size, index_length) where each entry is\n",
    "            an index in [0, sequence_length).\n",
    "\n",
    "    Returns:\n",
    "        Token tensor with shape (batch_size, index_length, dim) containing the\n",
    "        selected tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    index = expand_index_like(index, tokens)\n",
    "    return torch.gather(tokens, 1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNormActivation(torch.nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, ...]] = 3,\n",
    "        stride: Union[int, Tuple[int, ...]] = 1,\n",
    "        padding: Optional[Union[int, Tuple[int, ...], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        dilation: Union[int, Tuple[int, ...]] = 1,\n",
    "        inplace: Optional[bool] = True,\n",
    "        bias: Optional[bool] = None,\n",
    "        conv_layer: Callable[..., torch.nn.Module] = torch.nn.Conv2d,\n",
    "    ) -> None:\n",
    "\n",
    "        if padding is None:\n",
    "            if isinstance(kernel_size, int) and isinstance(dilation, int):\n",
    "                padding = (kernel_size - 1) // 2 * dilation\n",
    "            else:\n",
    "                _conv_dim = len(kernel_size) if isinstance(kernel_size, Sequence) else len(dilation)\n",
    "                kernel_size = make_ntuple(kernel_size, _conv_dim)\n",
    "                dilation = make_ntuple(dilation, _conv_dim)\n",
    "                padding = tuple((kernel_size[i] - 1) // 2 * dilation[i] for i in range(_conv_dim))\n",
    "        if bias is None:\n",
    "            bias = norm_layer is None\n",
    "\n",
    "        layers = [\n",
    "            conv_layer(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                dilation=dilation,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            layers.append(norm_layer(out_channels))\n",
    "\n",
    "        if activation_layer is not None:\n",
    "            params = {} if inplace is None else {\"inplace\": inplace}\n",
    "            layers.append(activation_layer(**params))\n",
    "        super().__init__(*layers)\n",
    "        log_api_usage_once(self)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if self.__class__ == ConvNormActivation:\n",
    "            warnings.warn(\n",
    "                \"Don't use ConvNormActivation directly, please use Conv2dNormActivation and Conv3dNormActivation instead.\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dNormActivation(ConvNormActivation):\n",
    "    \"\"\"\n",
    "    Configurable block used for Convolution2d-Normalization-Activation blocks.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input image\n",
    "        out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block\n",
    "        kernel_size: (int, optional): Size of the convolving kernel. Default: 3\n",
    "        stride (int, optional): Stride of the convolution. Default: 1\n",
    "        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation``\n",
    "        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        dilation (int): Spacing between kernel elements. Default: 1\n",
    "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
    "        bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "        stride: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: Optional[Union[int, Tuple[int, int], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        dilation: Union[int, Tuple[int, int]] = 1,\n",
    "        inplace: Optional[bool] = True,\n",
    "        bias: Optional[bool] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups,\n",
    "            norm_layer,\n",
    "            activation_layer,\n",
    "            dilation,\n",
    "            inplace,\n",
    "            bias,\n",
    "            torch.nn.Conv2d,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place.\n",
    "            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = None,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "        log_api_usage_once(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Kawb7bqFbydU"
   },
   "outputs": [],
   "source": [
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "jag1PLOzb3_a"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "vXSrz21eb-D6"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        num_cls_tokens: int,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "        self.seq_length = seq_length\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                input: torch.Tensor,\n",
    "                idx_keep: Optional[torch.Tensor] = None\n",
    "               ):\n",
    "\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "\n",
    "        input = input + self._interpolate_pos_encoding(input,self.num_cls_tokens)\n",
    "        if idx_keep is not None:\n",
    "            input = get_at_index(input, idx_keep)\n",
    "        return self.ln(self.layers(self.dropout(input)))\n",
    "\n",
    "    def _interpolate_pos_encoding(self,\n",
    "                                  input: torch.Tensor,\n",
    "                                  num_cls_tokens: int = 3):\n",
    "        \"\"\"Returns the interpolated positional embedding for the given input.\n",
    "\n",
    "        This function interpolates self.pos_embedding for all tokens in the input,\n",
    "        ignoring the class token. This allows encoding variable sized images.\n",
    "\n",
    "        Args:\n",
    "            input:\n",
    "               Input tensor with shape (batch_size, num_sequences).\n",
    "            num_cls_tokens:\n",
    "               number of classification tokens prepended to the\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = input.shape[1] - num_cls_tokens\n",
    "        N = self.pos_embedding.shape[1] - num_cls_tokens\n",
    "        diff = num_cls_tokens -1\n",
    "        if npatch == N:\n",
    "            return self.pos_embedding\n",
    "\n",
    "        else:\n",
    "            npatch += diff\n",
    "            class_emb = self.pos_embedding[:, 1]\n",
    "            pos_embedding = self.pos_embedding[:, 1+diff:]\n",
    "        dim = input.shape[-1]\n",
    "        pos_embedding = nn.functional.interpolate(\n",
    "            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode=\"bicubic\",\n",
    "        )\n",
    "        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "5d2kHQDAadf3"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 4,\n",
    "        hidden_dim: int = 512,\n",
    "        mlp_dim: int = 512,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 1000,\n",
    "        num_cls_tokens: int = 3,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        log_api_usage_once(self)\n",
    "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "\n",
    "        if conv_stem_configs is not None:\n",
    "            # As per https://arxiv.org/abs/2106.14881\n",
    "            seq_proj = nn.Sequential()\n",
    "            prev_channels = 3\n",
    "            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n",
    "                seq_proj.add_module(\n",
    "                    f\"conv_bn_relu_{i}\",\n",
    "                    Conv2dNormActivation(\n",
    "                        in_channels=prev_channels,\n",
    "                        out_channels=conv_stem_layer_config.out_channels,\n",
    "                        kernel_size=conv_stem_layer_config.kernel_size,\n",
    "                        stride=conv_stem_layer_config.stride,\n",
    "                        norm_layer=conv_stem_layer_config.norm_layer,\n",
    "                        activation_layer=conv_stem_layer_config.activation_layer,\n",
    "                    ),\n",
    "                )\n",
    "                prev_channels = conv_stem_layer_config.out_channels\n",
    "            seq_proj.add_module(\n",
    "                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n",
    "            )\n",
    "            self.conv_proj: nn.Module = seq_proj\n",
    "        else:\n",
    "            self.conv_proj = nn.Conv2d(\n",
    "                in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "            )\n",
    "\n",
    "        self.seq_length = (image_size // patch_size) ** 2\n",
    "        # Add a class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, self.num_cls_tokens, hidden_dim))\n",
    "        self.seq_length += self.num_cls_tokens\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            self.seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            num_cls_tokens,\n",
    "            norm_layer)\n",
    "        #self.seq_length = seq_length #+ self.num_cls_tokens\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        #torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        #torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                branch: str = 'target',\n",
    "                idx_keep: Optional[torch.Tensor] = None):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        if branch == 'target':\n",
    "            batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "            x = torch.cat([batch_class_token, x], dim=1)\n",
    "            x = self.encoder(x)\n",
    "            return x[:,0:self.num_cls_tokens]\n",
    "        elif branch == 'anchor':\n",
    "            batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "            x = torch.cat([batch_class_token, x], dim=1)\n",
    "            x = torch.cat((x[:,:1],x[:,self.num_cls_tokens:]),dim = 1)\n",
    "            x = self.encoder(x,idx_keep=idx_keep)\n",
    "            return x[:,0]\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        # else:\n",
    "        #x = x[:, 0]\n",
    "        #x = self.heads(x)\n",
    "\n",
    "        # return x\n",
    "# stemconfig = [ConvStemConfig(out_channels = 64, kernel_size = 3 , stride = 2) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "MrtSLM5XX08w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6665, -0.2615, -1.7997,  ...,  0.1416,  1.7796,  0.8774],\n",
       "        [-0.8177, -0.4620, -1.3975,  ..., -0.3456,  1.2394,  0.5589]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemconfig = [ConvStemConfig(out_channels = 64, kernel_size = 3 , stride = 2) for i in range(4)]\n",
    "x = torch.randn(2,3,224,224)\n",
    "VisionTransformer()(x,'anchor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q97CMPsBRCOF"
   },
   "source": [
    "# MSN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wNr1NT8n2m40"
   },
   "outputs": [],
   "source": [
    "# ! pip install lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bpb07aKp2kAe"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# from lightly.models import utils\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "U9LhQG-8Y_QL"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model:nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "R-7dmv136JjB"
   },
   "outputs": [],
   "source": [
    "def deactivate_requires_grad(model: nn.Module):\n",
    "    \"\"\"Deactivates the requires_grad flag for all parameters of a model.\n",
    "\n",
    "    This has the same effect as permanently executing the model within a `torch.no_grad()`\n",
    "    context. Use this method to disable gradient computation and therefore\n",
    "    training for a model.\n",
    "\n",
    "    Examples:\n",
    "        >>> backbone = resnet18()\n",
    "        >>> deactivate_requires_grad(backbone)\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q7dZ8GkR6ygF"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def update_momentum(model: nn.Module, model_ema: nn.Module, m: float):\n",
    "    \"\"\"Updates parameters of `model_ema` with Exponential Moving Average of `model`\n",
    "\n",
    "    Momentum encoders are a crucial component fo models such as MoCo or BYOL.\n",
    "\n",
    "    Examples:\n",
    "        >>> backbone = resnet18()\n",
    "        >>> projection_head = MoCoProjectionHead()\n",
    "        >>> backbone_momentum = copy.deepcopy(moco)\n",
    "        >>> projection_head_momentum = copy.deepcopy(projection_head)\n",
    "        >>>\n",
    "        >>> # update momentum\n",
    "        >>> update_momentum(moco, moco_momentum, m=0.999)\n",
    "        >>> update_momentum(projection_head, projection_head_momentum, m=0.999)\n",
    "    \"\"\"\n",
    "    for model_ema, model in zip(model_ema.parameters(), model.parameters()):\n",
    "        model_ema.data = model_ema.data * m + model.data * (1.0 - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RFvM_-Jyo3_w"
   },
   "outputs": [],
   "source": [
    "def random_token_mask(size: Tuple[int, int],\n",
    "                      mask_ratio: float = 0.6,\n",
    "                      mask_class_token: bool = False,\n",
    "                     ) -> torch.Tensor:\n",
    "    \"\"\"Creates random token masks.\n",
    "\n",
    "    Args:\n",
    "        size:\n",
    "            Size of the token batch for which to generate masks.\n",
    "            Should be (batch_size, sequence_length).\n",
    "        mask_ratio:\n",
    "            Percentage of tokens to mask.\n",
    "        mask_class_token:\n",
    "            If False the class token is never masked. If True the class token\n",
    "            might be masked.\n",
    "        device:\n",
    "            Device on which to create the index masks.\n",
    "\n",
    "    Returns:\n",
    "        A (index_keep, index_mask) tuple where each index is a tensor.\n",
    "        index_keep contains the indices of the unmasked tokens and has shape\n",
    "        (batch_size, num_keep). index_mask contains the indices of the masked\n",
    "        tokens and has shape (batch_size, sequence_length - num_keep).\n",
    "        num_keep is equal to sequence_length * (1- mask_ratio).\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, sequence_length = size\n",
    "    num_keep = int(sequence_length * (1 - mask_ratio))\n",
    "\n",
    "    noise = torch.rand(batch_size, sequence_length)\n",
    "    if not mask_class_token and sequence_length > 0:\n",
    "        # make sure that class token is not masked\n",
    "        noise[:, 0] = -1\n",
    "        num_keep = max(1, num_keep)\n",
    "\n",
    "    # get indices of tokens to keep\n",
    "    indices = torch.argsort(noise, dim=1)\n",
    "    idx_keep = indices[:, :num_keep]\n",
    "    idx_mask = indices[:, num_keep:]\n",
    "\n",
    "    return idx_keep, idx_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HuPHXqOPRJMD"
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int = 768,\n",
    "                 out_features: int = 2048,\n",
    "               bias: bool = False,\n",
    "              ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_block = nn.Sequential(nn.Linear(in_features= in_features,\n",
    "                                                   out_features= out_features,\n",
    "                                               bias=bias),\n",
    "                                         nn.LayerNorm(normalized_shape= out_features),\n",
    "                                         nn.GELU()\n",
    "                                        )\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.tensor:\n",
    "        x = self.dense_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxHs3WwQRllw",
    "outputId": "004d5fdf-e932-4f78-f2f2-d178b47846fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1536])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2,2,768)\n",
    "DenseBlock(768,768*2)(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "azM2ph3hRtfx"
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int = 512,\n",
    "                 hidden_features: int = 2048,\n",
    "                 out_features: int = 512,\n",
    "                 bias : bool = False\n",
    "                 ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.projection_head = nn.Sequential(DenseBlock(in_features= in_features,\n",
    "                                                    out_features= hidden_features,\n",
    "                                                    bias= bias\n",
    "                                                    ),\n",
    "                                         DenseBlock(in_features= hidden_features,\n",
    "                                                    out_features= hidden_features,\n",
    "                                                    bias= bias\n",
    "                                                    ),\n",
    "                                         nn.Linear(in_features= hidden_features,\n",
    "                                                   out_features= out_features,\n",
    "                                                   bias= bias\n",
    "                                                   )\n",
    "                                         )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        x = self.projection_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HqT_GdtHSoAo"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yZMPwdJFT4MX"
   },
   "outputs": [],
   "source": [
    "backbone = VisionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "V7hPJG6-b1OM"
   },
   "outputs": [],
   "source": [
    "class ChexMSN(nn.Module):\n",
    "    def __init__(self,\n",
    "               backbone: nn.Module,\n",
    "               masking_ratio: float = 0.15,\n",
    "               ema_p: float = 0.996,\n",
    "               focal: bool = True\n",
    "              ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.ema_p = ema_p\n",
    "        self.focal = focal\n",
    "\n",
    "        self.anchor_backbone = backbone\n",
    "        self.anchor_projection_head = ProjectionHead()\n",
    "\n",
    "        self.target_backbone = copy.deepcopy(self.anchor_backbone)\n",
    "        self.target_projection_head = copy.deepcopy(self.anchor_projection_head)\n",
    "\n",
    "        deactivate_requires_grad(self.target_backbone)\n",
    "        deactivate_requires_grad(self.target_projection_head)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                views:list[torch.tensor],\n",
    "                focal: bool = True\n",
    "                ) -> tuple[torch.Tensor]:\n",
    "\n",
    "        update_momentum(model= self.anchor_backbone,\n",
    "                        model_ema= self.target_backbone,\n",
    "                        m = self.ema_p,\n",
    "                       )\n",
    "        update_momentum(model= self.anchor_projection_head,\n",
    "                        model_ema= self.target_projection_head,\n",
    "                        m = self.ema_p,\n",
    "                       )\n",
    "        projections = self._forward_all(batch=views,focal=self.focal)\n",
    "        \n",
    "\n",
    "        return projections\n",
    "\n",
    "    def _target_forward(self,\n",
    "                        view: torch.tensor\n",
    "                        ) -> torch.Tensor:\n",
    "\n",
    "        target_encodings = self.target_backbone(x= view,\n",
    "                                                branch='target'\n",
    "                                                )\n",
    "        target_projections = self.target_projection_head(x= target_encodings)\n",
    "\n",
    "        return target_projections\n",
    "\n",
    "\n",
    "    def _anchor_forward(self,\n",
    "                        view: torch.tensor\n",
    "                        ) -> torch.Tensor:\n",
    "\n",
    "        batch_size, _, _, width = view.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, idx_mask = random_token_mask(size= (view.shape[0],seq_length),\n",
    "                                               mask_ratio= self.masking_ratio\n",
    "                                          )\n",
    "\n",
    "        anchor_encodings = self.anchor_backbone(x= view,\n",
    "                                                branch= 'anchor',\n",
    "                                                idx_keep= idx_keep)\n",
    "        anchor_projections = self.anchor_projection_head(x= anchor_encodings)\n",
    "\n",
    "        return anchor_projections\n",
    "    \n",
    "    \n",
    "    def _forward_all(self,\n",
    "                     batch: list,\n",
    "                     focal: bool = True\n",
    "                     ) -> torch.tensor:\n",
    "        \n",
    "        target_view = batch[0][0]\n",
    "        anchor_view_sim = batch[0][1]\n",
    "        focal_views_sim = torch.concat(batch[0][2:],dim=0)\n",
    "        anchor_view_age = batch[1][1]\n",
    "        focal_views_age = torch.concat(batch[1][2:],dim=0)\n",
    "        anchor_view_gender = batch[2][1]\n",
    "        focal_views_gender = torch.concat(batch[2][2:],dim=0)\n",
    "        \n",
    "        \n",
    "        target_projections = self._target_forward(target_view)\n",
    "        \n",
    "        anchor_projections_sim = self._anchor_forward(anchor_view_sim)\n",
    "        if focal:\n",
    "            anchor_focal_projections_sim = self._anchor_forward(focal_views_sim)\n",
    "            similarity_projections = self._arrange_tokens(anchor_projections_sim,\n",
    "                                                          anchor_focal_projections_sim,\n",
    "                                                          num_focal = 10)\n",
    "                                                          \n",
    "        \n",
    "        anchor_projections_age = self._anchor_forward(anchor_view_age)\n",
    "        if focal:\n",
    "            anchor_focal_projections_age =  self._anchor_forward(focal_views_age)\n",
    "            age_projections = self._arrange_tokens(anchor_projections_age,\n",
    "                                                   anchor_focal_projections_age,\n",
    "                                                   num_focal = 10)\n",
    "                                                    \n",
    "        \n",
    "        \n",
    "        anchor_projections_gender = self._anchor_forward(anchor_view_gender)\n",
    "        if focal:\n",
    "            anchor_focal_projections_gender = self._anchor_forward(focal_views_gender)  \n",
    "            gender_projections = self._arrange_tokens(anchor_projections_gender,\n",
    "                                                      anchor_focal_projections_gender,\n",
    "                                                      num_focal = 10)\n",
    "                                                          \n",
    "        if focal:\n",
    "            anchor_projections = torch.stack((similarity_projections,\n",
    "                                              age_projections,\n",
    "                                              gender_projections\n",
    "                                              ),\n",
    "                                         dim= 0)\n",
    "        else:\n",
    "            anchor_projections = torch.stack((anchor_projections_sim,\n",
    "                                              anchor_projections_age,\n",
    "                                              anchor_projections_gender\n",
    "                                              ),\n",
    "                                             dim= 1)       \n",
    "\n",
    "        return (anchor_projections,\n",
    "                target_projections)\n",
    "    \n",
    "    def _arrange_tokens(self,\n",
    "                        tensor1: torch.tensor,\n",
    "                        tensor2:torch.tensor,\n",
    "                        num_focal: int = 10\n",
    "                        ) ->torch.tensor:\n",
    "\n",
    "        a = torch.stack(torch.split(tensor1,1),0)\n",
    "        b = torch.stack(torch.split(tensor2,num_focal),0)\n",
    "        c = torch.cat((a.expand(-1,num_focal,-1),b),dim=1)[:,num_focal-1:]\n",
    "        arranged_tokens = torch.cat(c.split(1),1).squeeze(0)\n",
    "        return arranged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TVWxlItMo8V8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datalodaer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# x = []\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# x = [torch.randn(10,3,224,224),torch.randn(10,3,224,224)]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# y = [torch.randn(10,3,96,96)for i in range(10)]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# x.extend(y)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdatalodaer\u001b[49m))[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datalodaer' is not defined"
     ]
    }
   ],
   "source": [
    "# x = []\n",
    "# x = [torch.randn(10,3,224,224),torch.randn(10,3,224,224)]\n",
    "# y = [torch.randn(10,3,96,96)for i in range(10)]\n",
    "\n",
    "# x.extend(y)\n",
    "\n",
    "next(iter(datalodaer))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfZQ2sLoOjt4",
    "outputId": "a5674914-9f60-4fda-a2f2-f5a70a366297",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datalodaer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ChexMSN(backbone,focal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdatalodaer\u001b[49m))[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      3\u001b[0m model(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(datalodaer)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datalodaer' is not defined"
     ]
    }
   ],
   "source": [
    "model = ChexMSN(backbone,focal=True)\n",
    "print(len(next(iter(datalodaer))[0]))\n",
    "model(next(iter(datalodaer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def arrange_tokens(tensor1: torch.tensor,\n",
    "                   tensor2:torch.tensor,\n",
    "                   num_focal: int = 10\n",
    "                   ) ->torch.tensor:\n",
    "\n",
    "    a = torch.stack(torch.split(tensor1,1),0)\n",
    "    b = torch.stack(torch.split(tensor2,num_focal),0)\n",
    "    c = torch.cat((a.expand(-1,num_focal,-1),b),dim=1)[:,num_focal-1:]\n",
    "    \n",
    "    return torch.cat(c.split(1),1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rQ2_RAkemq2g"
   },
   "outputs": [],
   "source": [
    "def prototype_probabilities(queries: torch.tensor,\n",
    "                            prototypes: torch.tensor,\n",
    "                            temperature: float,\n",
    "                            ) -> torch.tensor:\n",
    "    \"\"\"Returns probability for each query to belong to each prototype.\n",
    "\n",
    "    Args:\n",
    "        queries:\n",
    "            Tensor with shape (batch_size, dim), projection head output\n",
    "        prototypes:\n",
    "            Tensor with shape (num_prototypes, dim)\n",
    "        temperature:\n",
    "            Inverse scaling factor for the similarity.\n",
    "\n",
    "    Returns:\n",
    "        Probability tensor with shape (batch_size, num_prototypes) which sums to 1 along\n",
    "        the num_prototypes dimension.\n",
    "\n",
    "    \"\"\"                           \n",
    "    return F.softmax(torch.matmul(queries, prototypes.T) / temperature, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(probabilities: torch.tensor, \n",
    "            temperature: float\n",
    "           ) -> torch.tensor:\n",
    "    \"\"\"Sharpens the probabilities with the given temperature.\n",
    "\n",
    "    Args:\n",
    "        probabilities:\n",
    "            Tensor with shape (batch_size, dim)\n",
    "        temperature:\n",
    "            Temperature in (0, 1]. Lower temperature results in stronger sharpening (\n",
    "            output probabilities are less uniform).\n",
    "    Returns:\n",
    "        Probabilities tensor with shape (batch_size, num_prototypes).\n",
    "\n",
    "    \"\"\"\n",
    "    probabilities = probabilities ** (1.0 / temperature)\n",
    "    probabilities /= torch.sum(probabilities, dim=1, keepdim=True)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sinkhorn(probabilities: torch.tensor,\n",
    "             iterations: int = 3,\n",
    "             gather_distributed: bool = False,\n",
    "            ) -> torch.tensor:\n",
    "    \"\"\"Runs sinkhorn normalization on the probabilities as described in [0].\n",
    "\n",
    "    Code inspired by [1].\n",
    "\n",
    "    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141\n",
    "    - [1]: https://github.com/facebookresearch/msn\n",
    "\n",
    "    Args:\n",
    "        probabilities:\n",
    "            Probabilities tensor with shape (batch_size, num_prototypes).\n",
    "        iterations:\n",
    "            Number of iterations of the sinkhorn algorithms. Set to 0 to disable.\n",
    "        gather_distributed:\n",
    "            If True then features from all gpus are gathered during normalization.\n",
    "    Returns:\n",
    "        A normalized probabilities tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    if iterations <= 0:\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    num_targets, num_prototypes = probabilities.shape\n",
    "    probabilities = probabilities.T\n",
    "    sum_probabilities = torch.sum(probabilities)\n",
    "\n",
    "    probabilities = probabilities / sum_probabilities\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # normalize rows\n",
    "        row_sum = torch.sum(probabilities, dim=1, keepdim=True)\n",
    "\n",
    "        probabilities /= row_sum\n",
    "        probabilities /= num_prototypes\n",
    "\n",
    "        # normalize columns\n",
    "        probabilities /= torch.sum(probabilities, dim=0, keepdim=True)\n",
    "        probabilities /= num_targets\n",
    "\n",
    "    probabilities *= num_targets\n",
    "    return probabilities.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_loss(mean_anchor_probs: torch.tensor\n",
    "                       ) -> torch.tensor:\n",
    "    \"\"\"Calculates mean entropy regularization loss.\"\"\"\n",
    "    loss = -torch.sum(torch.log(mean_anchor_probs ** (-mean_anchor_probs)))\n",
    "    loss += math.log(float(len(mean_anchor_probs)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototypes = nn.ModuleList([nn.Linear(512,1024),\n",
    "                           nn.Linear(512,1024),\n",
    "                           nn.Linear(512,1024)]\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FGozTVpeYsrm"
   },
   "outputs": [],
   "source": [
    "class MSNLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 temperature: float = 0.1,\n",
    "                 sinkhorn_iterations: int = 3,\n",
    "                 similarity_weight: float = 1.0,\n",
    "                 age_weight: float = 1.0,\n",
    "                 gender_weight: float = 1.0,\n",
    "                 regularization_weight: float = 1.0,\n",
    "               ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.sinkhorn_iterations = sinkhorn_iterations\n",
    "        self.similarity_weight = similarity_weight\n",
    "        self.age_weight = age_weight\n",
    "        self.gender_weight = gender_weight\n",
    "        self.regularization_weight = regularization_weight\n",
    "    \n",
    "    \n",
    "    def forward(self,\n",
    "                anchors: torch.tensor,\n",
    "                targets: torch.tensor,\n",
    "                prototypes: torch.tensor,\n",
    "                target_sharpen_temperature: float = 0.25,\n",
    "                focal: bool = True\n",
    "                      ) -> torch.tensor:\n",
    "        \n",
    "        similarity_loss = self.similarity_weight * self._forward_loss(anchors=anchors[0] if focal else anchors[:,0],\n",
    "                                                                      targets=targets[:,0],\n",
    "                                                                      prototypes=prototypes[0].weight\n",
    "                                                                     )\n",
    "        age_loss = self.age_weight * self._forward_loss(anchors=anchors[1] if focal else anchors[:,1],\n",
    "                                                           targets=targets[:,1],\n",
    "                                                           prototypes=prototypes[1].weight\n",
    "                                                          )\n",
    "        gender_loss = self.gender_weight * self._forward_loss(anchors=anchors[2] if focal else anchors[:,2],\n",
    "                                                              targets=targets[:,2],\n",
    "                                                              prototypes=prototypes[2].weight\n",
    "                                                             )\n",
    "        \n",
    "        loss = similarity_loss + gender_loss + gender_loss\n",
    "        return loss\n",
    "    \n",
    "    def _forward_loss(self,\n",
    "                anchors: torch.tensor,\n",
    "                targets: torch.tensor,\n",
    "                prototypes: torch.tensor,\n",
    "                target_sharpen_temperature: float = 0.25,\n",
    "               ) -> torch.tensor:\n",
    "\n",
    "        num_views = anchors.shape[0] // targets.shape[0]\n",
    "        anchors = F.normalize(anchors, dim=1)\n",
    "        targets = F.normalize(targets, dim=1)\n",
    "        prototypes = F.normalize(prototypes, dim=1)\n",
    "\n",
    "        anchor_probs = prototype_probabilities(anchors, \n",
    "                                               prototypes, \n",
    "                                               temperature=self.temperature\n",
    "                                              )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_probs = prototype_probabilities(targets, \n",
    "                                                   prototypes, \n",
    "                                                   temperature=self.temperature\n",
    "                                                   )\n",
    "            target_probs = sharpen(target_probs, temperature=target_sharpen_temperature)\n",
    "            if self.sinkhorn_iterations > 0:\n",
    "                target_probs = sinkhorn(probabilities=target_probs,\n",
    "                                        iterations=self.sinkhorn_iterations,\n",
    "                                        )\n",
    "            target_probs = torch.repeat_interleave(target_probs, repeats=num_views, dim=0)\n",
    "\n",
    "        loss = torch.mean(torch.sum(torch.log(anchor_probs ** (-target_probs)), dim=1))\n",
    "\n",
    "        # regularization loss\n",
    "        if self.regularization_weight > 0:\n",
    "            mean_anchor_probs = torch.mean(anchor_probs, dim=0)\n",
    "            reg_loss = regularization_loss(mean_anchor_probs=mean_anchor_probs)\n",
    "            loss += self.regularization_weight * reg_loss\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "aoccG0BJsr0X"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datalodaer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ChexMSN(backbone,focal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m a,b \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdatalodaer\u001b[49m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datalodaer' is not defined"
     ]
    }
   ],
   "source": [
    "model = ChexMSN(backbone,focal=True)\n",
    "a,b = model(next(iter(datalodaer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-LOPY1ONsyjk"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m MSNLoss(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m criterion(\u001b[43ma\u001b[49m, b,prototypes,focal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = MSNLoss(temperature=.5)\n",
    "criterion(a, b,prototypes,focal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL.Image import Image\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE\n",
    "\n",
    "\n",
    "class MSNTransform(MultiViewTransform):\n",
    "    \"\"\"Implements the transformations for MSN [0].\n",
    "\n",
    "    Input to this transform:\n",
    "        PIL Image or Tensor.\n",
    "\n",
    "    Output of this transform:\n",
    "        List of Tensor of length 2 * random_views + focal_views. (12 by default)\n",
    "\n",
    "    Applies the following augmentations by default:\n",
    "        - Random resized crop\n",
    "        - Random horizontal flip\n",
    "        - ImageNet normalization\n",
    "\n",
    "    Generates a set of random and focal views for each input image. The generated output\n",
    "    is (views, target, filenames) where views is list with the following entries:\n",
    "    [random_views_0, random_views_1, ..., focal_views_0, focal_views_1, ...].\n",
    "\n",
    "    - [0]: Masked Siamese Networks, 2022: https://arxiv.org/abs/2204.07141\n",
    "\n",
    "    Attributes:\n",
    "        random_size:\n",
    "            Size of the random image views in pixels.\n",
    "        focal_size:\n",
    "            Size of the focal image views in pixels.\n",
    "        random_views:\n",
    "            Number of random views to generate.\n",
    "        focal_views:\n",
    "            Number of focal views to generate.\n",
    "        random_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops for the relative to random_size.\n",
    "        focal_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops relative to focal_size.\n",
    "        hf_prob:\n",
    "            Probability that horizontal flip is applied.\n",
    "        vf_prob:\n",
    "            Probability that vertical flip is applied.\n",
    "        normalize:\n",
    "            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_size: int = 224,\n",
    "        focal_size: int = 96,\n",
    "        random_views: int = 2,\n",
    "        focal_views: int = 10,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        random_crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        focal_crop_scale: Tuple[float, float] = (0.05, 0.3),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.0,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,\n",
    "    ):\n",
    "        random_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=random_size,\n",
    "            crop_scale=random_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        focal_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=focal_size,\n",
    "            crop_scale=focal_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        transforms = [random_view_transform] * random_views\n",
    "        transforms += [focal_view_transform] * focal_views\n",
    "        super().__init__(transforms=transforms)\n",
    "\n",
    "\n",
    "class MSNViewTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        crop_size: int = 224,\n",
    "        crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.0,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,\n",
    "    ):\n",
    "\n",
    "        transform = [\n",
    "            T.RandomAffine(degrees=affine_dgrees, \n",
    "                           scale=affine_scale, \n",
    "                           shear=affine_shear, \n",
    "                           translate=affine_translate),\n",
    "            T.RandomResizedCrop(size=crop_size, scale=crop_scale),\n",
    "            T.RandomHorizontalFlip(p=hf_prob),\n",
    "            T.RandomVerticalFlip(p=vf_prob),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=normalize[\"mean\"], std=normalize[\"std\"]),\n",
    "        ]\n",
    "\n",
    "        self.transform = T.Compose(transform)\n",
    "\n",
    "    def __call__(self, image: Union[Tensor, Image]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Applies the transforms to the input image.\n",
    "\n",
    "        Args:\n",
    "            image:\n",
    "                The input image to apply the transforms to.\n",
    "\n",
    "        Returns:\n",
    "            The transformed image.\n",
    "\n",
    "        \"\"\"\n",
    "        transformed: Tensor = self.transform(image)\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import Tuple, Optional\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightly.transforms import MSNTransform\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE\n",
    "\n",
    "class ChexMSNDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str,\n",
    "                 transforms: nn.Module,\n",
    "                 same = True\n",
    "                 ) -> None:\n",
    "      \n",
    "        self.meta = pd.read_csv(data_dir)\n",
    "        self.all_images = list(self.meta.path)\n",
    "        self.transform = transforms\n",
    "        self.same = same\n",
    "        \n",
    "    def __len__(self\n",
    "                ) -> int:\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self,\n",
    "                    index: int\n",
    "                    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        \n",
    "        target_path = self.all_images[index]\n",
    "        image_id = target_path.split('/')[-1][:-4]\n",
    "        img_age_path, img_gender_path = self._retrieve_anchors(image_id=image_id,\n",
    "                                                               meta = self.meta,\n",
    "                                                               same=self.same)\n",
    "\n",
    "        img_target = Image.open(fp=target_path).convert('RGB')\n",
    "        img_target = self.transform(img_target)\n",
    "        \n",
    "        img_age = Image.open(fp=img_age_path).convert('RGB')\n",
    "        img_age = self.transform(img_age)\n",
    "\n",
    "        img_gender = Image.open(fp=img_gender_path).convert('RGB')\n",
    "        img_gender = self.transform(img_gender)\n",
    "\n",
    "        return (img_target,img_age,img_gender)\n",
    "    \n",
    "    \n",
    "    def _retrieve_anchors(self,\n",
    "                          image_id: str,\n",
    "                          meta: pd.DataFrame,\n",
    "                          same: bool = False) -> Tuple[str]:\n",
    "        record = meta[meta.dicom_id == image_id]\n",
    "    \n",
    "        subject_id = list(record.subject_id)[0]\n",
    "        age_groub =list(record.ageR5)[0] \n",
    "        gender = list(record.gender)[0]\n",
    "    \n",
    "        group = meta[meta.ageR5 == age_groub]\n",
    "    \n",
    "        if same:\n",
    "            candidate_anchors = group[group.gender == gender]\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.subject_id != subject_id]\n",
    "            images= list(candidate_anchors.path)\n",
    "            sampled_images = random.sample(images,k=2)\n",
    "            image_age, image_gender = sampled_images[0],sampled_images[1]\n",
    "            return image_age, image_gender\n",
    "        else:\n",
    "            candidate_anchors = group\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.subject_id != subject_id]\n",
    "            images= list(candidate_anchors.path)\n",
    "            image_age = random.sample(images,k=1)[0]\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.gender == gender]\n",
    "            images= list(candidate_anchors.path)\n",
    "            image_gender = random.sample(images,k=1)[0]\n",
    "            return image_age, image_gender\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = MSNTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChexMSNDataset(data_dir='../data/meta.csv',transforms= transforms)\n",
    "datalodaer = DataLoader(dataset=dataset,batch_size=2,num_workers=8,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SnRJFH1FYsjf",
    "outputId": "c53c206f-a597-4c18-b4ab-b54890ff3d37",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[[[-1.6213, -1.6213, -1.6213,  ..., -1.6213, -1.6213, -1.6213],\n",
       "            [-1.6213, -1.6213, -1.6213,  ..., -1.6213, -1.6213, -1.6213],\n",
       "            [-1.6213, -1.6213, -1.6213,  ..., -1.6042, -1.6213, -1.6213],\n",
       "            ...,\n",
       "            [-1.6213, -1.6213, -1.6213,  ..., -1.6213, -1.6213, -1.6213],\n",
       "            [-1.6213, -1.6213, -1.6213,  ..., -1.6213, -1.6213, -1.6213],\n",
       "            [-1.6213, -1.6213, -1.6213,  ..., -1.6213, -1.6213, -1.6213]],\n",
       "  \n",
       "           [[-1.5280, -1.5280, -1.5280,  ..., -1.5280, -1.5280, -1.5280],\n",
       "            [-1.5280, -1.5280, -1.5280,  ..., -1.5280, -1.5280, -1.5280],\n",
       "            [-1.5280, -1.5280, -1.5280,  ..., -1.5105, -1.5280, -1.5280],\n",
       "            ...,\n",
       "            [-1.5280, -1.5280, -1.5280,  ..., -1.5280, -1.5280, -1.5280],\n",
       "            [-1.5280, -1.5280, -1.5280,  ..., -1.5280, -1.5280, -1.5280],\n",
       "            [-1.5280, -1.5280, -1.5280,  ..., -1.5280, -1.5280, -1.5280]],\n",
       "  \n",
       "           [[-1.2990, -1.2990, -1.2990,  ..., -1.2990, -1.2990, -1.2990],\n",
       "            [-1.2990, -1.2990, -1.2990,  ..., -1.2990, -1.2990, -1.2990],\n",
       "            [-1.2990, -1.2990, -1.2990,  ..., -1.2816, -1.2990, -1.2990],\n",
       "            ...,\n",
       "            [-1.2990, -1.2990, -1.2990,  ..., -1.2990, -1.2990, -1.2990],\n",
       "            [-1.2990, -1.2990, -1.2990,  ..., -1.2990, -1.2990, -1.2990],\n",
       "            [-1.2990, -1.2990, -1.2990,  ..., -1.2990, -1.2990, -1.2990]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.9295, -1.9809, -2.0152,  ..., -1.2617, -1.2788, -1.2788],\n",
       "            [-1.9295, -1.9809, -2.0152,  ..., -1.3130, -1.3302, -1.2959],\n",
       "            [-1.9295, -1.9809, -2.0152,  ..., -1.2959, -1.2959, -1.2617],\n",
       "            ...,\n",
       "            [-2.0665, -2.0665, -2.0665,  ..., -2.0665, -2.0665, -2.0665],\n",
       "            [-2.0665, -2.0665, -2.0665,  ..., -2.0665, -2.0665, -2.0665],\n",
       "            [-2.0665, -2.0665, -2.0665,  ..., -2.0665, -2.0665, -2.0665]],\n",
       "  \n",
       "           [[-1.8431, -1.8957, -1.9307,  ..., -1.1604, -1.1779, -1.1779],\n",
       "            [-1.8431, -1.8957, -1.9307,  ..., -1.2129, -1.2304, -1.1954],\n",
       "            [-1.8431, -1.8957, -1.9307,  ..., -1.1954, -1.1954, -1.1604],\n",
       "            ...,\n",
       "            [-1.9832, -1.9832, -1.9832,  ..., -1.9832, -1.9832, -1.9832],\n",
       "            [-1.9832, -1.9832, -1.9832,  ..., -1.9832, -1.9832, -1.9832],\n",
       "            [-1.9832, -1.9832, -1.9832,  ..., -1.9832, -1.9832, -1.9832]],\n",
       "  \n",
       "           [[-1.6127, -1.6650, -1.6999,  ..., -0.9330, -0.9504, -0.9504],\n",
       "            [-1.6127, -1.6650, -1.6999,  ..., -0.9853, -1.0027, -0.9678],\n",
       "            [-1.6127, -1.6650, -1.6999,  ..., -0.9678, -0.9678, -0.9330],\n",
       "            ...,\n",
       "            [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522],\n",
       "            [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522],\n",
       "            [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522]]]]),\n",
       "  tensor([[[[-0.7822, -0.8164, -0.8335,  ..., -0.7479, -0.8849, -0.9363],\n",
       "            [-0.3712, -0.3712, -0.4054,  ..., -0.5424, -0.7479, -0.7993],\n",
       "            [-0.1657, -0.1828, -0.1999,  ..., -0.3883, -0.5253, -0.6281],\n",
       "            ...,\n",
       "            [-1.6042, -1.5699, -1.3815,  ...,  1.1358,  1.0844,  1.0844],\n",
       "            [-1.3644, -1.1589, -0.7993,  ...,  1.1358,  1.1015,  1.0844],\n",
       "            [-0.7479, -0.4226, -0.2342,  ...,  1.1700,  1.1187,  1.1187]],\n",
       "  \n",
       "           [[-0.6702, -0.7052, -0.7227,  ..., -0.6352, -0.7752, -0.8277],\n",
       "            [-0.2500, -0.2500, -0.2850,  ..., -0.4251, -0.6352, -0.6877],\n",
       "            [-0.0399, -0.0574, -0.0749,  ..., -0.2675, -0.4076, -0.5126],\n",
       "            ...,\n",
       "            [-1.5105, -1.4755, -1.2829,  ...,  1.2906,  1.2381,  1.2381],\n",
       "            [-1.2654, -1.0553, -0.6877,  ...,  1.2906,  1.2556,  1.2381],\n",
       "            [-0.6352, -0.3025, -0.1099,  ...,  1.3256,  1.2731,  1.2731]],\n",
       "  \n",
       "           [[-0.4450, -0.4798, -0.4973,  ..., -0.4101, -0.5495, -0.6018],\n",
       "            [-0.0267, -0.0267, -0.0615,  ..., -0.2010, -0.4101, -0.4624],\n",
       "            [ 0.1825,  0.1651,  0.1476,  ..., -0.0441, -0.1835, -0.2881],\n",
       "            ...,\n",
       "            [-1.2816, -1.2467, -1.0550,  ...,  1.5071,  1.4548,  1.4548],\n",
       "            [-1.0376, -0.8284, -0.4624,  ...,  1.5071,  1.4722,  1.4548],\n",
       "            [-0.4101, -0.0790,  0.1128,  ...,  1.5420,  1.4897,  1.4897]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  2.4286,  2.4286,  2.4286]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8044,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  2.6400,  2.6400,  2.6400]]]]),\n",
       "  tensor([[[[-2.1179, -2.0494, -1.9980,  ..., -0.3027, -0.3369, -0.2684],\n",
       "            [-2.1179, -2.1008, -1.9980,  ..., -0.2684, -0.3027, -0.2171],\n",
       "            [-2.1179, -2.1179, -2.1008,  ..., -0.2342, -0.2856, -0.2171],\n",
       "            ...,\n",
       "            [-1.8268, -1.9295, -1.2103,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-1.9980, -1.9809, -1.2103,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -1.9980, -1.2103,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-2.0357, -1.9657, -1.9132,  ..., -0.1800, -0.2150, -0.1450],\n",
       "            [-2.0357, -2.0182, -1.9132,  ..., -0.1450, -0.1800, -0.0924],\n",
       "            [-2.0357, -2.0357, -2.0182,  ..., -0.1099, -0.1625, -0.0924],\n",
       "            ...,\n",
       "            [-1.7381, -1.8431, -1.1078,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-1.9132, -1.8957, -1.1078,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -1.9132, -1.1078,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[-1.8044, -1.7347, -1.6824,  ...,  0.0431,  0.0082,  0.0779],\n",
       "            [-1.8044, -1.7870, -1.6824,  ...,  0.0779,  0.0431,  0.1302],\n",
       "            [-1.8044, -1.8044, -1.7870,  ...,  0.1128,  0.0605,  0.1302],\n",
       "            ...,\n",
       "            [-1.5081, -1.6127, -0.8807,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.6824, -1.6650, -0.8807,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.6824, -0.8807,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3712, -0.4054, -0.4397,  ..., -1.0219, -1.0390, -1.0562],\n",
       "            [-0.4226, -0.4397, -0.4739,  ..., -1.0219, -1.0390, -1.0562],\n",
       "            [-0.4568, -0.4739, -0.4911,  ..., -1.0048, -1.0390, -1.0390],\n",
       "            ...,\n",
       "            [-0.3198, -0.2856, -0.2856,  ..., -0.9877, -0.9877, -1.0048],\n",
       "            [-0.4054, -0.3712, -0.3198,  ..., -0.9877, -0.9877, -1.0048],\n",
       "            [-0.4568, -0.4226, -0.3541,  ..., -0.9877, -0.9877, -1.0048]],\n",
       "  \n",
       "           [[-0.2500, -0.2850, -0.3200,  ..., -0.9153, -0.9328, -0.9503],\n",
       "            [-0.3025, -0.3200, -0.3550,  ..., -0.9153, -0.9328, -0.9503],\n",
       "            [-0.3375, -0.3550, -0.3725,  ..., -0.8978, -0.9328, -0.9328],\n",
       "            ...,\n",
       "            [-0.1975, -0.1625, -0.1625,  ..., -0.8803, -0.8803, -0.8978],\n",
       "            [-0.2850, -0.2500, -0.1975,  ..., -0.8803, -0.8803, -0.8978],\n",
       "            [-0.3375, -0.3025, -0.2325,  ..., -0.8803, -0.8803, -0.8978]],\n",
       "  \n",
       "           [[-0.0267, -0.0615, -0.0964,  ..., -0.6890, -0.7064, -0.7238],\n",
       "            [-0.0790, -0.0964, -0.1312,  ..., -0.6890, -0.7064, -0.7238],\n",
       "            [-0.1138, -0.1312, -0.1487,  ..., -0.6715, -0.7064, -0.7064],\n",
       "            ...,\n",
       "            [ 0.0256,  0.0605,  0.0605,  ..., -0.6541, -0.6541, -0.6715],\n",
       "            [-0.0615, -0.0267,  0.0256,  ..., -0.6541, -0.6541, -0.6715],\n",
       "            [-0.1138, -0.0790, -0.0092,  ..., -0.6541, -0.6541, -0.6715]]]]),\n",
       "  tensor([[[[ 0.9988,  0.1426, -0.4226,  ..., -0.3027, -0.1999, -0.1314],\n",
       "            [ 1.1872,  0.3994, -0.0801,  ..., -0.3369, -0.2856, -0.2171],\n",
       "            [ 1.4783,  0.8618,  0.3994,  ..., -0.4397, -0.3369, -0.2513],\n",
       "            ...,\n",
       "            [-0.7137, -0.0287, -0.9020,  ...,  0.5193,  0.5536,  0.6049],\n",
       "            [-0.7137, -0.0287, -0.9020,  ...,  0.5536,  0.5878,  0.6734],\n",
       "            [-0.6965, -0.0287, -0.9192,  ...,  0.5536,  0.6221,  0.6906]],\n",
       "  \n",
       "           [[ 1.1506,  0.2752, -0.3025,  ..., -0.1800, -0.0749, -0.0049],\n",
       "            [ 1.3431,  0.5378,  0.0476,  ..., -0.2150, -0.1625, -0.0924],\n",
       "            [ 1.6408,  1.0105,  0.5378,  ..., -0.3200, -0.2150, -0.1275],\n",
       "            ...,\n",
       "            [-0.6001,  0.1001, -0.7927,  ...,  0.6604,  0.6954,  0.7479],\n",
       "            [-0.6001,  0.1001, -0.7927,  ...,  0.6954,  0.7304,  0.8179],\n",
       "            [-0.5826,  0.1001, -0.8102,  ...,  0.6954,  0.7654,  0.8354]],\n",
       "  \n",
       "           [[ 1.3677,  0.4962, -0.0790,  ...,  0.0431,  0.1476,  0.2173],\n",
       "            [ 1.5594,  0.7576,  0.2696,  ...,  0.0082,  0.0605,  0.1302],\n",
       "            [ 1.8557,  1.2282,  0.7576,  ..., -0.0964,  0.0082,  0.0953],\n",
       "            ...,\n",
       "            [-0.3753,  0.3219, -0.5670,  ...,  0.8797,  0.9145,  0.9668],\n",
       "            [-0.3753,  0.3219, -0.5670,  ...,  0.9145,  0.9494,  1.0365],\n",
       "            [-0.3578,  0.3219, -0.5844,  ...,  0.9145,  0.9842,  1.0539]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.9638, -2.1008, -2.1179,  ...,  0.0741,  0.0741, -0.0287],\n",
       "            [-1.9638, -2.1008, -2.1179,  ...,  0.0569,  0.0912, -0.0287],\n",
       "            [-1.9638, -2.1008, -2.1179,  ..., -0.0629, -0.0458, -0.1657],\n",
       "            ...,\n",
       "            [-1.8268, -1.9467, -2.0152,  ..., -1.4672, -1.4158, -1.3473],\n",
       "            [-1.8268, -1.9467, -2.0152,  ..., -1.4158, -1.3473, -1.4158],\n",
       "            [-1.8268, -1.9467, -2.0152,  ..., -1.4158, -1.3815, -1.4672]],\n",
       "  \n",
       "           [[-1.8782, -2.0182, -2.0357,  ...,  0.2052,  0.2052,  0.1001],\n",
       "            [-1.8782, -2.0182, -2.0357,  ...,  0.1877,  0.2227,  0.1001],\n",
       "            [-1.8782, -2.0182, -2.0357,  ...,  0.0651,  0.0826, -0.0399],\n",
       "            ...,\n",
       "            [-1.7381, -1.8606, -1.9307,  ..., -1.3704, -1.3179, -1.2479],\n",
       "            [-1.7381, -1.8606, -1.9307,  ..., -1.3179, -1.2479, -1.3179],\n",
       "            [-1.7381, -1.8606, -1.9307,  ..., -1.3179, -1.2829, -1.3704]],\n",
       "  \n",
       "           [[-1.6476, -1.7870, -1.8044,  ...,  0.4265,  0.4265,  0.3219],\n",
       "            [-1.6476, -1.7870, -1.8044,  ...,  0.4091,  0.4439,  0.3219],\n",
       "            [-1.6476, -1.7870, -1.8044,  ...,  0.2871,  0.3045,  0.1825],\n",
       "            ...,\n",
       "            [-1.5081, -1.6302, -1.6999,  ..., -1.1421, -1.0898, -1.0201],\n",
       "            [-1.5081, -1.6302, -1.6999,  ..., -1.0898, -1.0201, -1.0898],\n",
       "            [-1.5081, -1.6302, -1.6999,  ..., -1.0898, -1.0550, -1.1421]]]]),\n",
       "  tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            ...,\n",
       "            [ 1.6153,  1.7694,  1.9749,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 1.6153,  1.7694,  1.9749,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 1.6153,  1.7694,  1.9749,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            ...,\n",
       "            [ 1.7808,  1.9384,  2.1485,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 1.7808,  1.9384,  2.1485,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 1.7808,  1.9384,  2.1485,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            ...,\n",
       "            [ 1.9951,  2.1520,  2.3611,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 1.9951,  2.1520,  2.3611,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 1.9951,  2.1520,  2.3611,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.9638, -1.9638, -1.9638,  ..., -0.7479, -0.7479, -0.7479],\n",
       "            [-1.9638, -1.9638, -1.9638,  ..., -0.7479, -0.7650, -0.7650],\n",
       "            [-1.9638, -1.9638, -1.9638,  ..., -0.7650, -0.7650, -0.7650],\n",
       "            ...,\n",
       "            [-1.9638, -1.9638, -1.9638,  ..., -1.5014, -1.4843, -1.4843],\n",
       "            [-1.9638, -1.9638, -1.9638,  ..., -1.5185, -1.5185, -1.5185],\n",
       "            [-1.9638, -1.9638, -1.9638,  ..., -1.5357, -1.5357, -1.5357]],\n",
       "  \n",
       "           [[-1.8782, -1.8782, -1.8782,  ..., -0.6352, -0.6352, -0.6352],\n",
       "            [-1.8782, -1.8782, -1.8782,  ..., -0.6352, -0.6527, -0.6527],\n",
       "            [-1.8782, -1.8782, -1.8782,  ..., -0.6527, -0.6527, -0.6527],\n",
       "            ...,\n",
       "            [-1.8782, -1.8782, -1.8782,  ..., -1.4055, -1.3880, -1.3880],\n",
       "            [-1.8782, -1.8782, -1.8782,  ..., -1.4230, -1.4230, -1.4230],\n",
       "            [-1.8782, -1.8782, -1.8782,  ..., -1.4405, -1.4405, -1.4405]],\n",
       "  \n",
       "           [[-1.6476, -1.6476, -1.6476,  ..., -0.4101, -0.4101, -0.4101],\n",
       "            [-1.6476, -1.6476, -1.6476,  ..., -0.4101, -0.4275, -0.4275],\n",
       "            [-1.6476, -1.6476, -1.6476,  ..., -0.4275, -0.4275, -0.4275],\n",
       "            ...,\n",
       "            [-1.6476, -1.6476, -1.6476,  ..., -1.1770, -1.1596, -1.1596],\n",
       "            [-1.6476, -1.6476, -1.6476,  ..., -1.1944, -1.1944, -1.1944],\n",
       "            [-1.6476, -1.6476, -1.6476,  ..., -1.2119, -1.2119, -1.2119]]]]),\n",
       "  tensor([[[[-1.6213, -1.5528, -1.5185,  ..., -1.9295, -1.9638, -1.9980],\n",
       "            [-1.6384, -1.5870, -1.5699,  ..., -2.1008, -2.0665, -2.0494],\n",
       "            [-1.6384, -1.6042, -1.6213,  ..., -2.1179, -2.1179, -2.0494],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.0152,  ..., -1.8953, -1.8782, -1.9124],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -1.8097, -1.8439, -1.8953],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -1.8782, -1.9295, -1.9638]],\n",
       "  \n",
       "           [[-1.5280, -1.4580, -1.4230,  ..., -1.8431, -1.8782, -1.9132],\n",
       "            [-1.5455, -1.4930, -1.4755,  ..., -2.0182, -1.9832, -1.9657],\n",
       "            [-1.5455, -1.5105, -1.5280,  ..., -2.0357, -2.0357, -1.9657],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -1.9307,  ..., -1.8081, -1.7906, -1.8256],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -1.7206, -1.7556, -1.8081],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -1.7906, -1.8431, -1.8782]],\n",
       "  \n",
       "           [[-1.2990, -1.2293, -1.1944,  ..., -1.6127, -1.6476, -1.6824],\n",
       "            [-1.3164, -1.2641, -1.2467,  ..., -1.7870, -1.7522, -1.7347],\n",
       "            [-1.3164, -1.2816, -1.2990,  ..., -1.8044, -1.8044, -1.7347],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.6999,  ..., -1.5779, -1.5604, -1.5953],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.4907, -1.5256, -1.5779],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.5604, -1.6127, -1.6476]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.0837, -1.9980, -1.8097,  ...,  1.4440,  1.4440,  1.4269],\n",
       "            [-2.0837, -1.9980, -1.8097,  ...,  1.4098,  1.4098,  1.3927],\n",
       "            [-2.0837, -1.9980, -1.8097,  ...,  1.3584,  1.3584,  1.3413],\n",
       "            ...,\n",
       "            [-2.0494, -1.9295, -1.6898,  ..., -0.1486, -0.3369, -0.4568],\n",
       "            [-2.0494, -1.9295, -1.6898,  ..., -0.1314, -0.3712, -0.5082],\n",
       "            [-2.0494, -1.9295, -1.6898,  ..., -0.1486, -0.3883, -0.5424]],\n",
       "  \n",
       "           [[-2.0007, -1.9132, -1.7206,  ...,  1.6057,  1.6057,  1.5882],\n",
       "            [-2.0007, -1.9132, -1.7206,  ...,  1.5707,  1.5707,  1.5532],\n",
       "            [-2.0007, -1.9132, -1.7206,  ...,  1.5182,  1.5182,  1.5007],\n",
       "            ...,\n",
       "            [-1.9657, -1.8431, -1.5980,  ..., -0.0224, -0.2150, -0.3375],\n",
       "            [-1.9657, -1.8431, -1.5980,  ..., -0.0049, -0.2500, -0.3901],\n",
       "            [-1.9657, -1.8431, -1.5980,  ..., -0.0224, -0.2675, -0.4251]],\n",
       "  \n",
       "           [[-1.7696, -1.6824, -1.4907,  ...,  1.8208,  1.8208,  1.8034],\n",
       "            [-1.7696, -1.6824, -1.4907,  ...,  1.7860,  1.7860,  1.7685],\n",
       "            [-1.7696, -1.6824, -1.4907,  ...,  1.7337,  1.7337,  1.7163],\n",
       "            ...,\n",
       "            [-1.7347, -1.6127, -1.3687,  ...,  0.1999,  0.0082, -0.1138],\n",
       "            [-1.7347, -1.6127, -1.3687,  ...,  0.2173, -0.0267, -0.1661],\n",
       "            [-1.7347, -1.6127, -1.3687,  ...,  0.1999, -0.0441, -0.2010]]]]),\n",
       "  tensor([[[[ 0.7933,  0.7248,  0.7077,  ..., -0.8678, -0.6794, -0.5938],\n",
       "            [ 0.8961,  0.8276,  0.8104,  ..., -1.1247, -0.9363, -0.7993],\n",
       "            [ 0.9132,  0.9132,  0.9132,  ..., -1.3130, -1.1932, -1.0904],\n",
       "            ...,\n",
       "            [-1.9638, -1.9809, -2.0323,  ...,  0.1939,  0.2282,  0.1768],\n",
       "            [-1.9638, -1.9809, -2.0323,  ...,  0.5364,  0.6734,  0.6734],\n",
       "            [-1.9638, -1.9809, -2.0323,  ...,  0.6563,  0.7933,  0.8276]],\n",
       "  \n",
       "           [[ 0.9405,  0.8704,  0.8529,  ..., -0.7577, -0.5651, -0.4776],\n",
       "            [ 1.0455,  0.9755,  0.9580,  ..., -1.0203, -0.8277, -0.6877],\n",
       "            [ 1.0630,  1.0630,  1.0630,  ..., -1.2129, -1.0903, -0.9853],\n",
       "            ...,\n",
       "            [-1.8782, -1.8957, -1.9482,  ...,  0.3277,  0.3627,  0.3102],\n",
       "            [-1.8782, -1.8957, -1.9482,  ...,  0.6779,  0.8179,  0.8179],\n",
       "            [-1.8782, -1.8957, -1.9482,  ...,  0.8004,  0.9405,  0.9755]],\n",
       "  \n",
       "           [[ 1.1585,  1.0888,  1.0714,  ..., -0.5321, -0.3404, -0.2532],\n",
       "            [ 1.2631,  1.1934,  1.1759,  ..., -0.7936, -0.6018, -0.4624],\n",
       "            [ 1.2805,  1.2805,  1.2805,  ..., -0.9853, -0.8633, -0.7587],\n",
       "            ...,\n",
       "            [-1.6476, -1.6650, -1.7173,  ...,  0.5485,  0.5834,  0.5311],\n",
       "            [-1.6476, -1.6650, -1.7173,  ...,  0.8971,  1.0365,  1.0365],\n",
       "            [-1.6476, -1.6650, -1.7173,  ...,  1.0191,  1.1585,  1.1934]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2684, -0.1486,  0.1768,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-0.1486, -0.0801,  0.1597,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-0.1999, -0.0972,  0.2111,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            ...,\n",
       "            [ 2.1462,  2.0434,  1.8037,  ..., -1.8439, -1.8268, -1.8953],\n",
       "            [ 2.2489,  2.2489,  2.1290,  ..., -1.8439, -1.8268, -1.8953],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ..., -1.8439, -1.8268, -1.8953]],\n",
       "  \n",
       "           [[-0.1450, -0.0224,  0.3102,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-0.0224,  0.0476,  0.2927,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-0.0749,  0.0301,  0.3452,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            ...,\n",
       "            [ 2.3235,  2.2185,  1.9734,  ..., -1.7556, -1.7381, -1.8081],\n",
       "            [ 2.4286,  2.4286,  2.3060,  ..., -1.7556, -1.7381, -1.8081],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ..., -1.7556, -1.7381, -1.8081]],\n",
       "  \n",
       "           [[ 0.0779,  0.1999,  0.5311,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 0.1999,  0.2696,  0.5136,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 0.1476,  0.2522,  0.5659,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            ...,\n",
       "            [ 2.5354,  2.4308,  2.1868,  ..., -1.5256, -1.5081, -1.5779],\n",
       "            [ 2.6400,  2.6400,  2.5180,  ..., -1.5256, -1.5081, -1.5779],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ..., -1.5256, -1.5081, -1.5779]]]]),\n",
       "  tensor([[[[-2.1179, -2.1179, -2.1179,  ...,  0.1426,  0.3823,  0.5536],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  0.1597,  0.3823,  0.5364],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  0.1768,  0.3823,  0.5193],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  1.3927,  1.4269,  1.4440],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  1.4098,  1.4269,  1.4612],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  1.4098,  1.4440,  1.4783]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ...,  0.2752,  0.5203,  0.6954],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  0.2927,  0.5203,  0.6779],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  0.3102,  0.5203,  0.6604],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  1.5532,  1.5882,  1.6057],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  1.5707,  1.5882,  1.6232],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  1.5707,  1.6057,  1.6408]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8044,  ...,  0.4962,  0.7402,  0.9145],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  0.5136,  0.7402,  0.8971],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  0.5311,  0.7402,  0.8797],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  1.7685,  1.8034,  1.8208],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  1.7860,  1.8034,  1.8383],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  1.7860,  1.8208,  1.8557]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1083,  0.1254,  0.1597,  ..., -1.1418, -1.1418, -1.1418],\n",
       "            [ 0.0741,  0.1083,  0.1426,  ..., -1.1418, -1.1418, -1.1418],\n",
       "            [ 0.1083,  0.1597,  0.1939,  ..., -1.1418, -1.1418, -1.1418],\n",
       "            ...,\n",
       "            [-0.3541, -0.3541, -0.3883,  ..., -1.1418, -1.1418, -1.1418],\n",
       "            [-0.4054, -0.3883, -0.3883,  ..., -1.1418, -1.1418, -1.1418],\n",
       "            [-0.4226, -0.3541, -0.4054,  ..., -1.1418, -1.1418, -1.1418]],\n",
       "  \n",
       "           [[ 0.2402,  0.2577,  0.2927,  ..., -1.0378, -1.0378, -1.0378],\n",
       "            [ 0.2052,  0.2402,  0.2752,  ..., -1.0378, -1.0378, -1.0378],\n",
       "            [ 0.2402,  0.2927,  0.3277,  ..., -1.0378, -1.0378, -1.0378],\n",
       "            ...,\n",
       "            [-0.2325, -0.2325, -0.2675,  ..., -1.0378, -1.0378, -1.0378],\n",
       "            [-0.2850, -0.2675, -0.2675,  ..., -1.0378, -1.0378, -1.0378],\n",
       "            [-0.3025, -0.2325, -0.2850,  ..., -1.0378, -1.0378, -1.0378]],\n",
       "  \n",
       "           [[ 0.4614,  0.4788,  0.5136,  ..., -0.8110, -0.8110, -0.8110],\n",
       "            [ 0.4265,  0.4614,  0.4962,  ..., -0.8110, -0.8110, -0.8110],\n",
       "            [ 0.4614,  0.5136,  0.5485,  ..., -0.8110, -0.8110, -0.8110],\n",
       "            ...,\n",
       "            [-0.0092, -0.0092, -0.0441,  ..., -0.8110, -0.8110, -0.8110],\n",
       "            [-0.0615, -0.0441, -0.0441,  ..., -0.8110, -0.8110, -0.8110],\n",
       "            [-0.0790, -0.0092, -0.0615,  ..., -0.8110, -0.8110, -0.8110]]]]),\n",
       "  tensor([[[[-2.1008, -2.1008, -2.1008,  ..., -1.5699, -1.6042, -1.6042],\n",
       "            [-2.1008, -2.1008, -2.1008,  ..., -1.5528, -1.5699, -1.5699],\n",
       "            [-2.1008, -2.1008, -2.1008,  ..., -1.5528, -1.5699, -1.5699],\n",
       "            ...,\n",
       "            [-2.1008, -2.1008, -2.1008,  ..., -1.4329, -1.4672, -1.5014],\n",
       "            [-2.1008, -2.1008, -2.1008,  ..., -1.4329, -1.4672, -1.5014],\n",
       "            [-2.1008, -2.1008, -2.1008,  ..., -1.4500, -1.4672, -1.4843]],\n",
       "  \n",
       "           [[-2.0182, -2.0182, -2.0182,  ..., -1.4755, -1.5105, -1.5105],\n",
       "            [-2.0182, -2.0182, -2.0182,  ..., -1.4580, -1.4755, -1.4755],\n",
       "            [-2.0182, -2.0182, -2.0182,  ..., -1.4580, -1.4755, -1.4755],\n",
       "            ...,\n",
       "            [-2.0182, -2.0182, -2.0182,  ..., -1.3354, -1.3704, -1.4055],\n",
       "            [-2.0182, -2.0182, -2.0182,  ..., -1.3354, -1.3704, -1.4055],\n",
       "            [-2.0182, -2.0182, -2.0182,  ..., -1.3529, -1.3704, -1.3880]],\n",
       "  \n",
       "           [[-1.7870, -1.7870, -1.7870,  ..., -1.2467, -1.2816, -1.2816],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.2293, -1.2467, -1.2467],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.2293, -1.2467, -1.2467],\n",
       "            ...,\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.1073, -1.1421, -1.1770],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.1073, -1.1421, -1.1770],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.1247, -1.1421, -1.1596]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.9920,  1.9749,  1.9407,  ..., -0.9877, -1.1075, -1.0733],\n",
       "            [ 1.8722,  1.8208,  1.8037,  ..., -0.9534, -1.0733, -1.0733],\n",
       "            [ 1.8037,  1.7523,  1.7352,  ..., -0.8678, -1.0904, -1.0733],\n",
       "            ...,\n",
       "            [ 1.3755,  0.8447,  0.2796,  ..., -0.8678, -0.7993, -0.6109],\n",
       "            [ 1.1015,  0.6049,  0.1083,  ..., -0.8678, -0.7993, -0.6109],\n",
       "            [ 1.1015,  0.6049,  0.1597,  ..., -0.8678, -0.7993, -0.6109]],\n",
       "  \n",
       "           [[ 2.1660,  2.1485,  2.1134,  ..., -0.8803, -1.0028, -0.9678],\n",
       "            [ 2.0434,  1.9909,  1.9734,  ..., -0.8452, -0.9678, -0.9678],\n",
       "            [ 1.9734,  1.9209,  1.9034,  ..., -0.7577, -0.9853, -0.9678],\n",
       "            ...,\n",
       "            [ 1.5357,  0.9930,  0.4153,  ..., -0.7577, -0.6877, -0.4951],\n",
       "            [ 1.2556,  0.7479,  0.2402,  ..., -0.7577, -0.6877, -0.4951],\n",
       "            [ 1.2556,  0.7479,  0.2927,  ..., -0.7577, -0.6877, -0.4951]],\n",
       "  \n",
       "           [[ 2.3786,  2.3611,  2.3263,  ..., -0.6541, -0.7761, -0.7413],\n",
       "            [ 2.2566,  2.2043,  2.1868,  ..., -0.6193, -0.7413, -0.7413],\n",
       "            [ 2.1868,  2.1346,  2.1171,  ..., -0.5321, -0.7587, -0.7413],\n",
       "            ...,\n",
       "            [ 1.7511,  1.2108,  0.6356,  ..., -0.5321, -0.4624, -0.2707],\n",
       "            [ 1.4722,  0.9668,  0.4614,  ..., -0.5321, -0.4624, -0.2707],\n",
       "            [ 1.4722,  0.9668,  0.5136,  ..., -0.5321, -0.4624, -0.2707]]]]),\n",
       "  tensor([[[[-0.3198, -0.3198, -0.3198,  ..., -0.3198, -0.3198, -0.3198],\n",
       "            [-0.3198, -0.3198, -0.3198,  ..., -0.3198, -0.3198, -0.3198],\n",
       "            [-0.3198, -0.3198, -0.3198,  ..., -0.3198, -0.3198, -0.3198],\n",
       "            ...,\n",
       "            [ 0.7077,  0.8447,  0.8618,  ...,  0.1597,  0.1768,  0.2111],\n",
       "            [ 0.7248,  0.8276,  0.8618,  ...,  0.1426,  0.1597,  0.1597],\n",
       "            [ 0.7419,  0.8104,  0.8618,  ...,  0.0227,  0.0741,  0.1426]],\n",
       "  \n",
       "           [[-0.1975, -0.1975, -0.1975,  ..., -0.1975, -0.1975, -0.1975],\n",
       "            [-0.1975, -0.1975, -0.1975,  ..., -0.1975, -0.1975, -0.1975],\n",
       "            [-0.1975, -0.1975, -0.1975,  ..., -0.1975, -0.1975, -0.1975],\n",
       "            ...,\n",
       "            [ 0.8529,  0.9930,  1.0105,  ...,  0.2927,  0.3102,  0.3452],\n",
       "            [ 0.8704,  0.9755,  1.0105,  ...,  0.2752,  0.2927,  0.2927],\n",
       "            [ 0.8880,  0.9580,  1.0105,  ...,  0.1527,  0.2052,  0.2752]],\n",
       "  \n",
       "           [[ 0.0256,  0.0256,  0.0256,  ...,  0.0256,  0.0256,  0.0256],\n",
       "            [ 0.0256,  0.0256,  0.0256,  ...,  0.0256,  0.0256,  0.0256],\n",
       "            [ 0.0256,  0.0256,  0.0256,  ...,  0.0256,  0.0256,  0.0256],\n",
       "            ...,\n",
       "            [ 1.0714,  1.2108,  1.2282,  ...,  0.5136,  0.5311,  0.5659],\n",
       "            [ 1.0888,  1.1934,  1.2282,  ...,  0.4962,  0.5136,  0.5136],\n",
       "            [ 1.1062,  1.1759,  1.2282,  ...,  0.3742,  0.4265,  0.4962]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.7077,  0.7933,  0.8961,  ..., -0.9363, -0.9363, -0.9534],\n",
       "            [ 0.4337,  0.5878,  0.6563,  ..., -1.0219, -1.0219, -1.0390],\n",
       "            [ 0.1083,  0.2796,  0.3994,  ..., -1.0904, -1.0904, -1.1075],\n",
       "            ...,\n",
       "            [-0.4568, -0.4911, -0.5082,  ...,  1.1872,  1.1187,  1.0673],\n",
       "            [-0.3883, -0.4054, -0.4397,  ...,  1.2043,  1.1187,  0.9817],\n",
       "            [-0.4054, -0.3198, -0.3883,  ...,  1.1700,  1.1015,  0.9303]],\n",
       "  \n",
       "           [[ 0.8529,  0.9405,  1.0455,  ..., -0.8277, -0.8277, -0.8452],\n",
       "            [ 0.5728,  0.7304,  0.8004,  ..., -0.9153, -0.9153, -0.9328],\n",
       "            [ 0.2402,  0.4153,  0.5378,  ..., -0.9853, -0.9853, -1.0028],\n",
       "            ...,\n",
       "            [-0.3375, -0.3725, -0.3901,  ...,  1.3431,  1.2731,  1.2206],\n",
       "            [-0.2675, -0.2850, -0.3200,  ...,  1.3606,  1.2731,  1.1331],\n",
       "            [-0.2850, -0.1975, -0.2675,  ...,  1.3256,  1.2556,  1.0805]],\n",
       "  \n",
       "           [[ 1.0714,  1.1585,  1.2631,  ..., -0.6018, -0.6018, -0.6193],\n",
       "            [ 0.7925,  0.9494,  1.0191,  ..., -0.6890, -0.6890, -0.7064],\n",
       "            [ 0.4614,  0.6356,  0.7576,  ..., -0.7587, -0.7587, -0.7761],\n",
       "            ...,\n",
       "            [-0.1138, -0.1487, -0.1661,  ...,  1.5594,  1.4897,  1.4374],\n",
       "            [-0.0441, -0.0615, -0.0964,  ...,  1.5768,  1.4897,  1.3502],\n",
       "            [-0.0615,  0.0256, -0.0441,  ...,  1.5420,  1.4722,  1.2980]]]]),\n",
       "  tensor([[[[ 0.8276,  0.8447,  0.8789,  ...,  1.0331,  1.0331,  1.0331],\n",
       "            [ 0.6049,  0.6221,  0.6221,  ...,  0.8447,  0.8447,  0.8447],\n",
       "            [ 0.5022,  0.5193,  0.5022,  ...,  0.6563,  0.6392,  0.6392],\n",
       "            ...,\n",
       "            [ 0.4166,  0.3823,  0.3309,  ..., -0.2171, -0.2171, -0.2171],\n",
       "            [ 0.3823,  0.3994,  0.2453,  ..., -0.2342, -0.2342, -0.2342],\n",
       "            [ 0.3309,  0.3481,  0.1939,  ..., -0.2342, -0.2342, -0.2342]],\n",
       "  \n",
       "           [[ 0.9755,  0.9930,  1.0280,  ...,  1.1856,  1.1856,  1.1856],\n",
       "            [ 0.7479,  0.7654,  0.7654,  ...,  0.9930,  0.9930,  0.9930],\n",
       "            [ 0.6429,  0.6604,  0.6429,  ...,  0.8004,  0.7829,  0.7829],\n",
       "            ...,\n",
       "            [ 0.5553,  0.5203,  0.4678,  ..., -0.0924, -0.0924, -0.0924],\n",
       "            [ 0.5203,  0.5378,  0.3803,  ..., -0.1099, -0.1099, -0.1099],\n",
       "            [ 0.4678,  0.4853,  0.3277,  ..., -0.1099, -0.1099, -0.1099]],\n",
       "  \n",
       "           [[ 1.1934,  1.2108,  1.2457,  ...,  1.4025,  1.4025,  1.4025],\n",
       "            [ 0.9668,  0.9842,  0.9842,  ...,  1.2108,  1.2108,  1.2108],\n",
       "            [ 0.8622,  0.8797,  0.8622,  ...,  1.0191,  1.0017,  1.0017],\n",
       "            ...,\n",
       "            [ 0.7751,  0.7402,  0.6879,  ...,  0.1302,  0.1302,  0.1302],\n",
       "            [ 0.7402,  0.7576,  0.6008,  ...,  0.1128,  0.1128,  0.1128],\n",
       "            [ 0.6879,  0.7054,  0.5485,  ...,  0.1128,  0.1128,  0.1128]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.5528, -1.6727, -1.8268,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-1.5699, -1.6727, -1.8268,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-1.5699, -1.6898, -1.8439,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-1.4580, -1.5805, -1.7381,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-1.4755, -1.5805, -1.7381,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-1.4755, -1.5980, -1.7556,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[-1.2293, -1.3513, -1.5081,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-1.2467, -1.3513, -1.5081,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-1.2467, -1.3687, -1.5256,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]),\n",
       "  tensor([[[[-2.0152, -1.8610, -2.1179,  ..., -0.7822, -1.2959, -1.8610],\n",
       "            [-1.8953, -1.9980, -2.1179,  ..., -0.5253, -0.9705, -1.1075],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -0.2684, -0.6281, -0.6452],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-1.9307, -1.7731, -2.0357,  ..., -0.6702, -1.1954, -1.7731],\n",
       "            [-1.8081, -1.9132, -2.0357,  ..., -0.4076, -0.8627, -1.0028],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -0.1450, -0.5126, -0.5301],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[-1.6999, -1.5430, -1.8044,  ..., -0.4450, -0.9678, -1.5430],\n",
       "            [-1.5779, -1.6824, -1.8044,  ..., -0.1835, -0.6367, -0.7761],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  0.0779, -0.2881, -0.3055],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.1015,  1.2043,  1.3927,  ...,  0.6049,  0.7591,  0.7077],\n",
       "            [ 1.1529,  1.2385,  1.4098,  ...,  0.5707,  0.6563,  0.5878],\n",
       "            [ 1.2728,  1.3927,  1.5297,  ...,  0.5022,  0.4337,  0.2796],\n",
       "            ...,\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489]],\n",
       "  \n",
       "           [[ 1.2556,  1.3606,  1.5532,  ...,  0.7479,  0.9055,  0.8529],\n",
       "            [ 1.3081,  1.3957,  1.5707,  ...,  0.7129,  0.8004,  0.7304],\n",
       "            [ 1.4307,  1.5532,  1.6933,  ...,  0.6429,  0.5728,  0.4153],\n",
       "            ...,\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],\n",
       "  \n",
       "           [[ 1.4722,  1.5768,  1.7685,  ...,  0.9668,  1.1237,  1.0714],\n",
       "            [ 1.5245,  1.6117,  1.7860,  ...,  0.9319,  1.0191,  0.9494],\n",
       "            [ 1.6465,  1.7685,  1.9080,  ...,  0.8622,  0.7925,  0.6356],\n",
       "            ...,\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]]])],\n",
       " [tensor([[[[-1.8439, -1.8439, -1.8439,  ..., -1.6042, -1.6042, -1.6213],\n",
       "            [-1.8439, -1.8439, -1.8439,  ..., -1.6213, -1.6213, -1.6213],\n",
       "            [-1.8439, -1.8439, -1.8439,  ..., -1.6555, -1.6555, -1.6555],\n",
       "            ...,\n",
       "            [-1.8439, -1.8439, -1.8439,  ..., -0.7822, -0.7822, -0.7993],\n",
       "            [-1.8439, -1.8439, -1.8439,  ..., -0.7993, -0.7822, -0.7993],\n",
       "            [-1.8439, -1.8439, -1.8439,  ..., -0.7993, -0.7822, -0.7993]],\n",
       "  \n",
       "           [[-1.7556, -1.7556, -1.7556,  ..., -1.5105, -1.5105, -1.5280],\n",
       "            [-1.7556, -1.7556, -1.7556,  ..., -1.5280, -1.5280, -1.5280],\n",
       "            [-1.7556, -1.7556, -1.7556,  ..., -1.5630, -1.5630, -1.5630],\n",
       "            ...,\n",
       "            [-1.7556, -1.7556, -1.7556,  ..., -0.6702, -0.6702, -0.6877],\n",
       "            [-1.7556, -1.7556, -1.7556,  ..., -0.6877, -0.6702, -0.6877],\n",
       "            [-1.7556, -1.7556, -1.7556,  ..., -0.6877, -0.6702, -0.6877]],\n",
       "  \n",
       "           [[-1.5256, -1.5256, -1.5256,  ..., -1.2816, -1.2816, -1.2990],\n",
       "            [-1.5256, -1.5256, -1.5256,  ..., -1.2990, -1.2990, -1.2990],\n",
       "            [-1.5256, -1.5256, -1.5256,  ..., -1.3339, -1.3339, -1.3339],\n",
       "            ...,\n",
       "            [-1.5256, -1.5256, -1.5256,  ..., -0.4450, -0.4450, -0.4624],\n",
       "            [-1.5256, -1.5256, -1.5256,  ..., -0.4624, -0.4450, -0.4624],\n",
       "            [-1.5256, -1.5256, -1.5256,  ..., -0.4624, -0.4450, -0.4624]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.0048, -0.9877, -0.9877,  ..., -1.2959, -1.2959, -1.2959],\n",
       "            [-1.0048, -0.9877, -0.9877,  ..., -1.2959, -1.2959, -1.2959],\n",
       "            [-1.0048, -0.9877, -0.9877,  ..., -1.2959, -1.2959, -1.2959],\n",
       "            ...,\n",
       "            [-0.9192, -0.9192, -0.9192,  ..., -1.2959, -1.2959, -1.2959],\n",
       "            [-0.9363, -0.9363, -0.9363,  ..., -1.2959, -1.2959, -1.2959],\n",
       "            [-0.9363, -0.9192, -0.9192,  ..., -1.2959, -1.2959, -1.2959]],\n",
       "  \n",
       "           [[-0.8978, -0.8803, -0.8803,  ..., -1.1954, -1.1954, -1.1954],\n",
       "            [-0.8978, -0.8803, -0.8803,  ..., -1.1954, -1.1954, -1.1954],\n",
       "            [-0.8978, -0.8803, -0.8803,  ..., -1.1954, -1.1954, -1.1954],\n",
       "            ...,\n",
       "            [-0.8102, -0.8102, -0.8102,  ..., -1.1954, -1.1954, -1.1954],\n",
       "            [-0.8277, -0.8277, -0.8277,  ..., -1.1954, -1.1954, -1.1954],\n",
       "            [-0.8277, -0.8102, -0.8102,  ..., -1.1954, -1.1954, -1.1954]],\n",
       "  \n",
       "           [[-0.6715, -0.6541, -0.6541,  ..., -0.9678, -0.9678, -0.9678],\n",
       "            [-0.6715, -0.6541, -0.6541,  ..., -0.9678, -0.9678, -0.9678],\n",
       "            [-0.6715, -0.6541, -0.6541,  ..., -0.9678, -0.9678, -0.9678],\n",
       "            ...,\n",
       "            [-0.5844, -0.5844, -0.5844,  ..., -0.9678, -0.9678, -0.9678],\n",
       "            [-0.6018, -0.6018, -0.6018,  ..., -0.9678, -0.9678, -0.9678],\n",
       "            [-0.6018, -0.5844, -0.5844,  ..., -0.9678, -0.9678, -0.9678]]]]),\n",
       "  tensor([[[[-0.0116, -1.8953, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-0.3712, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-1.7925, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            ...,\n",
       "            [ 0.8104,  0.8104,  0.7762,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 0.7762,  0.7762,  0.7591,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 0.7762,  0.8104,  0.7762,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[ 0.1176, -1.8081, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-0.2500, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-1.7031, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            ...,\n",
       "            [ 0.9580,  0.9580,  0.9230,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 0.9230,  0.9230,  0.9055,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 0.9230,  0.9580,  0.9230,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[ 0.3393, -1.5779, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-0.0267, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.4733, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            ...,\n",
       "            [ 1.1759,  1.1759,  1.1411,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 1.1411,  1.1411,  1.1237,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 1.1411,  1.1759,  1.1411,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.7925, -1.7925, -1.7925,  ..., -1.8439, -1.8610, -1.8610],\n",
       "            [-1.7925, -1.7925, -1.7925,  ..., -1.8439, -1.8610, -1.8610],\n",
       "            [-1.7925, -1.7925, -1.7925,  ..., -1.8439, -1.8610, -1.8610],\n",
       "            ...,\n",
       "            [-1.5870, -1.5870, -1.5699,  ..., -1.8610, -1.8610, -1.8610],\n",
       "            [-1.6042, -1.5870, -1.5699,  ..., -1.8610, -1.8610, -1.8610],\n",
       "            [-1.6042, -1.6042, -1.5870,  ..., -1.8610, -1.8610, -1.8610]],\n",
       "  \n",
       "           [[-1.7031, -1.7031, -1.7031,  ..., -1.7556, -1.7731, -1.7731],\n",
       "            [-1.7031, -1.7031, -1.7031,  ..., -1.7556, -1.7731, -1.7731],\n",
       "            [-1.7031, -1.7031, -1.7031,  ..., -1.7556, -1.7731, -1.7731],\n",
       "            ...,\n",
       "            [-1.4930, -1.4930, -1.4755,  ..., -1.7731, -1.7731, -1.7731],\n",
       "            [-1.5105, -1.4930, -1.4755,  ..., -1.7731, -1.7731, -1.7731],\n",
       "            [-1.5105, -1.5105, -1.4930,  ..., -1.7731, -1.7731, -1.7731]],\n",
       "  \n",
       "           [[-1.4733, -1.4733, -1.4733,  ..., -1.5256, -1.5430, -1.5430],\n",
       "            [-1.4733, -1.4733, -1.4733,  ..., -1.5256, -1.5430, -1.5430],\n",
       "            [-1.4733, -1.4733, -1.4733,  ..., -1.5256, -1.5430, -1.5430],\n",
       "            ...,\n",
       "            [-1.2641, -1.2641, -1.2467,  ..., -1.5430, -1.5430, -1.5430],\n",
       "            [-1.2816, -1.2641, -1.2467,  ..., -1.5430, -1.5430, -1.5430],\n",
       "            [-1.2816, -1.2816, -1.2641,  ..., -1.5430, -1.5430, -1.5430]]]]),\n",
       "  tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -0.0116, -0.2171, -0.5253],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  0.0056, -0.1657, -0.4568],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  0.0398, -0.1143, -0.3541],\n",
       "            ...,\n",
       "            [-1.5699, -1.4158, -1.3130,  ..., -1.9980, -1.9980, -2.0494],\n",
       "            [-1.5699, -1.4329, -1.3130,  ..., -1.9638, -1.9638, -2.0152],\n",
       "            [-1.5699, -1.4672, -1.3473,  ..., -1.9638, -2.0152, -1.9980]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ...,  0.1176, -0.0924, -0.4076],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  0.1352, -0.0399, -0.3375],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  0.1702,  0.0126, -0.2325],\n",
       "            ...,\n",
       "            [-1.4755, -1.3179, -1.2129,  ..., -1.9132, -1.9132, -1.9657],\n",
       "            [-1.4755, -1.3354, -1.2129,  ..., -1.8782, -1.8782, -1.9307],\n",
       "            [-1.4755, -1.3704, -1.2479,  ..., -1.8782, -1.9307, -1.9132]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8044,  ...,  0.3393,  0.1302, -0.1835],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  0.3568,  0.1825, -0.1138],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  0.3916,  0.2348, -0.0092],\n",
       "            ...,\n",
       "            [-1.2467, -1.0898, -0.9853,  ..., -1.6824, -1.6824, -1.7347],\n",
       "            [-1.2467, -1.1073, -0.9853,  ..., -1.6476, -1.6476, -1.6999],\n",
       "            [-1.2467, -1.1421, -1.0201,  ..., -1.6476, -1.6999, -1.6824]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.0494, -2.0494, -2.0494,  ..., -0.7479, -0.7479, -0.7479],\n",
       "            [-2.0494, -2.0494, -2.0494,  ..., -0.7479, -0.7479, -0.7479],\n",
       "            [-2.0494, -2.0494, -2.0494,  ..., -0.7479, -0.7479, -0.7479],\n",
       "            ...,\n",
       "            [-1.8610, -1.8782, -1.8782,  ..., -1.4329, -1.4500, -1.4500],\n",
       "            [-1.8610, -1.8782, -1.8782,  ..., -1.4158, -1.4329, -1.4329],\n",
       "            [-1.8782, -1.8782, -1.8782,  ..., -1.3987, -1.3987, -1.4158]],\n",
       "  \n",
       "           [[-1.9657, -1.9657, -1.9657,  ..., -0.6352, -0.6352, -0.6352],\n",
       "            [-1.9657, -1.9657, -1.9657,  ..., -0.6352, -0.6352, -0.6352],\n",
       "            [-1.9657, -1.9657, -1.9657,  ..., -0.6352, -0.6352, -0.6352],\n",
       "            ...,\n",
       "            [-1.7731, -1.7906, -1.7906,  ..., -1.3354, -1.3529, -1.3529],\n",
       "            [-1.7731, -1.7906, -1.7906,  ..., -1.3179, -1.3354, -1.3354],\n",
       "            [-1.7906, -1.7906, -1.7906,  ..., -1.3004, -1.3004, -1.3179]],\n",
       "  \n",
       "           [[-1.7347, -1.7347, -1.7347,  ..., -0.4101, -0.4101, -0.4101],\n",
       "            [-1.7347, -1.7347, -1.7347,  ..., -0.4101, -0.4101, -0.4101],\n",
       "            [-1.7347, -1.7347, -1.7347,  ..., -0.4101, -0.4101, -0.4101],\n",
       "            ...,\n",
       "            [-1.5430, -1.5604, -1.5604,  ..., -1.1073, -1.1247, -1.1247],\n",
       "            [-1.5430, -1.5604, -1.5604,  ..., -1.0898, -1.1073, -1.1073],\n",
       "            [-1.5604, -1.5604, -1.5604,  ..., -1.0724, -1.0724, -1.0898]]]]),\n",
       "  tensor([[[[ 0.3652,  0.4166,  0.5707,  ...,  1.4269,  1.4612,  1.4954],\n",
       "            [ 0.3652,  0.4166,  0.5707,  ...,  1.4269,  1.4612,  1.5125],\n",
       "            [ 0.3652,  0.4166,  0.5707,  ...,  1.4954,  1.5468,  1.6153],\n",
       "            ...,\n",
       "            [ 0.0227,  0.0227,  0.0227,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [ 0.0056,  0.0056,  0.0056,  ...,  1.9749,  1.9749,  2.0092],\n",
       "            [-0.0629, -0.0629, -0.0629,  ...,  0.0227,  0.0227,  0.0227]],\n",
       "  \n",
       "           [[ 0.5028,  0.5553,  0.7129,  ...,  1.5882,  1.6232,  1.6583],\n",
       "            [ 0.5028,  0.5553,  0.7129,  ...,  1.5882,  1.6232,  1.6758],\n",
       "            [ 0.5028,  0.5553,  0.7129,  ...,  1.6583,  1.7108,  1.7808],\n",
       "            ...,\n",
       "            [ 0.1527,  0.1527,  0.1527,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [ 0.1352,  0.1352,  0.1352,  ...,  2.1485,  2.1485,  2.1835],\n",
       "            [ 0.0651,  0.0651,  0.0651,  ...,  0.1527,  0.1527,  0.1527]],\n",
       "  \n",
       "           [[ 0.7228,  0.7751,  0.9319,  ...,  1.8034,  1.8383,  1.8731],\n",
       "            [ 0.7228,  0.7751,  0.9319,  ...,  1.8034,  1.8383,  1.8905],\n",
       "            [ 0.7228,  0.7751,  0.9319,  ...,  1.8731,  1.9254,  1.9951],\n",
       "            ...,\n",
       "            [ 0.3742,  0.3742,  0.3742,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [ 0.3568,  0.3568,  0.3568,  ...,  2.3611,  2.3611,  2.3960],\n",
       "            [ 0.2871,  0.2871,  0.2871,  ...,  0.3742,  0.3742,  0.3742]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1999, -0.0801, -0.1143,  ..., -0.3198, -0.5253, -0.6109],\n",
       "            [-0.1999, -0.1143, -0.1314,  ..., -0.2513, -0.4568, -0.5767],\n",
       "            [-0.3712, -0.2513, -0.0972,  ..., -0.1999, -0.3712, -0.4911],\n",
       "            ...,\n",
       "            [ 1.0159,  0.9817,  0.9817,  ..., -1.3130, -0.9192, -0.4054],\n",
       "            [ 0.9474,  0.9474,  0.9817,  ..., -1.1932, -0.6623, -0.1828],\n",
       "            [ 0.9817,  0.9817,  0.9817,  ..., -1.0390, -0.4739, -0.0116]],\n",
       "  \n",
       "           [[-0.0749,  0.0476,  0.0126,  ..., -0.1975, -0.4076, -0.4951],\n",
       "            [-0.0749,  0.0126, -0.0049,  ..., -0.1275, -0.3375, -0.4601],\n",
       "            [-0.2500, -0.1275,  0.0301,  ..., -0.0749, -0.2500, -0.3725],\n",
       "            ...,\n",
       "            [ 1.1681,  1.1331,  1.1331,  ..., -1.2129, -0.8102, -0.2850],\n",
       "            [ 1.0980,  1.0980,  1.1331,  ..., -1.0903, -0.5476, -0.0574],\n",
       "            [ 1.1331,  1.1331,  1.1331,  ..., -0.9328, -0.3550,  0.1176]],\n",
       "  \n",
       "           [[ 0.1476,  0.2696,  0.2348,  ...,  0.0256, -0.1835, -0.2707],\n",
       "            [ 0.1476,  0.2348,  0.2173,  ...,  0.0953, -0.1138, -0.2358],\n",
       "            [-0.0267,  0.0953,  0.2522,  ...,  0.1476, -0.0267, -0.1487],\n",
       "            ...,\n",
       "            [ 1.3851,  1.3502,  1.3502,  ..., -0.9853, -0.5844, -0.0615],\n",
       "            [ 1.3154,  1.3154,  1.3502,  ..., -0.8633, -0.3230,  0.1651],\n",
       "            [ 1.3502,  1.3502,  1.3502,  ..., -0.7064, -0.1312,  0.3393]]]]),\n",
       "  tensor([[[[ 0.0741,  0.0741,  0.0741,  ..., -0.8678, -0.8507, -0.8507],\n",
       "            [ 0.0741,  0.0741,  0.0741,  ..., -0.7993, -0.7993, -0.7993],\n",
       "            [ 0.0741,  0.0741,  0.0741,  ..., -0.7822, -0.7993, -0.7993],\n",
       "            ...,\n",
       "            [-1.5699, -1.5699, -1.5357,  ..., -1.1932, -1.1932, -1.2274],\n",
       "            [-1.5528, -1.5357, -1.5014,  ..., -1.2445, -1.2617, -1.2445],\n",
       "            [-1.4500, -1.5014, -1.5014,  ..., -1.3130, -1.3473, -1.3473]],\n",
       "  \n",
       "           [[ 0.2052,  0.2052,  0.2052,  ..., -0.7577, -0.7402, -0.7402],\n",
       "            [ 0.2052,  0.2052,  0.2052,  ..., -0.6877, -0.6877, -0.6877],\n",
       "            [ 0.2052,  0.2052,  0.2052,  ..., -0.6702, -0.6877, -0.6877],\n",
       "            ...,\n",
       "            [-1.4755, -1.4755, -1.4405,  ..., -1.0903, -1.0903, -1.1253],\n",
       "            [-1.4580, -1.4405, -1.4055,  ..., -1.1429, -1.1604, -1.1429],\n",
       "            [-1.3529, -1.4055, -1.4055,  ..., -1.2129, -1.2479, -1.2479]],\n",
       "  \n",
       "           [[ 0.4265,  0.4265,  0.4265,  ..., -0.5321, -0.5147, -0.5147],\n",
       "            [ 0.4265,  0.4265,  0.4265,  ..., -0.4624, -0.4624, -0.4624],\n",
       "            [ 0.4265,  0.4265,  0.4265,  ..., -0.4450, -0.4624, -0.4624],\n",
       "            ...,\n",
       "            [-1.2467, -1.2467, -1.2119,  ..., -0.8633, -0.8633, -0.8981],\n",
       "            [-1.2293, -1.2119, -1.1770,  ..., -0.9156, -0.9330, -0.9156],\n",
       "            [-1.1247, -1.1770, -1.1770,  ..., -0.9853, -1.0201, -1.0201]]],\n",
       "  \n",
       "  \n",
       "          [[[ 2.2489,  2.2489,  2.2489,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            ...,\n",
       "            [ 1.2557,  1.3927,  1.9064,  ...,  0.8618,  0.7077,  0.5364],\n",
       "            [ 1.5982,  1.7523,  2.1119,  ...,  0.9817,  0.7419,  0.5878],\n",
       "            [ 1.6324,  1.8379,  2.1119,  ...,  1.1872,  0.8276,  0.6906]],\n",
       "  \n",
       "           [[ 2.4286,  2.4286,  2.4286,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            ...,\n",
       "            [ 1.4132,  1.5532,  2.0784,  ...,  1.0105,  0.8529,  0.6779],\n",
       "            [ 1.7633,  1.9209,  2.2885,  ...,  1.1331,  0.8880,  0.7304],\n",
       "            [ 1.7983,  2.0084,  2.2885,  ...,  1.3431,  0.9755,  0.8354]],\n",
       "  \n",
       "           [[ 2.6400,  2.6400,  2.6400,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            ...,\n",
       "            [ 1.6291,  1.7685,  2.2914,  ...,  1.2282,  1.0714,  0.8971],\n",
       "            [ 1.9777,  2.1346,  2.5006,  ...,  1.3502,  1.1062,  0.9494],\n",
       "            [ 2.0125,  2.2217,  2.5006,  ...,  1.5594,  1.1934,  1.0539]]]]),\n",
       "  tensor([[[[-1.7754, -1.8610, -1.8610,  ..., -0.4397, -0.6623, -0.8678],\n",
       "            [-1.7069, -1.7240, -1.7412,  ..., -0.4226, -0.6623, -0.8678],\n",
       "            [-1.6384, -1.5528, -1.4672,  ..., -0.4397, -0.6281, -0.8678],\n",
       "            ...,\n",
       "            [ 1.8550,  1.8550,  1.8550,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 1.8550,  1.8550,  1.8550,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [ 1.8550,  1.8550,  1.8550,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-1.6856, -1.7731, -1.7731,  ..., -0.3200, -0.5476, -0.7577],\n",
       "            [-1.6155, -1.6331, -1.6506,  ..., -0.3025, -0.5476, -0.7577],\n",
       "            [-1.5455, -1.4580, -1.3704,  ..., -0.3200, -0.5126, -0.7577],\n",
       "            ...,\n",
       "            [ 2.0259,  2.0259,  2.0259,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 2.0259,  2.0259,  2.0259,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [ 2.0259,  2.0259,  2.0259,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[-1.4559, -1.5430, -1.5430,  ..., -0.0964, -0.3230, -0.5321],\n",
       "            [-1.3861, -1.4036, -1.4210,  ..., -0.0790, -0.3230, -0.5321],\n",
       "            [-1.3164, -1.2293, -1.1421,  ..., -0.0964, -0.2881, -0.5321],\n",
       "            ...,\n",
       "            [ 2.2391,  2.2391,  2.2391,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 2.2391,  2.2391,  2.2391,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 2.2391,  2.2391,  2.2391,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2171, -0.1828, -0.1828,  ...,  0.1768,  0.0912,  0.0398],\n",
       "            [-0.2342, -0.2171, -0.1999,  ...,  0.2282,  0.1426,  0.0912],\n",
       "            [-0.2684, -0.2513, -0.2513,  ...,  0.2796,  0.2111,  0.1597],\n",
       "            ...,\n",
       "            [ 1.4954,  1.4954,  1.4783,  ..., -0.9020, -0.8849, -0.8849],\n",
       "            [ 1.4954,  1.4783,  1.4612,  ..., -0.9192, -0.9192, -0.9534],\n",
       "            [ 1.4954,  1.4783,  1.4612,  ..., -0.9192, -0.9534, -1.0048]],\n",
       "  \n",
       "           [[-0.0924, -0.0574, -0.0574,  ...,  0.3102,  0.2227,  0.1702],\n",
       "            [-0.1099, -0.0924, -0.0749,  ...,  0.3627,  0.2752,  0.2227],\n",
       "            [-0.1450, -0.1275, -0.1275,  ...,  0.4153,  0.3452,  0.2927],\n",
       "            ...,\n",
       "            [ 1.6583,  1.6583,  1.6408,  ..., -0.7927, -0.7752, -0.7752],\n",
       "            [ 1.6583,  1.6408,  1.6232,  ..., -0.8102, -0.8102, -0.8452],\n",
       "            [ 1.6583,  1.6408,  1.6232,  ..., -0.8102, -0.8452, -0.8978]],\n",
       "  \n",
       "           [[ 0.1302,  0.1651,  0.1651,  ...,  0.5311,  0.4439,  0.3916],\n",
       "            [ 0.1128,  0.1302,  0.1476,  ...,  0.5834,  0.4962,  0.4439],\n",
       "            [ 0.0779,  0.0953,  0.0953,  ...,  0.6356,  0.5659,  0.5136],\n",
       "            ...,\n",
       "            [ 1.8731,  1.8731,  1.8557,  ..., -0.5670, -0.5495, -0.5495],\n",
       "            [ 1.8731,  1.8557,  1.8383,  ..., -0.5844, -0.5844, -0.6193],\n",
       "            [ 1.8731,  1.8557,  1.8383,  ..., -0.5844, -0.6193, -0.6715]]]]),\n",
       "  tensor([[[[-0.1314, -0.1486, -0.1486,  ..., -0.5253, -0.5424, -0.5424],\n",
       "            [-0.1486, -0.1657, -0.1657,  ..., -0.5253, -0.5253, -0.5424],\n",
       "            [-0.1486, -0.1657, -0.1828,  ..., -0.5253, -0.5253, -0.5253],\n",
       "            ...,\n",
       "            [-0.4911, -0.4911, -0.4911,  ...,  0.3994,  0.3994,  0.3994],\n",
       "            [-0.5082, -0.5082, -0.4911,  ...,  0.3994,  0.3994,  0.3994],\n",
       "            [-0.5253, -0.5082, -0.4911,  ...,  0.3994,  0.3994,  0.3994]],\n",
       "  \n",
       "           [[-0.0049, -0.0224, -0.0224,  ..., -0.4076, -0.4251, -0.4251],\n",
       "            [-0.0224, -0.0399, -0.0399,  ..., -0.4076, -0.4076, -0.4251],\n",
       "            [-0.0224, -0.0399, -0.0574,  ..., -0.4076, -0.4076, -0.4076],\n",
       "            ...,\n",
       "            [-0.3725, -0.3725, -0.3725,  ...,  0.5378,  0.5378,  0.5378],\n",
       "            [-0.3901, -0.3901, -0.3725,  ...,  0.5378,  0.5378,  0.5378],\n",
       "            [-0.4076, -0.3901, -0.3725,  ...,  0.5378,  0.5378,  0.5378]],\n",
       "  \n",
       "           [[ 0.2173,  0.1999,  0.1999,  ..., -0.1835, -0.2010, -0.2010],\n",
       "            [ 0.1999,  0.1825,  0.1825,  ..., -0.1835, -0.1835, -0.2010],\n",
       "            [ 0.1999,  0.1825,  0.1651,  ..., -0.1835, -0.1835, -0.1835],\n",
       "            ...,\n",
       "            [-0.1487, -0.1487, -0.1487,  ...,  0.7576,  0.7576,  0.7576],\n",
       "            [-0.1661, -0.1661, -0.1487,  ...,  0.7576,  0.7576,  0.7576],\n",
       "            [-0.1835, -0.1661, -0.1487,  ...,  0.7576,  0.7576,  0.7576]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.1179, -2.1179, -2.1179,  ..., -1.6213, -1.6042, -1.7583],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -1.6213, -1.5870, -1.6727],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -1.6555, -1.6555, -1.6727],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ..., -1.5280, -1.5105, -1.6681],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -1.5280, -1.4930, -1.5805],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -1.5630, -1.5630, -1.5805],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8044,  ..., -1.2990, -1.2816, -1.4384],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.2990, -1.2641, -1.3513],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.3339, -1.3339, -1.3513],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]),\n",
       "  tensor([[[[ 0.3994,  0.3652,  0.2282,  ..., -0.1828, -0.2171, -0.2513],\n",
       "            [ 0.3652,  0.2967,  0.1939,  ..., -0.1486, -0.2171, -0.2684],\n",
       "            [ 0.3138,  0.2282,  0.1597,  ..., -0.1657, -0.2171, -0.2513],\n",
       "            ...,\n",
       "            [-1.5014, -1.6727, -1.6384,  ...,  0.2282,  0.1426,  0.1083],\n",
       "            [-1.5014, -1.6384, -1.6384,  ...,  0.2282,  0.1597,  0.1254],\n",
       "            [-1.4843, -1.6213, -1.6555,  ...,  0.2624,  0.1768,  0.1083]],\n",
       "  \n",
       "           [[ 0.5378,  0.5028,  0.3627,  ..., -0.0574, -0.0924, -0.1275],\n",
       "            [ 0.5028,  0.4328,  0.3277,  ..., -0.0224, -0.0924, -0.1450],\n",
       "            [ 0.4503,  0.3627,  0.2927,  ..., -0.0399, -0.0924, -0.1275],\n",
       "            ...,\n",
       "            [-1.4055, -1.5805, -1.5455,  ...,  0.3627,  0.2752,  0.2402],\n",
       "            [-1.4055, -1.5455, -1.5455,  ...,  0.3627,  0.2927,  0.2577],\n",
       "            [-1.3880, -1.5280, -1.5630,  ...,  0.3978,  0.3102,  0.2402]],\n",
       "  \n",
       "           [[ 0.7576,  0.7228,  0.5834,  ...,  0.1651,  0.1302,  0.0953],\n",
       "            [ 0.7228,  0.6531,  0.5485,  ...,  0.1999,  0.1302,  0.0779],\n",
       "            [ 0.6705,  0.5834,  0.5136,  ...,  0.1825,  0.1302,  0.0953],\n",
       "            ...,\n",
       "            [-1.1770, -1.3513, -1.3164,  ...,  0.5834,  0.4962,  0.4614],\n",
       "            [-1.1770, -1.3164, -1.3164,  ...,  0.5834,  0.5136,  0.4788],\n",
       "            [-1.1596, -1.2990, -1.3339,  ...,  0.6182,  0.5311,  0.4614]]],\n",
       "  \n",
       "  \n",
       "          [[[ 2.2489,  2.2489,  2.2489,  ...,  1.4269,  1.2557,  1.1358],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  1.3070,  1.1187,  0.9988],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  1.0673,  0.8789,  0.7591],\n",
       "            ...,\n",
       "            [-0.1314, -0.1143, -0.0629,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-0.1999, -0.1486, -0.0458,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-0.2342, -0.1657, -0.0287,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[ 2.4286,  2.4286,  2.4286,  ...,  1.5882,  1.4132,  1.2906],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  1.4657,  1.2731,  1.1506],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  1.2206,  1.0280,  0.9055],\n",
       "            ...,\n",
       "            [-0.0049,  0.0126,  0.0651,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-0.0749, -0.0224,  0.0826,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-0.1099, -0.0399,  0.1001,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[ 2.6400,  2.6400,  2.6400,  ...,  1.8034,  1.6291,  1.5071],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  1.6814,  1.4897,  1.3677],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  1.4374,  1.2457,  1.1237],\n",
       "            ...,\n",
       "            [ 0.2173,  0.2348,  0.2871,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 0.1476,  0.1999,  0.3045,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [ 0.1128,  0.1825,  0.3219,  ..., -1.8044, -1.8044, -1.8044]]]]),\n",
       "  tensor([[[[ 0.8961,  0.8961,  0.8961,  ..., -0.1828, -0.1999, -0.2342],\n",
       "            [ 0.8789,  0.8961,  0.8961,  ..., -0.1657, -0.1999, -0.2342],\n",
       "            [ 0.8789,  0.8961,  0.8961,  ..., -0.1143, -0.1828, -0.2513],\n",
       "            ...,\n",
       "            [-0.7137, -0.7479, -0.7479,  ..., -1.1589, -1.0048, -0.9534],\n",
       "            [-0.6452, -0.6965, -0.6965,  ..., -1.2274, -1.1075, -1.0048],\n",
       "            [-0.5938, -0.6452, -0.6965,  ..., -1.3130, -1.1760, -1.0562]],\n",
       "  \n",
       "           [[ 1.0455,  1.0455,  1.0455,  ..., -0.0574, -0.0749, -0.1099],\n",
       "            [ 1.0280,  1.0455,  1.0455,  ..., -0.0399, -0.0749, -0.1099],\n",
       "            [ 1.0280,  1.0455,  1.0455,  ...,  0.0126, -0.0574, -0.1275],\n",
       "            ...,\n",
       "            [-0.6001, -0.6352, -0.6352,  ..., -1.0553, -0.8978, -0.8452],\n",
       "            [-0.5301, -0.5826, -0.5826,  ..., -1.1253, -1.0028, -0.8978],\n",
       "            [-0.4776, -0.5301, -0.5826,  ..., -1.2129, -1.0728, -0.9503]],\n",
       "  \n",
       "           [[ 1.2631,  1.2631,  1.2631,  ...,  0.1651,  0.1476,  0.1128],\n",
       "            [ 1.2457,  1.2631,  1.2631,  ...,  0.1825,  0.1476,  0.1128],\n",
       "            [ 1.2457,  1.2631,  1.2631,  ...,  0.2348,  0.1651,  0.0953],\n",
       "            ...,\n",
       "            [-0.3753, -0.4101, -0.4101,  ..., -0.8284, -0.6715, -0.6193],\n",
       "            [-0.3055, -0.3578, -0.3578,  ..., -0.8981, -0.7761, -0.6715],\n",
       "            [-0.2532, -0.3055, -0.3578,  ..., -0.9853, -0.8458, -0.7238]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.5528, -1.5528, -1.5699,  ..., -1.3815, -1.3987, -1.3987],\n",
       "            [-1.5528, -1.5528, -1.5699,  ..., -1.3815, -1.3815, -1.3987],\n",
       "            [-1.5528, -1.5699, -1.5699,  ..., -1.3815, -1.3987, -1.3987],\n",
       "            ...,\n",
       "            [-1.4500, -1.4329, -1.4329,  ..., -1.3815, -1.3987, -1.3987],\n",
       "            [-1.4500, -1.4500, -1.4329,  ..., -1.3987, -1.3987, -1.3987],\n",
       "            [-1.4500, -1.4500, -1.4329,  ..., -1.3987, -1.3987, -1.3987]],\n",
       "  \n",
       "           [[-1.4580, -1.4580, -1.4755,  ..., -1.2829, -1.3004, -1.3004],\n",
       "            [-1.4580, -1.4580, -1.4755,  ..., -1.2829, -1.2829, -1.3004],\n",
       "            [-1.4580, -1.4755, -1.4755,  ..., -1.2829, -1.3004, -1.3004],\n",
       "            ...,\n",
       "            [-1.3529, -1.3354, -1.3354,  ..., -1.2829, -1.3004, -1.3004],\n",
       "            [-1.3529, -1.3529, -1.3354,  ..., -1.3004, -1.3004, -1.3004],\n",
       "            [-1.3529, -1.3529, -1.3354,  ..., -1.3004, -1.3004, -1.3004]],\n",
       "  \n",
       "           [[-1.2293, -1.2293, -1.2467,  ..., -1.0550, -1.0724, -1.0724],\n",
       "            [-1.2293, -1.2293, -1.2467,  ..., -1.0550, -1.0550, -1.0724],\n",
       "            [-1.2293, -1.2467, -1.2467,  ..., -1.0550, -1.0724, -1.0724],\n",
       "            ...,\n",
       "            [-1.1247, -1.1073, -1.1073,  ..., -1.0550, -1.0724, -1.0724],\n",
       "            [-1.1247, -1.1247, -1.1073,  ..., -1.0724, -1.0724, -1.0724],\n",
       "            [-1.1247, -1.1247, -1.1073,  ..., -1.0724, -1.0724, -1.0724]]]]),\n",
       "  tensor([[[[ 1.9235,  1.9235,  1.9235,  ..., -0.0629, -0.1486, -0.1486],\n",
       "            [ 1.9235,  1.9235,  1.9235,  ..., -0.0116, -0.1143, -0.1828],\n",
       "            [ 1.9235,  1.9235,  1.9235,  ...,  0.0741, -0.0801, -0.1657],\n",
       "            ...,\n",
       "            [ 0.9303,  0.9303,  0.9132,  ...,  0.2796,  0.3994,  0.5878],\n",
       "            [ 1.0331,  1.0159,  1.0159,  ...,  0.3138,  0.4851,  0.6221],\n",
       "            [ 1.0502,  1.0502,  1.0502,  ...,  0.3823,  0.4508,  0.5878]],\n",
       "  \n",
       "           [[ 2.0959,  2.0959,  2.0959,  ...,  0.0651, -0.0224, -0.0224],\n",
       "            [ 2.0959,  2.0959,  2.0959,  ...,  0.1176,  0.0126, -0.0574],\n",
       "            [ 2.0959,  2.0959,  2.0959,  ...,  0.2052,  0.0476, -0.0399],\n",
       "            ...,\n",
       "            [ 1.0805,  1.0805,  1.0630,  ...,  0.4153,  0.5378,  0.7304],\n",
       "            [ 1.1856,  1.1681,  1.1681,  ...,  0.4503,  0.6254,  0.7654],\n",
       "            [ 1.2031,  1.2031,  1.2031,  ...,  0.5203,  0.5903,  0.7304]],\n",
       "  \n",
       "           [[ 2.3088,  2.3088,  2.3088,  ...,  0.2871,  0.1999,  0.1999],\n",
       "            [ 2.3088,  2.3088,  2.3088,  ...,  0.3393,  0.2348,  0.1651],\n",
       "            [ 2.3088,  2.3088,  2.3088,  ...,  0.4265,  0.2696,  0.1825],\n",
       "            ...,\n",
       "            [ 1.2980,  1.2980,  1.2805,  ...,  0.6356,  0.7576,  0.9494],\n",
       "            [ 1.4025,  1.3851,  1.3851,  ...,  0.6705,  0.8448,  0.9842],\n",
       "            [ 1.4200,  1.4200,  1.4200,  ...,  0.7402,  0.8099,  0.9494]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.5810,  1.6495,  1.6667,  ..., -0.1486, -0.1486, -0.1486],\n",
       "            [ 1.5297,  1.5639,  1.6667,  ..., -0.1486, -0.1486, -0.1486],\n",
       "            [ 1.4440,  1.5468,  1.6667,  ..., -0.1486, -0.1486, -0.1486],\n",
       "            ...,\n",
       "            [ 1.6667,  1.6667,  1.6667,  ...,  0.0569,  0.0398,  0.0227],\n",
       "            [ 1.6667,  1.6667,  1.6667,  ...,  0.0569,  0.0398,  0.0227],\n",
       "            [ 1.6667,  1.6667,  1.6667,  ...,  0.0398,  0.0227,  0.0227]],\n",
       "  \n",
       "           [[ 1.7458,  1.8158,  1.8333,  ..., -0.0224, -0.0224, -0.0224],\n",
       "            [ 1.6933,  1.7283,  1.8333,  ..., -0.0224, -0.0224, -0.0224],\n",
       "            [ 1.6057,  1.7108,  1.8333,  ..., -0.0224, -0.0224, -0.0224],\n",
       "            ...,\n",
       "            [ 1.8333,  1.8333,  1.8333,  ...,  0.1877,  0.1702,  0.1527],\n",
       "            [ 1.8333,  1.8333,  1.8333,  ...,  0.1877,  0.1702,  0.1527],\n",
       "            [ 1.8333,  1.8333,  1.8333,  ...,  0.1702,  0.1527,  0.1527]],\n",
       "  \n",
       "           [[ 1.9603,  2.0300,  2.0474,  ...,  0.1999,  0.1999,  0.1999],\n",
       "            [ 1.9080,  1.9428,  2.0474,  ...,  0.1999,  0.1999,  0.1999],\n",
       "            [ 1.8208,  1.9254,  2.0474,  ...,  0.1999,  0.1999,  0.1999],\n",
       "            ...,\n",
       "            [ 2.0474,  2.0474,  2.0474,  ...,  0.4091,  0.3916,  0.3742],\n",
       "            [ 2.0474,  2.0474,  2.0474,  ...,  0.4091,  0.3916,  0.3742],\n",
       "            [ 2.0474,  2.0474,  2.0474,  ...,  0.3916,  0.3742,  0.3742]]]]),\n",
       "  tensor([[[[-0.8335, -0.8678, -0.8335,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-0.7308, -0.7993, -0.7822,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [-0.7993, -0.8335, -0.8335,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            ...,\n",
       "            [ 1.2214,  1.2385,  0.8104,  ..., -0.3027, -0.2171, -0.3541],\n",
       "            [ 1.3242,  1.2214,  0.7591,  ..., -0.3027, -0.4568, -0.5596],\n",
       "            [ 1.3242,  1.0844,  0.7077,  ..., -0.3027, -0.4054, -0.4911]],\n",
       "  \n",
       "           [[-0.7227, -0.7577, -0.7227,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-0.6176, -0.6877, -0.6702,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [-0.6877, -0.7227, -0.7227,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            ...,\n",
       "            [ 1.3782,  1.3957,  0.9580,  ..., -0.1800, -0.0924, -0.2325],\n",
       "            [ 1.4832,  1.3782,  0.9055,  ..., -0.1800, -0.3375, -0.4426],\n",
       "            [ 1.4832,  1.2381,  0.8529,  ..., -0.1800, -0.2850, -0.3725]],\n",
       "  \n",
       "           [[-0.4973, -0.5321, -0.4973,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-0.3927, -0.4624, -0.4450,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [-0.4624, -0.4973, -0.4973,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            ...,\n",
       "            [ 1.5942,  1.6117,  1.1759,  ...,  0.0431,  0.1302, -0.0092],\n",
       "            [ 1.6988,  1.5942,  1.1237,  ...,  0.0431, -0.1138, -0.2184],\n",
       "            [ 1.6988,  1.4548,  1.0714,  ...,  0.0431, -0.0615, -0.1487]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1768,  0.2453,  0.2624,  ...,  0.7591,  0.6734,  0.6221],\n",
       "            [ 0.2282,  0.2796,  0.3138,  ...,  0.6734,  0.5707,  0.5707],\n",
       "            [ 0.2624,  0.2796,  0.3138,  ...,  0.5193,  0.3823,  0.4166],\n",
       "            ...,\n",
       "            [-0.9534, -0.9534, -1.0048,  ..., -0.9534, -0.9534, -0.9192],\n",
       "            [-0.9192, -0.9192, -1.0048,  ..., -0.9192, -0.9534, -0.9534],\n",
       "            [-0.9705, -1.0048, -1.0219,  ..., -0.9020, -0.9020, -0.9192]],\n",
       "  \n",
       "           [[ 0.3102,  0.3803,  0.3978,  ...,  0.9055,  0.8179,  0.7654],\n",
       "            [ 0.3627,  0.4153,  0.4503,  ...,  0.8179,  0.7129,  0.7129],\n",
       "            [ 0.3978,  0.4153,  0.4503,  ...,  0.6604,  0.5203,  0.5553],\n",
       "            ...,\n",
       "            [-0.8452, -0.8452, -0.8978,  ..., -0.8452, -0.8452, -0.8102],\n",
       "            [-0.8102, -0.8102, -0.8978,  ..., -0.8102, -0.8452, -0.8452],\n",
       "            [-0.8627, -0.8978, -0.9153,  ..., -0.7927, -0.7927, -0.8102]],\n",
       "  \n",
       "           [[ 0.5311,  0.6008,  0.6182,  ...,  1.1237,  1.0365,  0.9842],\n",
       "            [ 0.5834,  0.6356,  0.6705,  ...,  1.0365,  0.9319,  0.9319],\n",
       "            [ 0.6182,  0.6356,  0.6705,  ...,  0.8797,  0.7402,  0.7751],\n",
       "            ...,\n",
       "            [-0.6193, -0.6193, -0.6715,  ..., -0.6193, -0.6193, -0.5844],\n",
       "            [-0.5844, -0.5844, -0.6715,  ..., -0.5844, -0.6193, -0.6193],\n",
       "            [-0.6367, -0.6715, -0.6890,  ..., -0.5670, -0.5670, -0.5844]]]]),\n",
       "  tensor([[[[ 0.8961,  0.8961,  0.8961,  ..., -1.0390, -1.0390, -1.0390],\n",
       "            [ 0.8961,  0.8961,  0.8961,  ..., -1.0562, -1.0562, -1.0562],\n",
       "            [ 0.8961,  0.8961,  0.8961,  ..., -1.0562, -1.0562, -1.0733],\n",
       "            ...,\n",
       "            [-0.7822, -0.7650, -0.7650,  ..., -0.0287, -0.0629, -0.0801],\n",
       "            [-0.7308, -0.7137, -0.7137,  ..., -0.0116, -0.0287, -0.0458],\n",
       "            [-0.6965, -0.6794, -0.6794,  ...,  0.0056, -0.0116, -0.0287]],\n",
       "  \n",
       "           [[ 1.0455,  1.0455,  1.0455,  ..., -0.9328, -0.9328, -0.9328],\n",
       "            [ 1.0455,  1.0455,  1.0455,  ..., -0.9503, -0.9503, -0.9503],\n",
       "            [ 1.0455,  1.0455,  1.0455,  ..., -0.9503, -0.9503, -0.9678],\n",
       "            ...,\n",
       "            [-0.6702, -0.6527, -0.6527,  ...,  0.1001,  0.0651,  0.0476],\n",
       "            [-0.6176, -0.6001, -0.6001,  ...,  0.1176,  0.1001,  0.0826],\n",
       "            [-0.5826, -0.5651, -0.5651,  ...,  0.1352,  0.1176,  0.1001]],\n",
       "  \n",
       "           [[ 1.2631,  1.2631,  1.2631,  ..., -0.7064, -0.7064, -0.7064],\n",
       "            [ 1.2631,  1.2631,  1.2631,  ..., -0.7238, -0.7238, -0.7238],\n",
       "            [ 1.2631,  1.2631,  1.2631,  ..., -0.7238, -0.7238, -0.7413],\n",
       "            ...,\n",
       "            [-0.4450, -0.4275, -0.4275,  ...,  0.3219,  0.2871,  0.2696],\n",
       "            [-0.3927, -0.3753, -0.3753,  ...,  0.3393,  0.3219,  0.3045],\n",
       "            [-0.3578, -0.3404, -0.3404,  ...,  0.3568,  0.3393,  0.3219]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0801, -0.0287,  0.0398,  ...,  0.2624,  0.2111,  0.1597],\n",
       "            [-0.0629, -0.0116,  0.0569,  ...,  0.2111,  0.1597,  0.1254],\n",
       "            [-0.0458,  0.0056,  0.0569,  ...,  0.1768,  0.1254,  0.0741],\n",
       "            ...,\n",
       "            [-1.4672, -1.4158, -1.3473,  ..., -0.5424, -0.5082, -0.5253],\n",
       "            [-1.4672, -1.4329, -1.4329,  ..., -0.5424, -0.4226, -0.4054],\n",
       "            [-1.4672, -1.4158, -1.4158,  ..., -0.5082, -0.4397, -0.3883]],\n",
       "  \n",
       "           [[ 0.0476,  0.1001,  0.1702,  ...,  0.3978,  0.3452,  0.2927],\n",
       "            [ 0.0651,  0.1176,  0.1877,  ...,  0.3452,  0.2927,  0.2577],\n",
       "            [ 0.0826,  0.1352,  0.1877,  ...,  0.3102,  0.2577,  0.2052],\n",
       "            ...,\n",
       "            [-1.3704, -1.3179, -1.2479,  ..., -0.4251, -0.3901, -0.4076],\n",
       "            [-1.3704, -1.3354, -1.3354,  ..., -0.4251, -0.3025, -0.2850],\n",
       "            [-1.3704, -1.3179, -1.3179,  ..., -0.3901, -0.3200, -0.2675]],\n",
       "  \n",
       "           [[ 0.2696,  0.3219,  0.3916,  ...,  0.6182,  0.5659,  0.5136],\n",
       "            [ 0.2871,  0.3393,  0.4091,  ...,  0.5659,  0.5136,  0.4788],\n",
       "            [ 0.3045,  0.3568,  0.4091,  ...,  0.5311,  0.4788,  0.4265],\n",
       "            ...,\n",
       "            [-1.1421, -1.0898, -1.0201,  ..., -0.2010, -0.1661, -0.1835],\n",
       "            [-1.1421, -1.1073, -1.1073,  ..., -0.2010, -0.0790, -0.0615],\n",
       "            [-1.1421, -1.0898, -1.0898,  ..., -0.1661, -0.0964, -0.0441]]]])],\n",
       " [tensor([[[[-0.2513, -0.3027, -0.3369,  ..., -0.9363, -0.9705, -0.9877],\n",
       "            [-0.2856, -0.3198, -0.3541,  ..., -0.9192, -0.9534, -0.9705],\n",
       "            [-0.3198, -0.3198, -0.3369,  ..., -0.8678, -0.8849, -0.9020],\n",
       "            ...,\n",
       "            [ 0.4166,  0.4337,  0.4679,  ..., -0.5253, -0.5767, -0.6281],\n",
       "            [ 0.4337,  0.4508,  0.4851,  ..., -0.5424, -0.6109, -0.6623],\n",
       "            [ 0.4337,  0.4679,  0.5022,  ..., -0.5596, -0.6281, -0.6794]],\n",
       "  \n",
       "           [[-0.1275, -0.1800, -0.2150,  ..., -0.8277, -0.8627, -0.8803],\n",
       "            [-0.1625, -0.1975, -0.2325,  ..., -0.8102, -0.8452, -0.8627],\n",
       "            [-0.1975, -0.1975, -0.2150,  ..., -0.7577, -0.7752, -0.7927],\n",
       "            ...,\n",
       "            [ 0.5553,  0.5728,  0.6078,  ..., -0.4076, -0.4601, -0.5126],\n",
       "            [ 0.5728,  0.5903,  0.6254,  ..., -0.4251, -0.4951, -0.5476],\n",
       "            [ 0.5728,  0.6078,  0.6429,  ..., -0.4426, -0.5126, -0.5651]],\n",
       "  \n",
       "           [[ 0.0953,  0.0431,  0.0082,  ..., -0.6018, -0.6367, -0.6541],\n",
       "            [ 0.0605,  0.0256, -0.0092,  ..., -0.5844, -0.6193, -0.6367],\n",
       "            [ 0.0256,  0.0256,  0.0082,  ..., -0.5321, -0.5495, -0.5670],\n",
       "            ...,\n",
       "            [ 0.7751,  0.7925,  0.8274,  ..., -0.1835, -0.2358, -0.2881],\n",
       "            [ 0.7925,  0.8099,  0.8448,  ..., -0.2010, -0.2707, -0.3230],\n",
       "            [ 0.7925,  0.8274,  0.8622,  ..., -0.2184, -0.2881, -0.3404]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1008, -2.1008],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1008, -2.1008],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1008, -2.1008],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -1.8097, -1.9809, -2.0323],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -1.8610, -2.0323, -2.0323],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -1.8953, -1.9980, -2.0494]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0182, -2.0182],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0182, -2.0182],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0182, -2.0182],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -1.7206, -1.8957, -1.9482],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -1.7731, -1.9482, -1.9482],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -1.8081, -1.9132, -1.9657]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.7870, -1.7870],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.7870, -1.7870],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.7870, -1.7870],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.4907, -1.6650, -1.7173],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.5430, -1.7173, -1.7173],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.5779, -1.6824, -1.7347]]]]),\n",
       "  tensor([[[[-0.6452, -0.6965, -0.7650,  ..., -0.9534, -0.7822, -1.2445],\n",
       "            [-0.6109, -0.6965, -0.7650,  ..., -1.2617, -1.2617, -1.2788],\n",
       "            [-0.6794, -0.7137, -0.7308,  ..., -1.2617, -1.2788, -1.2788],\n",
       "            ...,\n",
       "            [-0.0972, -0.0972, -0.0972,  ..., -0.5596, -0.5767, -0.6109],\n",
       "            [-0.1314, -0.1314, -0.1143,  ..., -0.5596, -0.5767, -0.5938],\n",
       "            [-0.1314, -0.1314, -0.1143,  ..., -0.5596, -0.5596, -0.5767]],\n",
       "  \n",
       "           [[-0.5301, -0.5826, -0.6527,  ..., -0.8452, -0.6702, -1.1429],\n",
       "            [-0.4951, -0.5826, -0.6527,  ..., -1.1604, -1.1604, -1.1779],\n",
       "            [-0.5651, -0.6001, -0.6176,  ..., -1.1604, -1.1779, -1.1779],\n",
       "            ...,\n",
       "            [ 0.0301,  0.0301,  0.0301,  ..., -0.4426, -0.4601, -0.4951],\n",
       "            [-0.0049, -0.0049,  0.0126,  ..., -0.4426, -0.4601, -0.4776],\n",
       "            [-0.0049, -0.0049,  0.0126,  ..., -0.4426, -0.4426, -0.4601]],\n",
       "  \n",
       "           [[-0.3055, -0.3578, -0.4275,  ..., -0.6193, -0.4450, -0.9156],\n",
       "            [-0.2707, -0.3578, -0.4275,  ..., -0.9330, -0.9330, -0.9504],\n",
       "            [-0.3404, -0.3753, -0.3927,  ..., -0.9330, -0.9504, -0.9504],\n",
       "            ...,\n",
       "            [ 0.2522,  0.2522,  0.2522,  ..., -0.2184, -0.2358, -0.2707],\n",
       "            [ 0.2173,  0.2173,  0.2348,  ..., -0.2184, -0.2358, -0.2532],\n",
       "            [ 0.2173,  0.2173,  0.2348,  ..., -0.2184, -0.2184, -0.2358]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.4672, -1.4672, -1.4672,  ..., -1.5870, -1.5870, -1.5870],\n",
       "            [-1.4672, -1.4672, -1.4672,  ..., -1.5870, -1.5870, -1.5870],\n",
       "            [-1.4672, -1.4843, -1.4843,  ..., -1.5870, -1.5870, -1.5870],\n",
       "            ...,\n",
       "            [-1.4158, -1.3987, -1.3987,  ..., -1.3987, -1.3987, -1.3987],\n",
       "            [-1.3987, -1.3987, -1.3987,  ..., -1.3987, -1.3987, -1.3987],\n",
       "            [-1.3987, -1.3987, -1.3987,  ..., -1.3987, -1.3987, -1.3815]],\n",
       "  \n",
       "           [[-1.3704, -1.3704, -1.3704,  ..., -1.4930, -1.4930, -1.4930],\n",
       "            [-1.3704, -1.3704, -1.3704,  ..., -1.4930, -1.4930, -1.4930],\n",
       "            [-1.3704, -1.3880, -1.3880,  ..., -1.4930, -1.4930, -1.4930],\n",
       "            ...,\n",
       "            [-1.3179, -1.3004, -1.3004,  ..., -1.3004, -1.3004, -1.3004],\n",
       "            [-1.3004, -1.3004, -1.3004,  ..., -1.3004, -1.3004, -1.3004],\n",
       "            [-1.3004, -1.3004, -1.3004,  ..., -1.3004, -1.3004, -1.2829]],\n",
       "  \n",
       "           [[-1.1421, -1.1421, -1.1421,  ..., -1.2641, -1.2641, -1.2641],\n",
       "            [-1.1421, -1.1421, -1.1421,  ..., -1.2641, -1.2641, -1.2641],\n",
       "            [-1.1421, -1.1596, -1.1596,  ..., -1.2641, -1.2641, -1.2641],\n",
       "            ...,\n",
       "            [-1.0898, -1.0724, -1.0724,  ..., -1.0724, -1.0724, -1.0724],\n",
       "            [-1.0724, -1.0724, -1.0724,  ..., -1.0724, -1.0724, -1.0724],\n",
       "            [-1.0724, -1.0724, -1.0724,  ..., -1.0724, -1.0724, -1.0550]]]]),\n",
       "  tensor([[[[-1.5185, -1.5699, -1.6213,  ..., -1.6898, -1.7925, -1.8439],\n",
       "            [-1.5357, -1.5699, -1.5699,  ..., -1.6898, -1.7925, -1.8439],\n",
       "            [-1.5357, -1.5699, -1.5185,  ..., -1.6898, -1.7754, -1.8268],\n",
       "            ...,\n",
       "            [-1.0733, -1.0733, -1.0562,  ..., -0.9877, -0.9877, -1.0048],\n",
       "            [-1.0562, -1.0562, -1.0562,  ..., -0.9877, -0.9877, -0.9877],\n",
       "            [-1.0733, -1.0733, -1.0733,  ..., -0.9877, -0.9877, -0.9877]],\n",
       "  \n",
       "           [[-1.4230, -1.4755, -1.5280,  ..., -1.5980, -1.7031, -1.7556],\n",
       "            [-1.4405, -1.4755, -1.4755,  ..., -1.5980, -1.7031, -1.7556],\n",
       "            [-1.4405, -1.4755, -1.4230,  ..., -1.5980, -1.6856, -1.7381],\n",
       "            ...,\n",
       "            [-0.9678, -0.9678, -0.9503,  ..., -0.8803, -0.8803, -0.8978],\n",
       "            [-0.9503, -0.9503, -0.9503,  ..., -0.8803, -0.8803, -0.8803],\n",
       "            [-0.9678, -0.9678, -0.9678,  ..., -0.8803, -0.8803, -0.8803]],\n",
       "  \n",
       "           [[-1.1944, -1.2467, -1.2990,  ..., -1.3687, -1.4733, -1.5256],\n",
       "            [-1.2119, -1.2467, -1.2467,  ..., -1.3687, -1.4733, -1.5256],\n",
       "            [-1.2119, -1.2467, -1.1944,  ..., -1.3687, -1.4559, -1.5081],\n",
       "            ...,\n",
       "            [-0.7413, -0.7413, -0.7238,  ..., -0.6541, -0.6541, -0.6715],\n",
       "            [-0.7238, -0.7238, -0.7238,  ..., -0.6541, -0.6541, -0.6541],\n",
       "            [-0.7413, -0.7413, -0.7413,  ..., -0.6541, -0.6541, -0.6541]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.0562, -1.0562, -1.0733,  ..., -0.9192, -0.8849, -0.8335],\n",
       "            [-1.0562, -1.0562, -1.0562,  ..., -0.9192, -0.9020, -0.8335],\n",
       "            [-1.0562, -1.0562, -1.0733,  ..., -0.9192, -0.9020, -0.8335],\n",
       "            ...,\n",
       "            [-0.7822, -0.7822, -0.7822,  ..., -0.9877, -0.9877, -1.0219],\n",
       "            [-0.7822, -0.7822, -0.7822,  ..., -0.9877, -1.0048, -1.0219],\n",
       "            [-0.8335, -0.8335, -0.8335,  ..., -1.0219, -1.0390, -1.0562]],\n",
       "  \n",
       "           [[-0.9503, -0.9503, -0.9678,  ..., -0.8102, -0.7752, -0.7227],\n",
       "            [-0.9503, -0.9503, -0.9503,  ..., -0.8102, -0.7927, -0.7227],\n",
       "            [-0.9503, -0.9503, -0.9678,  ..., -0.8102, -0.7927, -0.7227],\n",
       "            ...,\n",
       "            [-0.6702, -0.6702, -0.6702,  ..., -0.8803, -0.8803, -0.9153],\n",
       "            [-0.6702, -0.6702, -0.6702,  ..., -0.8803, -0.8978, -0.9153],\n",
       "            [-0.7227, -0.7227, -0.7227,  ..., -0.9153, -0.9328, -0.9503]],\n",
       "  \n",
       "           [[-0.7238, -0.7238, -0.7413,  ..., -0.5844, -0.5495, -0.4973],\n",
       "            [-0.7238, -0.7238, -0.7238,  ..., -0.5844, -0.5670, -0.4973],\n",
       "            [-0.7238, -0.7238, -0.7413,  ..., -0.5844, -0.5670, -0.4973],\n",
       "            ...,\n",
       "            [-0.4450, -0.4450, -0.4450,  ..., -0.6541, -0.6541, -0.6890],\n",
       "            [-0.4450, -0.4450, -0.4450,  ..., -0.6541, -0.6715, -0.6890],\n",
       "            [-0.4973, -0.4973, -0.4973,  ..., -0.6890, -0.7064, -0.7238]]]]),\n",
       "  tensor([[[[-1.3130, -1.1589, -1.1589,  ...,  0.0056, -0.4054, -0.5253],\n",
       "            [-1.2103, -1.1589, -1.2445,  ...,  0.1939, -0.0801, -0.3712],\n",
       "            [-1.1418, -1.2445, -1.3473,  ...,  0.2282,  0.1768,  0.0398],\n",
       "            ...,\n",
       "            [-1.3473, -1.2617, -1.1075,  ..., -1.8953, -1.7925, -1.7412],\n",
       "            [-1.3130, -1.2274, -1.1075,  ..., -1.8610, -1.7925, -1.7583],\n",
       "            [-1.2274, -1.1932, -1.0904,  ..., -1.8097, -1.7240, -1.7240]],\n",
       "  \n",
       "           [[-1.2129, -1.0553, -1.0553,  ...,  0.1352, -0.2850, -0.4076],\n",
       "            [-1.1078, -1.0553, -1.1429,  ...,  0.3277,  0.0476, -0.2500],\n",
       "            [-1.0378, -1.1429, -1.2479,  ...,  0.3627,  0.3102,  0.1702],\n",
       "            ...,\n",
       "            [-1.2479, -1.1604, -1.0028,  ..., -1.8081, -1.7031, -1.6506],\n",
       "            [-1.2129, -1.1253, -1.0028,  ..., -1.7731, -1.7031, -1.6681],\n",
       "            [-1.1253, -1.0903, -0.9853,  ..., -1.7206, -1.6331, -1.6331]],\n",
       "  \n",
       "           [[-0.9853, -0.8284, -0.8284,  ...,  0.3568, -0.0615, -0.1835],\n",
       "            [-0.8807, -0.8284, -0.9156,  ...,  0.5485,  0.2696, -0.0267],\n",
       "            [-0.8110, -0.9156, -1.0201,  ...,  0.5834,  0.5311,  0.3916],\n",
       "            ...,\n",
       "            [-1.0201, -0.9330, -0.7761,  ..., -1.5779, -1.4733, -1.4210],\n",
       "            [-0.9853, -0.8981, -0.7761,  ..., -1.5430, -1.4733, -1.4384],\n",
       "            [-0.8981, -0.8633, -0.7587,  ..., -1.4907, -1.4036, -1.4036]]],\n",
       "  \n",
       "  \n",
       "          [[[ 2.2489,  2.2489,  2.2489,  ..., -0.0801,  0.1426,  0.3138],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ..., -0.1314,  0.0741,  0.2282],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ..., -0.2342, -0.0287,  0.0912],\n",
       "            ...,\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489]],\n",
       "  \n",
       "           [[ 2.4286,  2.4286,  2.4286,  ...,  0.0476,  0.2752,  0.4503],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ..., -0.0049,  0.2052,  0.3627],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ..., -0.1099,  0.1001,  0.2227],\n",
       "            ...,\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],\n",
       "  \n",
       "           [[ 2.6400,  2.6400,  2.6400,  ...,  0.2696,  0.4962,  0.6705],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  0.2173,  0.4265,  0.5834],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  0.1128,  0.3219,  0.4439],\n",
       "            ...,\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]]]),\n",
       "  tensor([[[[-1.0048, -0.6109, -0.5596,  ..., -0.9020, -0.8678, -0.8164],\n",
       "            [-1.0048, -0.7137, -0.6965,  ..., -0.9020, -0.8678, -0.8335],\n",
       "            [-1.0219, -0.7822, -0.7308,  ..., -0.9020, -0.8678, -0.8335],\n",
       "            ...,\n",
       "            [-0.9020, -0.7137, -0.6965,  ..., -0.4568, -0.4568, -0.4568],\n",
       "            [-0.8849, -0.7137, -0.6965,  ..., -0.4568, -0.4397, -0.4568],\n",
       "            [-0.8849, -0.7137, -0.6965,  ..., -0.4397, -0.4397, -0.4397]],\n",
       "  \n",
       "           [[-0.8978, -0.4951, -0.4426,  ..., -0.7927, -0.7577, -0.7052],\n",
       "            [-0.8978, -0.6001, -0.5826,  ..., -0.7927, -0.7577, -0.7227],\n",
       "            [-0.9153, -0.6702, -0.6176,  ..., -0.7927, -0.7577, -0.7227],\n",
       "            ...,\n",
       "            [-0.7927, -0.6001, -0.5826,  ..., -0.3375, -0.3375, -0.3375],\n",
       "            [-0.7752, -0.6001, -0.5826,  ..., -0.3375, -0.3200, -0.3375],\n",
       "            [-0.7752, -0.6001, -0.5826,  ..., -0.3200, -0.3200, -0.3200]],\n",
       "  \n",
       "           [[-0.6715, -0.2707, -0.2184,  ..., -0.5670, -0.5321, -0.4798],\n",
       "            [-0.6715, -0.3753, -0.3578,  ..., -0.5670, -0.5321, -0.4973],\n",
       "            [-0.6890, -0.4450, -0.3927,  ..., -0.5670, -0.5321, -0.4973],\n",
       "            ...,\n",
       "            [-0.5670, -0.3753, -0.3578,  ..., -0.1138, -0.1138, -0.1138],\n",
       "            [-0.5495, -0.3753, -0.3578,  ..., -0.1138, -0.0964, -0.1138],\n",
       "            [-0.5495, -0.3753, -0.3578,  ..., -0.0964, -0.0964, -0.0964]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.6906,  0.5878,  0.5022,  ...,  0.9988,  1.0331,  1.0844],\n",
       "            [ 0.4508,  0.3823,  0.4508,  ...,  1.1358,  1.1015,  1.0502],\n",
       "            [ 0.3138,  0.3138,  0.3652,  ...,  1.1015,  1.0331,  0.9988],\n",
       "            ...,\n",
       "            [ 0.5364,  0.7419,  0.7419,  ...,  0.8104,  0.8618,  0.8618],\n",
       "            [ 0.7419,  0.9132,  0.8276,  ...,  0.8276,  0.8789,  0.9132],\n",
       "            [ 0.9132,  0.9988,  0.9646,  ...,  0.7762,  0.8618,  0.8789]],\n",
       "  \n",
       "           [[ 0.8354,  0.7304,  0.6429,  ...,  1.1506,  1.1856,  1.2381],\n",
       "            [ 0.5903,  0.5203,  0.5903,  ...,  1.2906,  1.2556,  1.2031],\n",
       "            [ 0.4503,  0.4503,  0.5028,  ...,  1.2556,  1.1856,  1.1506],\n",
       "            ...,\n",
       "            [ 0.6779,  0.8880,  0.8880,  ...,  0.9580,  1.0105,  1.0105],\n",
       "            [ 0.8880,  1.0630,  0.9755,  ...,  0.9755,  1.0280,  1.0630],\n",
       "            [ 1.0630,  1.1506,  1.1155,  ...,  0.9230,  1.0105,  1.0280]],\n",
       "  \n",
       "           [[ 1.0539,  0.9494,  0.8622,  ...,  1.3677,  1.4025,  1.4548],\n",
       "            [ 0.8099,  0.7402,  0.8099,  ...,  1.5071,  1.4722,  1.4200],\n",
       "            [ 0.6705,  0.6705,  0.7228,  ...,  1.4722,  1.4025,  1.3677],\n",
       "            ...,\n",
       "            [ 0.8971,  1.1062,  1.1062,  ...,  1.1759,  1.2282,  1.2282],\n",
       "            [ 1.1062,  1.2805,  1.1934,  ...,  1.1934,  1.2457,  1.2805],\n",
       "            [ 1.2805,  1.3677,  1.3328,  ...,  1.1411,  1.2282,  1.2457]]]]),\n",
       "  tensor([[[[-2.1179, -2.1179, -2.1179,  ...,  0.4166,  0.4166,  0.3138],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  0.5878,  0.4166,  0.3994],\n",
       "            [-2.1179, -2.1179, -2.1179,  ...,  0.6906,  0.3994,  0.4166],\n",
       "            ...,\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ...,  0.5553,  0.5553,  0.4503],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  0.7304,  0.5553,  0.5378],\n",
       "            [-2.0357, -2.0357, -2.0357,  ...,  0.8354,  0.5378,  0.5553],\n",
       "            ...,\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8044,  ...,  0.7751,  0.7751,  0.6705],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  0.9494,  0.7751,  0.7576],\n",
       "            [-1.8044, -1.8044, -1.8044,  ...,  1.0539,  0.7576,  0.7751],\n",
       "            ...,\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.4158, -1.4158, -1.4500,  ...,  1.3584,  1.2899,  1.3070],\n",
       "            [-1.3130, -1.3644, -1.4158,  ...,  1.3584,  1.3242,  1.3070],\n",
       "            [-1.3987, -1.4843, -1.5357,  ...,  1.3755,  1.3584,  1.3755],\n",
       "            ...,\n",
       "            [ 1.3242,  1.3242,  1.3070,  ...,  1.8550,  1.8550,  1.8550],\n",
       "            [ 1.2899,  1.2557,  1.2385,  ...,  1.8550,  1.8550,  1.8550],\n",
       "            [ 1.2557,  1.2557,  1.2557,  ...,  1.8550,  1.8550,  1.8550]],\n",
       "  \n",
       "           [[-1.3179, -1.3179, -1.3529,  ...,  1.5182,  1.4482,  1.4657],\n",
       "            [-1.2129, -1.2654, -1.3179,  ...,  1.5182,  1.4832,  1.4657],\n",
       "            [-1.3004, -1.3880, -1.4405,  ...,  1.5357,  1.5182,  1.5357],\n",
       "            ...,\n",
       "            [ 1.4832,  1.4832,  1.4657,  ...,  2.0259,  2.0259,  2.0259],\n",
       "            [ 1.4482,  1.4132,  1.3957,  ...,  2.0259,  2.0259,  2.0259],\n",
       "            [ 1.4132,  1.4132,  1.4132,  ...,  2.0259,  2.0259,  2.0259]],\n",
       "  \n",
       "           [[-1.0898, -1.0898, -1.1247,  ...,  1.7337,  1.6640,  1.6814],\n",
       "            [-0.9853, -1.0376, -1.0898,  ...,  1.7337,  1.6988,  1.6814],\n",
       "            [-1.0724, -1.1596, -1.2119,  ...,  1.7511,  1.7337,  1.7511],\n",
       "            ...,\n",
       "            [ 1.6988,  1.6988,  1.6814,  ...,  2.2391,  2.2391,  2.2391],\n",
       "            [ 1.6640,  1.6291,  1.6117,  ...,  2.2391,  2.2391,  2.2391],\n",
       "            [ 1.6291,  1.6291,  1.6291,  ...,  2.2391,  2.2391,  2.2391]]]]),\n",
       "  tensor([[[[-0.5938, -0.6281, -0.6794,  ...,  0.7077,  0.8104,  0.8789],\n",
       "            [-0.5424, -0.5938, -0.6623,  ...,  0.7248,  0.8276,  0.8789],\n",
       "            [-0.5424, -0.5596, -0.5938,  ...,  0.7419,  0.8276,  0.8789],\n",
       "            ...,\n",
       "            [ 0.5364,  0.5536,  0.6049,  ...,  1.8208,  1.8379,  1.8208],\n",
       "            [ 0.5364,  0.5707,  0.6221,  ...,  1.8208,  1.8208,  1.8208],\n",
       "            [ 0.5193,  0.5707,  0.6049,  ...,  1.8037,  1.8037,  1.8208]],\n",
       "  \n",
       "           [[-0.4776, -0.5126, -0.5651,  ...,  0.8529,  0.9580,  1.0280],\n",
       "            [-0.4251, -0.4776, -0.5476,  ...,  0.8704,  0.9755,  1.0280],\n",
       "            [-0.4251, -0.4426, -0.4776,  ...,  0.8880,  0.9755,  1.0280],\n",
       "            ...,\n",
       "            [ 0.6779,  0.6954,  0.7479,  ...,  1.9909,  2.0084,  1.9909],\n",
       "            [ 0.6779,  0.7129,  0.7654,  ...,  1.9909,  1.9909,  1.9909],\n",
       "            [ 0.6604,  0.7129,  0.7479,  ...,  1.9734,  1.9734,  1.9909]],\n",
       "  \n",
       "           [[-0.2532, -0.2881, -0.3404,  ...,  1.0714,  1.1759,  1.2457],\n",
       "            [-0.2010, -0.2532, -0.3230,  ...,  1.0888,  1.1934,  1.2457],\n",
       "            [-0.2010, -0.2184, -0.2532,  ...,  1.1062,  1.1934,  1.2457],\n",
       "            ...,\n",
       "            [ 0.8971,  0.9145,  0.9668,  ...,  2.2043,  2.2217,  2.2043],\n",
       "            [ 0.8971,  0.9319,  0.9842,  ...,  2.2043,  2.2043,  2.2043],\n",
       "            [ 0.8797,  0.9319,  0.9668,  ...,  2.1868,  2.1868,  2.2043]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.3584,  1.5297,  1.6495,  ..., -0.3027, -0.4054, -0.7479],\n",
       "            [ 1.3755,  1.5468,  1.6495,  ..., -0.2342, -0.2856, -0.4397],\n",
       "            [ 1.3755,  1.5639,  1.6495,  ..., -0.5596, -0.5424, -0.5253],\n",
       "            ...,\n",
       "            [ 1.0159,  1.1187,  1.2899,  ...,  2.0777,  2.0605,  2.0434],\n",
       "            [ 0.9988,  1.1187,  1.2899,  ...,  2.0948,  2.0605,  2.0434],\n",
       "            [ 0.9817,  1.1187,  1.2899,  ...,  2.0948,  2.0605,  2.0434]],\n",
       "  \n",
       "           [[ 1.5182,  1.6933,  1.8158,  ..., -0.1800, -0.2850, -0.6352],\n",
       "            [ 1.5357,  1.7108,  1.8158,  ..., -0.1099, -0.1625, -0.3200],\n",
       "            [ 1.5357,  1.7283,  1.8158,  ..., -0.4426, -0.4251, -0.4076],\n",
       "            ...,\n",
       "            [ 1.1681,  1.2731,  1.4482,  ...,  2.2535,  2.2360,  2.2185],\n",
       "            [ 1.1506,  1.2731,  1.4482,  ...,  2.2710,  2.2360,  2.2185],\n",
       "            [ 1.1331,  1.2731,  1.4482,  ...,  2.2710,  2.2360,  2.2185]],\n",
       "  \n",
       "           [[ 1.7337,  1.9080,  2.0300,  ...,  0.0431, -0.0615, -0.4101],\n",
       "            [ 1.7511,  1.9254,  2.0300,  ...,  0.1128,  0.0605, -0.0964],\n",
       "            [ 1.7511,  1.9428,  2.0300,  ..., -0.2184, -0.2010, -0.1835],\n",
       "            ...,\n",
       "            [ 1.3851,  1.4897,  1.6640,  ...,  2.4657,  2.4483,  2.4308],\n",
       "            [ 1.3677,  1.4897,  1.6640,  ...,  2.4831,  2.4483,  2.4308],\n",
       "            [ 1.3502,  1.4897,  1.6640,  ...,  2.4831,  2.4483,  2.4308]]]]),\n",
       "  tensor([[[[-1.3130, -1.3130, -1.3130,  ..., -1.1760, -1.1589, -1.1075],\n",
       "            [-1.3130, -1.2959, -1.2959,  ..., -1.1418, -1.1418, -1.1075],\n",
       "            [-1.2959, -1.2788, -1.2788,  ..., -1.1247, -1.1247, -1.1075],\n",
       "            ...,\n",
       "            [-1.1589, -1.1760, -1.1760,  ..., -1.3130, -1.3130, -1.3130],\n",
       "            [-1.1589, -1.1760, -1.1760,  ..., -1.2274, -1.2445, -1.2274],\n",
       "            [-1.1418, -1.1589, -1.1932,  ..., -1.1247, -1.1247, -1.1075]],\n",
       "  \n",
       "           [[-1.2129, -1.2129, -1.2129,  ..., -1.0728, -1.0553, -1.0028],\n",
       "            [-1.2129, -1.1954, -1.1954,  ..., -1.0378, -1.0378, -1.0028],\n",
       "            [-1.1954, -1.1779, -1.1779,  ..., -1.0203, -1.0203, -1.0028],\n",
       "            ...,\n",
       "            [-1.0553, -1.0728, -1.0728,  ..., -1.2129, -1.2129, -1.2129],\n",
       "            [-1.0553, -1.0728, -1.0728,  ..., -1.1253, -1.1429, -1.1253],\n",
       "            [-1.0378, -1.0553, -1.0903,  ..., -1.0203, -1.0203, -1.0028]],\n",
       "  \n",
       "           [[-0.9853, -0.9853, -0.9853,  ..., -0.8458, -0.8284, -0.7761],\n",
       "            [-0.9853, -0.9678, -0.9678,  ..., -0.8110, -0.8110, -0.7761],\n",
       "            [-0.9678, -0.9504, -0.9504,  ..., -0.7936, -0.7936, -0.7761],\n",
       "            ...,\n",
       "            [-0.8284, -0.8458, -0.8458,  ..., -0.9853, -0.9853, -0.9853],\n",
       "            [-0.8284, -0.8458, -0.8458,  ..., -0.8981, -0.9156, -0.8981],\n",
       "            [-0.8110, -0.8284, -0.8633,  ..., -0.7936, -0.7936, -0.7761]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.5364,  1.0502,  1.6838,  ..., -0.5082, -1.2103, -1.8782],\n",
       "            [-0.2513,  0.5878,  1.0331,  ..., -1.2103, -1.8782, -2.1179],\n",
       "            [-1.0904,  0.0227,  0.4166,  ..., -1.8610, -2.1179, -2.1179],\n",
       "            ...,\n",
       "            [-1.9809, -1.9809, -1.9467,  ..., -1.6213, -1.6727, -1.7240],\n",
       "            [-1.9467, -2.0152, -1.9809,  ..., -1.7412, -1.8097, -1.8097],\n",
       "            [-1.9295, -2.0152, -2.0152,  ..., -1.7412, -1.8097, -1.8097]],\n",
       "  \n",
       "           [[ 0.6779,  1.2031,  1.8508,  ..., -0.3901, -1.1078, -1.7906],\n",
       "            [-0.1275,  0.7304,  1.1856,  ..., -1.1078, -1.7906, -2.0357],\n",
       "            [-0.9853,  0.1527,  0.5553,  ..., -1.7731, -2.0357, -2.0357],\n",
       "            ...,\n",
       "            [-1.8957, -1.8957, -1.8606,  ..., -1.5280, -1.5805, -1.6331],\n",
       "            [-1.8606, -1.9307, -1.8957,  ..., -1.6506, -1.7206, -1.7206],\n",
       "            [-1.8431, -1.9307, -1.9307,  ..., -1.6506, -1.7206, -1.7206]],\n",
       "  \n",
       "           [[ 0.8971,  1.4200,  2.0648,  ..., -0.1661, -0.8807, -1.5604],\n",
       "            [ 0.0953,  0.9494,  1.4025,  ..., -0.8807, -1.5604, -1.8044],\n",
       "            [-0.7587,  0.3742,  0.7751,  ..., -1.5430, -1.8044, -1.8044],\n",
       "            ...,\n",
       "            [-1.6650, -1.6650, -1.6302,  ..., -1.2990, -1.3513, -1.4036],\n",
       "            [-1.6302, -1.6999, -1.6650,  ..., -1.4210, -1.4907, -1.4907],\n",
       "            [-1.6127, -1.6999, -1.6999,  ..., -1.4210, -1.4907, -1.4907]]]]),\n",
       "  tensor([[[[ 0.2453,  0.2624,  0.2624,  ..., -1.0219, -0.8507, -0.8164],\n",
       "            [ 0.1254,  0.1083,  0.1254,  ..., -1.1418, -1.0219, -0.9363],\n",
       "            [ 0.0741,  0.0227,  0.0398,  ..., -1.3130, -1.1075, -1.0390],\n",
       "            ...,\n",
       "            [ 0.9646,  0.9474,  0.9474,  ..., -0.1657, -0.2342, -0.2684],\n",
       "            [ 0.9817,  0.9817,  0.9646,  ..., -0.0287, -0.1486, -0.2342],\n",
       "            [ 0.9988,  1.0159,  0.9988,  ..., -0.0116, -0.0116, -0.1143]],\n",
       "  \n",
       "           [[ 0.3803,  0.3978,  0.3978,  ..., -0.9153, -0.7402, -0.7052],\n",
       "            [ 0.2577,  0.2402,  0.2577,  ..., -1.0378, -0.9153, -0.8277],\n",
       "            [ 0.2052,  0.1527,  0.1702,  ..., -1.2129, -1.0028, -0.9328],\n",
       "            ...,\n",
       "            [ 1.1155,  1.0980,  1.0980,  ..., -0.0399, -0.1099, -0.1450],\n",
       "            [ 1.1331,  1.1331,  1.1155,  ...,  0.1001, -0.0224, -0.1099],\n",
       "            [ 1.1506,  1.1681,  1.1506,  ...,  0.1176,  0.1176,  0.0126]],\n",
       "  \n",
       "           [[ 0.6008,  0.6182,  0.6182,  ..., -0.6890, -0.5147, -0.4798],\n",
       "            [ 0.4788,  0.4614,  0.4788,  ..., -0.8110, -0.6890, -0.6018],\n",
       "            [ 0.4265,  0.3742,  0.3916,  ..., -0.9853, -0.7761, -0.7064],\n",
       "            ...,\n",
       "            [ 1.3328,  1.3154,  1.3154,  ...,  0.1825,  0.1128,  0.0779],\n",
       "            [ 1.3502,  1.3502,  1.3328,  ...,  0.3219,  0.1999,  0.1128],\n",
       "            [ 1.3677,  1.3851,  1.3677,  ...,  0.3393,  0.3393,  0.2348]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4226, -0.4226, -0.4226,  ...,  0.0741,  0.0912,  0.0912],\n",
       "            [-0.4226, -0.4226, -0.4226,  ...,  0.0912,  0.0912,  0.1254],\n",
       "            [-0.4397, -0.4397, -0.4397,  ...,  0.0912,  0.0912,  0.1254],\n",
       "            ...,\n",
       "            [ 0.3652,  0.3652,  0.3994,  ...,  0.3309,  0.3652,  0.3652],\n",
       "            [ 0.3652,  0.3652,  0.3994,  ...,  0.3309,  0.3652,  0.3994],\n",
       "            [ 0.3652,  0.3994,  0.3994,  ...,  0.3481,  0.3994,  0.4337]],\n",
       "  \n",
       "           [[-0.3025, -0.3025, -0.3025,  ...,  0.2052,  0.2227,  0.2227],\n",
       "            [-0.3025, -0.3025, -0.3025,  ...,  0.2227,  0.2227,  0.2577],\n",
       "            [-0.3200, -0.3200, -0.3200,  ...,  0.2227,  0.2227,  0.2577],\n",
       "            ...,\n",
       "            [ 0.5028,  0.5028,  0.5378,  ...,  0.4678,  0.5028,  0.5028],\n",
       "            [ 0.5028,  0.5028,  0.5378,  ...,  0.4678,  0.5028,  0.5378],\n",
       "            [ 0.5028,  0.5378,  0.5378,  ...,  0.4853,  0.5378,  0.5728]],\n",
       "  \n",
       "           [[-0.0790, -0.0790, -0.0790,  ...,  0.4265,  0.4439,  0.4439],\n",
       "            [-0.0790, -0.0790, -0.0790,  ...,  0.4439,  0.4439,  0.4788],\n",
       "            [-0.0964, -0.0964, -0.0964,  ...,  0.4439,  0.4439,  0.4788],\n",
       "            ...,\n",
       "            [ 0.7228,  0.7228,  0.7576,  ...,  0.6879,  0.7228,  0.7228],\n",
       "            [ 0.7228,  0.7228,  0.7576,  ...,  0.6879,  0.7228,  0.7576],\n",
       "            [ 0.7228,  0.7576,  0.7576,  ...,  0.7054,  0.7576,  0.7925]]]]),\n",
       "  tensor([[[[-0.5424, -0.5424, -0.5767,  ..., -0.8507, -0.7993, -0.7479],\n",
       "            [-0.5938, -0.6623, -0.6623,  ..., -0.7650, -0.7822, -0.6794],\n",
       "            [-0.6281, -0.6623, -0.7308,  ..., -0.7137, -0.7479, -0.4911],\n",
       "            ...,\n",
       "            [ 0.8618,  0.8447,  0.8104,  ..., -0.3198, -0.3198, -0.3198],\n",
       "            [ 0.8447,  0.8789,  0.8618,  ..., -0.2856, -0.3198, -0.3369],\n",
       "            [ 0.8618,  0.8789,  0.8276,  ..., -0.3198, -0.3198, -0.3541]],\n",
       "  \n",
       "           [[-0.4251, -0.4251, -0.4601,  ..., -0.7402, -0.6877, -0.6352],\n",
       "            [-0.4776, -0.5476, -0.5476,  ..., -0.6527, -0.6702, -0.5651],\n",
       "            [-0.5126, -0.5476, -0.6176,  ..., -0.6001, -0.6352, -0.3725],\n",
       "            ...,\n",
       "            [ 1.0105,  0.9930,  0.9580,  ..., -0.1975, -0.1975, -0.1975],\n",
       "            [ 0.9930,  1.0280,  1.0105,  ..., -0.1625, -0.1975, -0.2150],\n",
       "            [ 1.0105,  1.0280,  0.9755,  ..., -0.1975, -0.1975, -0.2325]],\n",
       "  \n",
       "           [[-0.2010, -0.2010, -0.2358,  ..., -0.5147, -0.4624, -0.4101],\n",
       "            [-0.2532, -0.3230, -0.3230,  ..., -0.4275, -0.4450, -0.3404],\n",
       "            [-0.2881, -0.3230, -0.3927,  ..., -0.3753, -0.4101, -0.1487],\n",
       "            ...,\n",
       "            [ 1.2282,  1.2108,  1.1759,  ...,  0.0256,  0.0256,  0.0256],\n",
       "            [ 1.2108,  1.2457,  1.2282,  ...,  0.0605,  0.0256,  0.0082],\n",
       "            [ 1.2282,  1.2457,  1.1934,  ...,  0.0256,  0.0256, -0.0092]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.6727, -2.1179, -2.1179,  ..., -2.0152, -2.0665, -1.9467],\n",
       "            [-1.5870, -2.0665, -2.1179,  ..., -1.8610, -1.7754, -1.8097],\n",
       "            [-1.8610, -2.1179, -2.1179,  ..., -1.4329, -1.5014, -1.5870],\n",
       "            ...,\n",
       "            [ 2.2489,  2.2489,  2.2489,  ..., -0.1999, -0.5767, -0.4911],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  0.3994, -0.1999, -0.4911],\n",
       "            [ 2.2489,  2.2489,  2.2489,  ...,  1.2899,  0.8447,  0.5364]],\n",
       "  \n",
       "           [[-1.5805, -2.0357, -2.0357,  ..., -1.9307, -1.9832, -1.8606],\n",
       "            [-1.4930, -1.9832, -2.0357,  ..., -1.7731, -1.6856, -1.7206],\n",
       "            [-1.7731, -2.0357, -2.0357,  ..., -1.3354, -1.4055, -1.4930],\n",
       "            ...,\n",
       "            [ 2.4286,  2.4286,  2.4286,  ..., -0.0749, -0.4601, -0.3725],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  0.5378, -0.0749, -0.3725],\n",
       "            [ 2.4286,  2.4286,  2.4286,  ...,  1.4482,  0.9930,  0.6779]],\n",
       "  \n",
       "           [[-1.3513, -1.8044, -1.8044,  ..., -1.6999, -1.7522, -1.6302],\n",
       "            [-1.2641, -1.7522, -1.8044,  ..., -1.5430, -1.4559, -1.4907],\n",
       "            [-1.5430, -1.8044, -1.8044,  ..., -1.1073, -1.1770, -1.2641],\n",
       "            ...,\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  0.1476, -0.2358, -0.1487],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  0.7576,  0.1476, -0.1487],\n",
       "            [ 2.6400,  2.6400,  2.6400,  ...,  1.6640,  1.2108,  0.8971]]]]),\n",
       "  tensor([[[[-0.8164, -0.8164, -0.7993,  ..., -0.9534, -0.9363, -0.9363],\n",
       "            [-0.8164, -0.7993, -0.7993,  ..., -0.9534, -0.9363, -0.9192],\n",
       "            [-0.7993, -0.7822, -0.7822,  ..., -0.9534, -0.9363, -0.9192],\n",
       "            ...,\n",
       "            [ 0.1254,  0.1426,  0.1597,  ...,  0.3652,  0.3652,  0.3652],\n",
       "            [ 0.1254,  0.1426,  0.1597,  ...,  0.3823,  0.3823,  0.3823],\n",
       "            [ 0.1254,  0.1426,  0.1597,  ...,  0.3823,  0.3823,  0.3823]],\n",
       "  \n",
       "           [[-0.7052, -0.7052, -0.6877,  ..., -0.8452, -0.8277, -0.8277],\n",
       "            [-0.7052, -0.6877, -0.6877,  ..., -0.8452, -0.8277, -0.8102],\n",
       "            [-0.6877, -0.6702, -0.6702,  ..., -0.8452, -0.8277, -0.8102],\n",
       "            ...,\n",
       "            [ 0.2577,  0.2752,  0.2927,  ...,  0.5028,  0.5028,  0.5028],\n",
       "            [ 0.2577,  0.2752,  0.2927,  ...,  0.5203,  0.5203,  0.5203],\n",
       "            [ 0.2577,  0.2752,  0.2927,  ...,  0.5203,  0.5203,  0.5203]],\n",
       "  \n",
       "           [[-0.4798, -0.4798, -0.4624,  ..., -0.6193, -0.6018, -0.6018],\n",
       "            [-0.4798, -0.4624, -0.4624,  ..., -0.6193, -0.6018, -0.5844],\n",
       "            [-0.4624, -0.4450, -0.4450,  ..., -0.6193, -0.6018, -0.5844],\n",
       "            ...,\n",
       "            [ 0.4788,  0.4962,  0.5136,  ...,  0.7228,  0.7228,  0.7228],\n",
       "            [ 0.4788,  0.4962,  0.5136,  ...,  0.7402,  0.7402,  0.7402],\n",
       "            [ 0.4788,  0.4962,  0.5136,  ...,  0.7402,  0.7402,  0.7402]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1254,  0.0227, -0.1314,  ..., -1.3302, -1.3130, -1.3130],\n",
       "            [ 0.1083,  0.0056, -0.1314,  ..., -1.3302, -1.3302, -1.3130],\n",
       "            [ 0.0912,  0.0056, -0.1314,  ..., -1.3302, -1.3302, -1.3302],\n",
       "            ...,\n",
       "            [-1.2788, -1.2274, -1.1589,  ..., -0.1828, -0.3198, -0.3883],\n",
       "            [-1.2617, -1.1932, -1.0904,  ..., -0.1999, -0.3369, -0.4054],\n",
       "            [-1.2274, -1.1589, -1.0219,  ..., -0.2171, -0.3541, -0.4226]],\n",
       "  \n",
       "           [[ 0.2577,  0.1527, -0.0049,  ..., -1.2304, -1.2129, -1.2129],\n",
       "            [ 0.2402,  0.1352, -0.0049,  ..., -1.2304, -1.2304, -1.2129],\n",
       "            [ 0.2227,  0.1352, -0.0049,  ..., -1.2304, -1.2304, -1.2304],\n",
       "            ...,\n",
       "            [-1.1779, -1.1253, -1.0553,  ..., -0.0574, -0.1975, -0.2675],\n",
       "            [-1.1604, -1.0903, -0.9853,  ..., -0.0749, -0.2150, -0.2850],\n",
       "            [-1.1253, -1.0553, -0.9153,  ..., -0.0924, -0.2325, -0.3025]],\n",
       "  \n",
       "           [[ 0.4788,  0.3742,  0.2173,  ..., -1.0027, -0.9853, -0.9853],\n",
       "            [ 0.4614,  0.3568,  0.2173,  ..., -1.0027, -1.0027, -0.9853],\n",
       "            [ 0.4439,  0.3568,  0.2173,  ..., -1.0027, -1.0027, -1.0027],\n",
       "            ...,\n",
       "            [-0.9504, -0.8981, -0.8284,  ...,  0.1651,  0.0256, -0.0441],\n",
       "            [-0.9330, -0.8633, -0.7587,  ...,  0.1476,  0.0082, -0.0615],\n",
       "            [-0.8981, -0.8284, -0.6890,  ...,  0.1302, -0.0092, -0.0790]]]]),\n",
       "  tensor([[[[ 0.8789,  0.9132,  0.9303,  ..., -0.9705, -0.8335, -0.9705],\n",
       "            [ 0.9303,  0.9132,  0.9303,  ..., -0.8335, -0.8335, -0.9705],\n",
       "            [ 0.9474,  0.8618,  0.8618,  ..., -0.5596, -0.6452, -0.9705],\n",
       "            ...,\n",
       "            [ 1.6838,  1.6667,  1.7180,  ...,  1.9235,  1.9235,  1.9235],\n",
       "            [ 1.7009,  1.7009,  1.7009,  ...,  1.9235,  1.9235,  1.9235],\n",
       "            [ 1.7009,  1.7523,  1.6495,  ...,  1.9235,  1.9235,  1.9235]],\n",
       "  \n",
       "           [[ 1.0280,  1.0630,  1.0805,  ..., -0.8627, -0.7227, -0.8627],\n",
       "            [ 1.0805,  1.0630,  1.0805,  ..., -0.7227, -0.7227, -0.8627],\n",
       "            [ 1.0980,  1.0105,  1.0105,  ..., -0.4426, -0.5301, -0.8627],\n",
       "            ...,\n",
       "            [ 1.8508,  1.8333,  1.8859,  ...,  2.0959,  2.0959,  2.0959],\n",
       "            [ 1.8683,  1.8683,  1.8683,  ...,  2.0959,  2.0959,  2.0959],\n",
       "            [ 1.8683,  1.9209,  1.8158,  ...,  2.0959,  2.0959,  2.0959]],\n",
       "  \n",
       "           [[ 1.2457,  1.2805,  1.2980,  ..., -0.6367, -0.4973, -0.6367],\n",
       "            [ 1.2980,  1.2805,  1.2980,  ..., -0.4973, -0.4973, -0.6367],\n",
       "            [ 1.3154,  1.2282,  1.2282,  ..., -0.2184, -0.3055, -0.6367],\n",
       "            ...,\n",
       "            [ 2.0648,  2.0474,  2.0997,  ...,  2.3088,  2.3088,  2.3088],\n",
       "            [ 2.0823,  2.0823,  2.0823,  ...,  2.3088,  2.3088,  2.3088],\n",
       "            [ 2.0823,  2.1346,  2.0300,  ...,  2.3088,  2.3088,  2.3088]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.1932, -1.1932, -1.1932,  ..., -1.0390, -1.0219, -1.0219],\n",
       "            [-1.1932, -1.1932, -1.1932,  ..., -1.0390, -1.0219, -1.0219],\n",
       "            [-1.1932, -1.1932, -1.2103,  ..., -1.0390, -1.0390, -1.0390],\n",
       "            ...,\n",
       "            [ 0.2967,  0.2624,  0.2624,  ...,  1.1529,  1.2385,  1.2385],\n",
       "            [ 0.5536,  0.4337,  0.2967,  ...,  1.1872,  1.2385,  1.2385],\n",
       "            [ 0.5536,  0.4679,  0.2624,  ...,  1.3242,  1.2385,  1.3242]],\n",
       "  \n",
       "           [[-1.0903, -1.0903, -1.0903,  ..., -0.9328, -0.9153, -0.9153],\n",
       "            [-1.0903, -1.0903, -1.0903,  ..., -0.9328, -0.9153, -0.9153],\n",
       "            [-1.0903, -1.0903, -1.1078,  ..., -0.9328, -0.9328, -0.9328],\n",
       "            ...,\n",
       "            [ 0.4328,  0.3978,  0.3978,  ...,  1.3081,  1.3957,  1.3957],\n",
       "            [ 0.6954,  0.5728,  0.4328,  ...,  1.3431,  1.3957,  1.3957],\n",
       "            [ 0.6954,  0.6078,  0.3978,  ...,  1.4832,  1.3957,  1.4832]],\n",
       "  \n",
       "           [[-0.8633, -0.8633, -0.8633,  ..., -0.7064, -0.6890, -0.6890],\n",
       "            [-0.8633, -0.8633, -0.8633,  ..., -0.7064, -0.6890, -0.6890],\n",
       "            [-0.8633, -0.8633, -0.8807,  ..., -0.7064, -0.7064, -0.7064],\n",
       "            ...,\n",
       "            [ 0.6531,  0.6182,  0.6182,  ...,  1.5245,  1.6117,  1.6117],\n",
       "            [ 0.9145,  0.7925,  0.6531,  ...,  1.5594,  1.6117,  1.6117],\n",
       "            [ 0.9145,  0.8274,  0.6182,  ...,  1.6988,  1.6117,  1.6988]]]])]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(datalodaer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "gt5sjygmYsmp"
   },
   "outputs": [],
   "source": [
    "class ChexMSNModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 criterion: nn.Module,\n",
    "                 num_prototypes: int = 2048,\n",
    "                 learning_rate: float =  1e-3,\n",
    "                 weight_decay: float = 0.0,\n",
    "                 max_epochs: int = 50,\n",
    "                 mask_ratio: float = 0.15,\n",
    "\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.prototypes = nn.ModuleList([nn.Linear(in_features=512,out_features=num_prototypes),\n",
    "                                        nn.Linear(in_features=512,out_features=num_prototypes),\n",
    "                                        nn.Linear(in_features=512,out_features=num_prototypes)])\n",
    "                \n",
    "    def training_step(self, \n",
    "                      batch: List[Tensor], \n",
    "                      batch_idx: int\n",
    "                     ) -> float:\n",
    "        \n",
    "\n",
    "        anchors, target = model(batch) \n",
    "        loss = criterion(anchors,target,self.prototypes)\n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=False , logger=True)      \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr=self.lr,\n",
    "                                      weight_decay=self.weight_decay)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                               eta_min=0.00001,\n",
    "                                                               T_max=self.max_epochs)\n",
    "        return {'optimizer': optimizer,\n",
    "               'lr_scheduler': scheduler\n",
    "               }\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "RA6Fn_RDYsg5"
   },
   "outputs": [],
   "source": [
    "chexmsn = ChexMSNModel(model=model,criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ZN3QbyoxYseC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/pytorch_lightning/core/module.py:420: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(22.8914, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chexmsn.training_step(next(iter(datalodaer)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "GqlbSVIiYsYj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(datalodaer))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "TEA6uT82YsV7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0263,  0.2974, -0.1393,  ...,  0.5074,  0.1716, -0.1627],\n",
       "          [ 0.1130,  0.0252,  0.1407,  ...,  0.4792, -0.2396, -0.1600],\n",
       "          [ 0.1798,  0.1764,  0.1032,  ...,  0.1357, -0.6334,  0.0844],\n",
       "          ...,\n",
       "          [ 0.3475,  0.3314,  0.0842,  ...,  0.2358, -0.3051, -0.0203],\n",
       "          [ 0.4003,  0.0806, -0.2598,  ...,  0.1931, -0.7471,  0.0807],\n",
       "          [ 0.1596,  0.3951, -0.0781,  ...,  0.2155,  0.0606,  0.2141]],\n",
       " \n",
       "         [[-0.0033,  0.2807, -0.0862,  ...,  0.4857,  0.1266, -0.1063],\n",
       "          [ 0.0346,  0.0416, -0.0430,  ...,  0.3623, -0.0658,  0.0156],\n",
       "          [ 0.2710,  0.1721,  0.0216,  ...,  0.0565, -0.7129,  0.1693],\n",
       "          ...,\n",
       "          [ 0.2687,  0.3021,  0.1107,  ...,  0.2366, -0.4238, -0.0339],\n",
       "          [ 0.3268, -0.0498, -0.3285,  ...,  0.1033, -0.7735,  0.1527],\n",
       "          [ 0.1666,  0.3889, -0.1307,  ...,  0.1600,  0.1442,  0.2036]],\n",
       " \n",
       "         [[ 0.0555,  0.2447, -0.1671,  ...,  0.5027,  0.1655, -0.1685],\n",
       "          [-0.0567,  0.0804,  0.1361,  ...,  0.4990, -0.2004, -0.0336],\n",
       "          [ 0.2384,  0.1704,  0.0965,  ...,  0.0872, -0.6820,  0.1052],\n",
       "          ...,\n",
       "          [ 0.4277,  0.2578, -0.0228,  ...,  0.0222, -0.6986, -0.2515],\n",
       "          [ 0.3533,  0.0219, -0.3103,  ...,  0.1540, -0.7896,  0.0945],\n",
       "          [ 0.2374,  0.4419,  0.0681,  ...,  0.2509, -0.3244,  0.4282]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.1326, -0.0608,  0.1261,  ..., -0.0807, -0.0111, -0.0475],\n",
       "          [-0.2001,  0.0739, -0.0802,  ...,  0.3355,  0.0671,  0.0187],\n",
       "          [ 0.0343,  0.0880, -0.3801,  ...,  0.0468, -0.3762, -0.0036]],\n",
       " \n",
       "         [[ 0.1230,  0.4241,  0.2217,  ..., -0.2757,  0.0336, -0.2313],\n",
       "          [ 0.1381,  0.3881,  0.0194,  ...,  0.3341, -0.1842,  0.0713],\n",
       "          [ 0.1005,  0.0058, -0.0963,  ..., -0.0167, -0.1565, -0.1204]]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(datalodaer))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NAR0JgkYsTU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SICuTJZYsQ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRORG9nnYsOF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ci_Rz4amYsLh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXFghtHbYsGB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L69HyAS6YsDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuiXBby1YsA1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGA-UEYTYr9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jqX4qsmYr4k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFl78FRPYruq",
    "outputId": "3ac39185-f35c-4074-bcad-96c04c2e830a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb045fec710>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo2kXn4xa8Uu"
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                 images_size: int =  224,\n",
    "                 patch_size: int = 16,\n",
    "                 num_channels: int = 3,\n",
    "                 embedding_dim: int = 512\n",
    "                ) -> None:\n",
    "      super().__init__()\n",
    "      \"\"\"\n",
    "        Role: convert the input image of size = (b,c,h,w) into patches sequence\n",
    "              of size = (b,p,embedding_dim) where\n",
    "              b = batch size\n",
    "              c = number of channels\n",
    "              h = height\n",
    "              w = width\n",
    "\n",
    "        Inputs:\n",
    "            image_size - Dimensionality of the input image (width or height)\n",
    "            patch_size - Dimensionality of the image patch (width or height)\n",
    "            embedding_dim - Dimensionality of the patch embedding\n",
    "            num_channels - Number of channels in the input images (usually 3)\n",
    "      \"\"\"\n",
    "      self.image_size = images_size\n",
    "      self.patch_size = patch_size\n",
    "      self.num_channels = num_channels\n",
    "      self.hidden_size = embedding_dim\n",
    "\n",
    "      self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "      self.projection_layer = nn.Conv2d(in_channels= self.num_channels,\n",
    "                                        out_channels= self.hidden_size,\n",
    "                                        kernel_size= self.patch_size,\n",
    "                                        stride= self.patch_size\n",
    "                                       )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "      x = self.projection_layer(x)\n",
    "      x = x.flatten(2).transpose(1,2)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgIQjWfMbCad"
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                 images_size: int =  224,\n",
    "                 num_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embedding_dim: int = 512,\n",
    "                 num_cls_tokens: int = 3,\n",
    "                 dropout_p: float = 0.0,\n",
    "                 learnable_pos_encoding: bool = True\n",
    "                ) -> None:\n",
    "      super().__init__()\n",
    "      \"\"\"\n",
    "        Role: convert the input image into patches as well as adding CLS token\n",
    "        and positional encoding\n",
    "\n",
    "        Inputs:\n",
    "            image_size - Dimensionality of the input image (width or height)\n",
    "            num_channels - Number of channels in the input images (usually 3)\n",
    "            patch_size - Dimensionality of the image patch (width or height)\n",
    "            embedding_dim - Dimensionality of the patch embedding\n",
    "            num_cls_tokens - number of classification tokens to add to the sequence\n",
    "            dropout_p -  percentage of applied dropout\n",
    "            learnable_pos_encoding - specify typ of positional encoding (fixed or learnable)\n",
    "      \"\"\"\n",
    "      self.image_size = images_size\n",
    "      self.patch_size = patch_size\n",
    "      self.num_channels = num_channels\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.num_cls_tokens = num_cls_tokens\n",
    "      self.dropout_p = dropout_p\n",
    "      self.learnable_pos_encoding = learnable_pos_encoding\n",
    "\n",
    "      self.patch_embeddings = PatchEmbeddings(images_size= self.image_size,\n",
    "                                              patch_size= self.patch_size,\n",
    "                                              num_channels= self.num_channels,\n",
    "                                              embedding_dim= self.embedding_dim\n",
    "                                              )\n",
    "\n",
    "      self.cls_token = nn.Parameter(torch.randn(size= (1, self.num_cls_tokens, self.embedding_dim)))\n",
    "\n",
    "      num_patches = self.patch_embeddings.num_patches\n",
    "      if self.learnable_pos_encoding:\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.positional_embeddings = nn.Parameter(torch.empty(1, num_patches , self.embedding_dim).normal_(std=0.02))\n",
    "\n",
    "      else:\n",
    "        self.positional_embeddings = self._get_positional_embeddings(num_patches=num_patches,\n",
    "                                                                     embedding_dim=self.embedding_dim\n",
    "                                                                    )\n",
    "\n",
    "      self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "\n",
    "      x = self.patch_embeddings(x)\n",
    "      batch_size, _, _ = x.size()\n",
    "      cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "      x = torch.cat([cls_token,x], dim=1)\n",
    "      x = x + self.positional_embeddings\n",
    "      x = self.dropout(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "    # def _get_positional_embeddings(self,\n",
    "    #                              num_patches: int,\n",
    "    #                              embedding_dim: int\n",
    "    #                              ) -> torch.Tensor:\n",
    "    #   poaitional_embeddings  = torch.ones(num_patches + self.num_cls_tokens, embedding_dim)\n",
    "    #   for i in range(num_patches + self.num_cls_tokens):\n",
    "    #       for j in range(embedding_dim):\n",
    "    #           poaitional_embeddings [i][j] = np.sin(i / (10000 ** (j / embedding_dim))) \\\n",
    "    #           if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / embedding_dim)))\n",
    "    #   return poaitional_embeddings.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RB8v9TUbHOV"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self,\n",
    "               embedding_dim: int = 512,\n",
    "               hidden_dim = 192,\n",
    "               dropout_p: float = 0.0\n",
    "               ) -> None:\n",
    "      super().__init__()\n",
    "      \"\"\"\n",
    "        Role: Feed forward netork of attention head\n",
    "\n",
    "        Inputs:\n",
    "            embedding_dim - Dimensionality of the patch embedding\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "            dropout_p -  percentage of applied dropout\n",
    "            learnable_pos_encoding - specify typ of positional encoding (fixed or learnable)\n",
    "      \"\"\"\n",
    "      self.feed_forward = nn.Sequential(nn.Linear(in_features= embedding_dim,\n",
    "                                                  out_features= hidden_dim\n",
    "                                                 ),\n",
    "                                        nn.GELU(),\n",
    "                                        nn.Linear(in_features=hidden_dim,\n",
    "                                                  out_features=embedding_dim\n",
    "                                                 ),\n",
    "                                        nn.Dropout(dropout_p)\n",
    "                                       )\n",
    "    def forward(self,\n",
    "              x: torch.tensor\n",
    "              )-> torch.Tensor:\n",
    "      x = self.feed_forward(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWWhv6RubWr3"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 512,\n",
    "                 hidden_dim: int = 192,\n",
    "                 num_heads: int = 4,\n",
    "                 dropout_p=0.0,\n",
    "                 pre_layer_norm: bool= True\n",
    "                 ) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "      \"\"\" Role: capture the interaction between\n",
    "          Inputs:\n",
    "              embedding_dim - Dimensionality of input and attention feature vectors\n",
    "              hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "              num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "              dropout - Amount of dropout to apply in the feed-forward network\n",
    "              pre_layer_norm - Specifies where to apply the layer norm (before or after)\n",
    "      \"\"\"\n",
    "      self.pre_layer_norm = pre_layer_norm\n",
    "      self.layer_norm_1 = nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "\n",
    "      self.attention_head = nn.MultiheadAttention(embed_dim= embedding_dim,\n",
    "                                                  num_heads= num_heads,\n",
    "                                                  dropout= dropout_p)\n",
    "\n",
    "      self.layer_norm_2 = nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "\n",
    "      self.feed_forward = FeedForwardNet(embedding_dim= embedding_dim,\n",
    "                                         hidden_dim= hidden_dim,\n",
    "                                         dropout_p= dropout_p\n",
    "                                        )\n",
    "\n",
    "    #attention weights from here\n",
    "    def forward(self,\n",
    "                x: torch.tensor\n",
    "                )-> torch.Tensor:\n",
    "\n",
    "        if self.pre_layer_norm:\n",
    "          normalization_out1 = self.layer_norm_1(x)\n",
    "          attention_out = self.attention_head(normalization_out1, normalization_out1, normalization_out1)[0]\n",
    "          addition_out1 = x + attention_out\n",
    "          normalization_out2 = self.layer_norm_2(addition_out1)\n",
    "          feed_forward_out = self.feed_forward(normalization_out2)\n",
    "          layer_out = addition_out1 + feed_forward_out\n",
    "\n",
    "        else:\n",
    "          attention_out = self.attention_head(x,x,x)[0]\n",
    "          addition_out1 = x + attention_out\n",
    "          normalization_out = self.layer_norm_1(addition_out1)\n",
    "          feed_forward_out = self.feed_forward(normalization_out)\n",
    "          addition_out2 = normalization_out + feed_forward_out\n",
    "          layer_out = self.layer_norm_2(addition_out2)\n",
    "\n",
    "        return attention_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGe0sAbCbcSF"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers: int = 6,\n",
    "                 num_heads: int = 4,\n",
    "                 hidden_dim: int = 192,\n",
    "                 pre_layer_norm: bool= True,\n",
    "                 images_size: int =  224,\n",
    "                 num_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embedding_dim: int = 512,\n",
    "                 num_cls_tokens: int = 3,\n",
    "                 dropout_p: float = 0.0,\n",
    "                 learnable_pos_encoding: bool = False\n",
    "                ) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "\n",
    "      self.image_size = images_size\n",
    "      self.patch_size = patch_size\n",
    "      self.num_channels = num_channels\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.num_cls_tokens = num_cls_tokens\n",
    "      self.dropout_p = dropout_p\n",
    "      self.learnable_pos_encoding = learnable_pos_encoding\n",
    "\n",
    "      self.input_embeddings = PatchEmbeddings(images_size= self.image_size,\n",
    "                                              num_channels= self.num_channels,\n",
    "                                              patch_size= self.patch_size,\n",
    "                                              embedding_dim= self.embedding_dim,\n",
    "                                              )\n",
    "                                              # num_cls_tokens= self.num_cls_tokens,\n",
    "                                              # dropout_p=self.dropout_p,\n",
    "                                              # learnable_pos_encoding=self.learnable_pos_encoding)\n",
    "\n",
    "\n",
    "\n",
    "      self.num_layers = num_layers\n",
    "      self.num_heads = num_heads\n",
    "      self.hidden_dim = hidden_dim\n",
    "      self.pre_layer_norm = pre_layer_norm\n",
    "\n",
    "      self.attention_block = AttentionBlock(embedding_dim= self.embedding_dim,\n",
    "                                            hidden_dim= self.hidden_dim,\n",
    "                                            num_heads= self.num_heads,\n",
    "                                            dropout_p=self.dropout_p,\n",
    "                                            pre_layer_norm=self.pre_layer_norm)\n",
    "\n",
    "\n",
    "      self.cls_token = nn.Parameter(torch.randn(size= (1, self.num_cls_tokens, self.embedding_dim)))\n",
    "      num_patches = self.input_embeddings.num_patches\n",
    "      self.positional_embeddings = nn.Parameter(torch.empty(1, num_patches + self.num_cls_tokens, self.embedding_dim).normal_(std=0.02))\n",
    "\n",
    "      self.encoder = nn.Sequential(*[self.attention_block for layer in range(num_layers)])\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.tensor,\n",
    "                branch: str = 'target'\n",
    "                )-> torch.Tensor:\n",
    "\n",
    "      if branch == 'target':\n",
    "        x = self.input_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "        x = torch.cat([cls_token,x], dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.encoder(x)\n",
    "        return x#[:,0:self.num_cls_tokens]\n",
    "\n",
    "      elif branch == 'anchor':\n",
    "        x = self.input_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "        x = torch.cat([cls_token,x], dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = torch.cat((x[:,:1],x[:,self.num_cls_tokens:]),dim = 1)\n",
    "        x = self.encoder(x)\n",
    "        return x#[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "IEXComnnbk7u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tensor([[1,2,3],[4,5,6]])\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.repeat((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.repeat(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([h for _ in range(5)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chexmsn-env\n",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
