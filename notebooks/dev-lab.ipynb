{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xREDrfXYsQ2v"
   },
   "source": [
    "# Transformer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sv_KTqnFsQfM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple\n",
    "from types import FunctionType\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "31q37yTHbxE8"
   },
   "outputs": [],
   "source": [
    "class ConvStemConfig(NamedTuple):\n",
    "    out_channels: int = 64\n",
    "    kernel_size: int = 3\n",
    "    stride: int = 2\n",
    "    norm_layer: Callable[..., nn.Module] = nn.BatchNorm2d\n",
    "    activation_layer: Callable[..., nn.Module] = nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_api_usage_once(obj: Any) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Logs API usage(module and name) within an organization.\n",
    "    In a large ecosystem, it's often useful to track the PyTorch and\n",
    "    TorchVision APIs usage. This API provides the similar functionality to the\n",
    "    logging module in the Python stdlib. It can be used for debugging purpose\n",
    "    to log which methods are used and by default it is inactive, unless the user\n",
    "    manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_.\n",
    "    Please note it is triggered only once for the same API call within a process.\n",
    "    It does not collect any data from open-source users since it is no-op by default.\n",
    "    For more information, please refer to\n",
    "    * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging;\n",
    "    * Logging policy: https://github.com/pytorch/vision/issues/5052;\n",
    "\n",
    "    Args:\n",
    "        obj (class instance or method): an object to extract info from.\n",
    "    \"\"\"\n",
    "    module = obj.__module__\n",
    "    if not module.startswith(\"torchvision\"):\n",
    "        module = f\"torchvision.internal.{module}\"\n",
    "    name = obj.__class__.__name__\n",
    "    if isinstance(obj, FunctionType):\n",
    "        name = obj.__name__\n",
    "    torch._C._log_api_usage_once(f\"{module}.{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ntuple(x: Any, n: int) -> Tuple[Any, ...]:\n",
    "    \"\"\"\n",
    "    Make n-tuple from input x. If x is an iterable, then we just convert it to tuple.\n",
    "    Otherwise, we will make a tuple of length n, all with value of x.\n",
    "    reference: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/utils.py#L8\n",
    "\n",
    "    Args:\n",
    "        x (Any): input value\n",
    "        n (int): length of the resulting tuple\n",
    "    \"\"\"\n",
    "    if isinstance(x, collections.abc.Iterable):\n",
    "        return tuple(x)\n",
    "    return tuple(repeat(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S8P8AHvJf-wr"
   },
   "outputs": [],
   "source": [
    "def expand_index_like(index: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Expands the index along the last dimension of the input tokens.\n",
    "\n",
    "    Args:\n",
    "        index:\n",
    "            Index tensor with shape (batch_size, idx_length) where each entry is\n",
    "            an index in [0, sequence_length).\n",
    "        tokens:\n",
    "            Tokens tensor with shape (batch_size, sequence_length, dim).\n",
    "\n",
    "    Returns:\n",
    "        Index tensor with shape (batch_size, idx_length, dim) where the original\n",
    "        indices are repeated dim times along the last dimension.\n",
    "\n",
    "    \"\"\"\n",
    "    dim = tokens.shape[-1]\n",
    "    index = index.unsqueeze(-1).expand(-1, -1, dim)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yhPIIxm3fsP7"
   },
   "outputs": [],
   "source": [
    "def get_at_index(tokens: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Selects tokens at index.\n",
    "\n",
    "    Args:\n",
    "        tokens:\n",
    "            Token tensor with shape (batch_size, sequence_length, dim).\n",
    "        index:\n",
    "            Index tensor with shape (batch_size, index_length) where each entry is\n",
    "            an index in [0, sequence_length).\n",
    "\n",
    "    Returns:\n",
    "        Token tensor with shape (batch_size, index_length, dim) containing the\n",
    "        selected tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    index = expand_index_like(index, tokens)\n",
    "    return torch.gather(tokens, 1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place.\n",
    "            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = None,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "        log_api_usage_once(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kawb7bqFbydU"
   },
   "outputs": [],
   "source": [
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jag1PLOzb3_a"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vXSrz21eb-D6"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        num_cls_tokens: int,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "        self.seq_length = seq_length\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                input: torch.Tensor,\n",
    "                idx_keep: Optional[torch.Tensor] = None\n",
    "               ):\n",
    "\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "\n",
    "        input = input + self._interpolate_pos_encoding(input,self.num_cls_tokens)\n",
    "        if idx_keep is not None:\n",
    "            input = get_at_index(input, idx_keep)\n",
    "        return self.ln(self.layers(self.dropout(input)))\n",
    "\n",
    "    def _interpolate_pos_encoding(self,\n",
    "                                  input: torch.Tensor,\n",
    "                                  num_cls_tokens: int = 3):\n",
    "        \"\"\"Returns the interpolated positional embedding for the given input.\n",
    "\n",
    "        This function interpolates self.pos_embedding for all tokens in the input,\n",
    "        ignoring the class token. This allows encoding variable sized images.\n",
    "\n",
    "        Args:\n",
    "            input:\n",
    "               Input tensor with shape (batch_size, num_sequences).\n",
    "            num_cls_tokens:\n",
    "               number of classification tokens prepended to the\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = input.shape[1] - num_cls_tokens\n",
    "        N = self.pos_embedding.shape[1] - num_cls_tokens\n",
    "        diff = num_cls_tokens -1\n",
    "        if npatch == N:\n",
    "            return self.pos_embedding\n",
    "\n",
    "        else:\n",
    "            npatch += diff\n",
    "            class_emb = self.pos_embedding[:, 1]\n",
    "            pos_embedding = self.pos_embedding[:, 1+diff:]\n",
    "        dim = input.shape[-1]\n",
    "        pos_embedding = nn.functional.interpolate(\n",
    "            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode=\"bicubic\",\n",
    "        )\n",
    "        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5d2kHQDAadf3"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        num_layers: int = 6,\n",
    "        num_heads: int = 4,\n",
    "        hidden_dim: int = 512,\n",
    "        mlp_dim: int = 512,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 1000,\n",
    "        num_cls_tokens: int = 3,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        log_api_usage_once(self)\n",
    "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "\n",
    "        if conv_stem_configs is not None:\n",
    "            # As per https://arxiv.org/abs/2106.14881\n",
    "            seq_proj = nn.Sequential()\n",
    "            prev_channels = 3\n",
    "            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n",
    "                seq_proj.add_module(\n",
    "                    f\"conv_bn_relu_{i}\",\n",
    "                    Conv2dNormActivation(\n",
    "                        in_channels=prev_channels,\n",
    "                        out_channels=conv_stem_layer_config.out_channels,\n",
    "                        kernel_size=conv_stem_layer_config.kernel_size,\n",
    "                        stride=conv_stem_layer_config.stride,\n",
    "                        norm_layer=conv_stem_layer_config.norm_layer,\n",
    "                        activation_layer=conv_stem_layer_config.activation_layer,\n",
    "                    ),\n",
    "                )\n",
    "                prev_channels = conv_stem_layer_config.out_channels\n",
    "            seq_proj.add_module(\n",
    "                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n",
    "            )\n",
    "            self.conv_proj: nn.Module = seq_proj\n",
    "        else:\n",
    "            self.conv_proj = nn.Conv2d(\n",
    "                in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "            )\n",
    "\n",
    "        self.seq_length = (image_size // patch_size) ** 2\n",
    "        # Add a class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, self.num_cls_tokens, hidden_dim))\n",
    "        self.seq_length += self.num_cls_tokens\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            self.seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            num_cls_tokens,\n",
    "            norm_layer)\n",
    "        #self.seq_length = seq_length #+ self.num_cls_tokens\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        #torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        #torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                branch: str = 'target',\n",
    "                idx_keep: Optional[torch.Tensor] = None):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        if branch == 'target':\n",
    "            batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "            x = torch.cat([batch_class_token, x], dim=1)\n",
    "            x = self.encoder(x)\n",
    "            return x[:,0:self.num_cls_tokens]\n",
    "        elif branch == 'anchor':\n",
    "            batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "            x = torch.cat([batch_class_token, x], dim=1)\n",
    "            x = torch.cat((x[:,:1],x[:,self.num_cls_tokens:]),dim = 1)\n",
    "            x = self.encoder(x,idx_keep=idx_keep)\n",
    "            return x[:,0]\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        # else:\n",
    "        #x = x[:, 0]\n",
    "        #x = self.heads(x)\n",
    "\n",
    "        # return x\n",
    "# stemconfig = [ConvStemConfig(out_channels = 64, kernel_size = 3 , stride = 2) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MrtSLM5XX08w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2714, -0.6781, -0.6948,  ...,  0.1526,  0.8451, -2.2882],\n",
       "        [ 1.1049, -1.1516, -1.2511,  ..., -0.8798,  0.1034, -1.7765]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemconfig = [ConvStemConfig(out_channels = 64, kernel_size = 3 , stride = 2) for i in range(4)]\n",
    "x = torch.randn(2,3,224,224)\n",
    "VisionTransformer()(x,'anchor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q97CMPsBRCOF"
   },
   "source": [
    "# MSN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wNr1NT8n2m40"
   },
   "outputs": [],
   "source": [
    "# ! pip install lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bpb07aKp2kAe"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# from lightly.models import utils\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "U9LhQG-8Y_QL"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model:nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "R-7dmv136JjB"
   },
   "outputs": [],
   "source": [
    "def deactivate_requires_grad(model: nn.Module):\n",
    "    \"\"\"Deactivates the requires_grad flag for all parameters of a model.\n",
    "\n",
    "    This has the same effect as permanently executing the model within a `torch.no_grad()`\n",
    "    context. Use this method to disable gradient computation and therefore\n",
    "    training for a model.\n",
    "\n",
    "    Examples:\n",
    "        >>> backbone = resnet18()\n",
    "        >>> deactivate_requires_grad(backbone)\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q7dZ8GkR6ygF"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def update_momentum(model: nn.Module, model_ema: nn.Module, m: float):\n",
    "    \"\"\"Updates parameters of `model_ema` with Exponential Moving Average of `model`\n",
    "\n",
    "    Momentum encoders are a crucial component fo models such as MoCo or BYOL.\n",
    "\n",
    "    Examples:\n",
    "        >>> backbone = resnet18()\n",
    "        >>> projection_head = MoCoProjectionHead()\n",
    "        >>> backbone_momentum = copy.deepcopy(moco)\n",
    "        >>> projection_head_momentum = copy.deepcopy(projection_head)\n",
    "        >>>\n",
    "        >>> # update momentum\n",
    "        >>> update_momentum(moco, moco_momentum, m=0.999)\n",
    "        >>> update_momentum(projection_head, projection_head_momentum, m=0.999)\n",
    "    \"\"\"\n",
    "    for model_ema, model in zip(model_ema.parameters(), model.parameters()):\n",
    "        model_ema.data = model_ema.data * m + model.data * (1.0 - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RFvM_-Jyo3_w"
   },
   "outputs": [],
   "source": [
    "def random_token_mask(size: Tuple[int, int],\n",
    "                      mask_ratio: float = 0.6,\n",
    "                      mask_class_token: bool = False,\n",
    "                     ) -> torch.Tensor:\n",
    "    \"\"\"Creates random token masks.\n",
    "\n",
    "    Args:\n",
    "        size:\n",
    "            Size of the token batch for which to generate masks.\n",
    "            Should be (batch_size, sequence_length).\n",
    "        mask_ratio:\n",
    "            Percentage of tokens to mask.\n",
    "        mask_class_token:\n",
    "            If False the class token is never masked. If True the class token\n",
    "            might be masked.\n",
    "        device:\n",
    "            Device on which to create the index masks.\n",
    "\n",
    "    Returns:\n",
    "        A (index_keep, index_mask) tuple where each index is a tensor.\n",
    "        index_keep contains the indices of the unmasked tokens and has shape\n",
    "        (batch_size, num_keep). index_mask contains the indices of the masked\n",
    "        tokens and has shape (batch_size, sequence_length - num_keep).\n",
    "        num_keep is equal to sequence_length * (1- mask_ratio).\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, sequence_length = size\n",
    "    num_keep = int(sequence_length * (1 - mask_ratio))\n",
    "\n",
    "    noise = torch.rand(batch_size, sequence_length)\n",
    "    if not mask_class_token and sequence_length > 0:\n",
    "        # make sure that class token is not masked\n",
    "        noise[:, 0] = -1\n",
    "        num_keep = max(1, num_keep)\n",
    "\n",
    "    # get indices of tokens to keep\n",
    "    indices = torch.argsort(noise, dim=1)\n",
    "    idx_keep = indices[:, :num_keep]\n",
    "    idx_mask = indices[:, num_keep:]\n",
    "\n",
    "    return idx_keep, idx_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HuPHXqOPRJMD"
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int = 768,\n",
    "                 out_features: int = 2048,\n",
    "               bias: bool = False,\n",
    "              ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_block = nn.Sequential(nn.Linear(in_features= in_features,\n",
    "                                                   out_features= out_features,\n",
    "                                               bias=bias),\n",
    "                                         nn.LayerNorm(normalized_shape= out_features),\n",
    "                                         nn.GELU()\n",
    "                                        )\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.tensor:\n",
    "        x = self.dense_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxHs3WwQRllw",
    "outputId": "004d5fdf-e932-4f78-f2f2-d178b47846fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1536])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2,2,768)\n",
    "DenseBlock(768,768*2)(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "azM2ph3hRtfx"
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int = 512,\n",
    "                 hidden_features: int = 2048,\n",
    "                 out_features: int = 512,\n",
    "                 bias : bool = False\n",
    "                 ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.projection_head = nn.Sequential(DenseBlock(in_features= in_features,\n",
    "                                                    out_features= hidden_features,\n",
    "                                                    bias= bias\n",
    "                                                    ),\n",
    "                                         DenseBlock(in_features= hidden_features,\n",
    "                                                    out_features= hidden_features,\n",
    "                                                    bias= bias\n",
    "                                                    ),\n",
    "                                         nn.Linear(in_features= hidden_features,\n",
    "                                                   out_features= out_features,\n",
    "                                                   bias= bias\n",
    "                                                   )\n",
    "                                         )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        x = self.projection_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HqT_GdtHSoAo"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yZMPwdJFT4MX"
   },
   "outputs": [],
   "source": [
    "backbone = VisionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "V7hPJG6-b1OM"
   },
   "outputs": [],
   "source": [
    "class MSN(nn.Module):\n",
    "    def __init__(self,\n",
    "               backbone: nn.Module,\n",
    "               masking_ratio: float = 0.15,\n",
    "               ema_p: float = 0.996\n",
    "              ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.ema_p = ema_p\n",
    "\n",
    "        self.anchor_backbone = backbone\n",
    "        self.anchor_projection_head = ProjectionHead()\n",
    "\n",
    "        self.target_backbone = copy.deepcopy(self.anchor_backbone)\n",
    "        self.target_projection_head = copy.deepcopy(self.anchor_projection_head)\n",
    "\n",
    "        deactivate_requires_grad(self.target_backbone)\n",
    "        deactivate_requires_grad(self.target_projection_head)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                views:list[torch.tensor],\n",
    "                focal: bool = True\n",
    "                ) -> tuple[torch.Tensor]:\n",
    "\n",
    "        update_momentum(model= self.anchor_backbone,\n",
    "                        model_ema= self.target_backbone,\n",
    "                        m = self.ema_p,\n",
    "                       )\n",
    "        update_momentum(model= self.anchor_projection_head,\n",
    "                        model_ema= self.target_projection_head,\n",
    "                        m = self.ema_p,\n",
    "                       )\n",
    "        projections = self._forward_all(batch=views,focal=focal)\n",
    "        \n",
    "\n",
    "        return projections\n",
    "\n",
    "    def _target_forward(self,\n",
    "                        view: torch.tensor\n",
    "                        ) -> torch.Tensor:\n",
    "\n",
    "        target_encodings = self.target_backbone(x= view,\n",
    "                                                branch='target'\n",
    "                                                )\n",
    "        target_projections = self.target_projection_head(x= target_encodings)\n",
    "\n",
    "        return target_projections\n",
    "\n",
    "\n",
    "    def _anchor_forward(self,\n",
    "                        view: torch.tensor\n",
    "                        ) -> torch.Tensor:\n",
    "\n",
    "        batch_size, _, _, width = view.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, idx_mask = random_token_mask(size= (view.shape[0],seq_length),\n",
    "                                               mask_ratio= self.masking_ratio\n",
    "                                          )\n",
    "\n",
    "        anchor_encodings = self.anchor_backbone(x= view,\n",
    "                                                branch= 'anchor',\n",
    "                                                idx_keep= idx_keep)\n",
    "        anchor_projections = self.anchor_projection_head(x= anchor_encodings)\n",
    "\n",
    "        return anchor_projections\n",
    "    \n",
    "    \n",
    "    def _forward_all(self,\n",
    "                     batch: list,\n",
    "                     focal: bool = True\n",
    "                     ) -> torch.tensor:\n",
    "        target_view = batch[0]\n",
    "        anchor_view = batch[1]\n",
    "        focal_views = torch.concat(batch[2:],dim=0)\n",
    "        \n",
    "        target_projections = self._target_forward(target_view)\n",
    "        \n",
    "        anchor_projections_sim = self._anchor_forward(anchor_view)\n",
    "        if focal:\n",
    "            anchor_focal_projections_sim = self._anchor_forward(focal_views)\n",
    "            similarity_projections = self._arrange_tokens(anchor_projections_sim,\n",
    "                                                          anchor_focal_projections_sim,\n",
    "                                                          num_focal = 10)\n",
    "                                                          \n",
    "        \n",
    "        anchor_projections_age = self._anchor_forward(anchor_view)\n",
    "        if focal:\n",
    "            anchor_focal_projections_age =  self._anchor_forward(focal_views)\n",
    "            age_projections = self._arrange_tokens(anchor_projections_age,\n",
    "                                                   anchor_focal_projections_age,\n",
    "                                                   num_focal = 10)\n",
    "                                                    \n",
    "        \n",
    "        \n",
    "        anchor_projections_gender = self._anchor_forward(anchor_view)\n",
    "        if focal:\n",
    "            anchor_focal_projections_gender = self._anchor_forward(focal_views)  \n",
    "            gender_projections = self._arrange_tokens(anchor_projections_gender,\n",
    "                                                      anchor_focal_projections_gender,\n",
    "                                                      num_focal = 10)\n",
    "                                                          \n",
    "        if focal:\n",
    "            anchor_projections = torch.stack((similarity_projections,\n",
    "                                              age_projections,\n",
    "                                              gender_projections\n",
    "                                              ),\n",
    "                                         dim= 0)\n",
    "        else:\n",
    "            anchor_projections = torch.stack((anchor_projections_sim,\n",
    "                                              anchor_projections_age,\n",
    "                                              anchor_projections_gender\n",
    "                                              ),\n",
    "                                             dim= 1)       \n",
    "\n",
    "        return (anchor_projections,\n",
    "                target_projections)\n",
    "    \n",
    "    def _arrange_tokens(self,\n",
    "                        tensor1: torch.tensor,\n",
    "                        tensor2:torch.tensor,\n",
    "                        num_focal: int = 10\n",
    "                        ) ->torch.tensor:\n",
    "\n",
    "        a = torch.stack(torch.split(tensor1,1),0)\n",
    "        b = torch.stack(torch.split(tensor2,num_focal),0)\n",
    "        c = torch.cat((a.expand(-1,num_focal,-1),b),dim=1)[:,num_focal-1:]\n",
    "        arranged_tokens = torch.cat(c.split(1),1).squeeze(0)\n",
    "        return arranged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "TVWxlItMo8V8"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "x = [torch.randn(2,3,224,224),torch.randn(2,3,224,224)]\n",
    "y = [torch.randn(2,3,96,96)for i in range(10)]\n",
    "\n",
    "x.extend(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfZQ2sLoOjt4",
    "outputId": "a5674914-9f60-4fda-a2f2-f5a70a366297",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4704, -0.3376,  0.1522,  ..., -0.1661,  0.6276,  0.0915],\n",
       "          [-0.0845, -0.0505,  0.5235,  ..., -0.2112,  0.4576,  0.2529],\n",
       "          [-0.1418, -0.5804, -0.5138,  ...,  0.2369, -0.2312, -0.3145],\n",
       "          ...,\n",
       "          [-0.5032,  0.0192,  0.4612,  ..., -0.0613,  0.1134,  0.0996],\n",
       "          [-0.1924, -0.2111,  0.2520,  ..., -0.1567, -0.2568,  0.0364],\n",
       "          [-0.2364,  0.0917, -0.2849,  ..., -0.1833,  0.4666, -0.0425]],\n",
       " \n",
       "         [[-0.4215, -0.2740,  0.2386,  ..., -0.5361,  0.9382, -0.0598],\n",
       "          [-0.0657, -0.1054,  0.3207,  ..., -0.6392,  0.4943,  0.1487],\n",
       "          [-0.1788, -0.3178, -0.2859,  ...,  0.1638,  0.0708, -0.2906],\n",
       "          ...,\n",
       "          [-0.1527, -0.4237,  0.1908,  ..., -0.2720,  0.0829,  0.1354],\n",
       "          [-0.4077, -0.3229,  0.1643,  ..., -0.0764, -0.2964,  0.2302],\n",
       "          [-0.0745, -0.3376, -0.1103,  ..., -0.2582,  0.4418, -0.0499]],\n",
       " \n",
       "         [[-0.3160, -0.5049,  0.1559,  ..., -0.6149,  0.6547,  0.0198],\n",
       "          [ 0.1932,  0.0645,  0.7429,  ..., -0.5457,  0.3315,  0.2528],\n",
       "          [-0.0111, -0.4433, -0.0909,  ..., -0.2561, -0.0063, -0.4199],\n",
       "          ...,\n",
       "          [-0.4555,  0.2611,  0.3063,  ..., -0.3658,  0.2512, -0.2402],\n",
       "          [-0.3224, -0.1279,  0.2583,  ...,  0.0524, -0.3127, -0.0563],\n",
       "          [-0.2182,  0.0483, -0.1249,  ..., -0.1686,  0.1465,  0.1159]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[ 0.0100, -0.5113, -0.6490,  ...,  0.0017, -0.5561,  0.4104],\n",
       "          [-0.2082, -0.3751,  0.5161,  ..., -0.6149,  0.5128,  0.1529],\n",
       "          [-0.3591, -0.6157,  0.2134,  ..., -0.2254,  0.1165,  0.2187]],\n",
       " \n",
       "         [[ 0.0646, -0.4967,  0.0877,  ..., -0.3527, -0.1439,  0.1701],\n",
       "          [-0.3551, -0.3460,  0.0752,  ..., -0.3555,  0.3418, -0.0494],\n",
       "          [-0.5506, -0.3267,  0.3135,  ..., -0.4563,  0.2485,  0.0170]]]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSN(backbone)(x, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def arrange_tokens(tensor1: torch.tensor,\n",
    "                   tensor2:torch.tensor,\n",
    "                   num_focal: int = 10\n",
    "                   ) ->torch.tensor:\n",
    "\n",
    "    a = torch.stack(torch.split(tensor1,1),0)\n",
    "    b = torch.stack(torch.split(tensor2,num_focal),0)\n",
    "    c = torch.cat((a.expand(-1,num_focal,-1),b),dim=1)[:,num_focal-1:]\n",
    "    \n",
    "    return torch.cat(c.split(1),1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "rQ2_RAkemq2g"
   },
   "outputs": [],
   "source": [
    "def prototype_probabilities(queries: torch.tensor,\n",
    "                            prototypes: torch.tensor,\n",
    "                            temperature: float,\n",
    "                            ) -> torch.tensor:\n",
    "    \"\"\"Returns probability for each query to belong to each prototype.\n",
    "\n",
    "    Args:\n",
    "        queries:\n",
    "            Tensor with shape (batch_size, dim), projection head output\n",
    "        prototypes:\n",
    "            Tensor with shape (num_prototypes, dim)\n",
    "        temperature:\n",
    "            Inverse scaling factor for the similarity.\n",
    "\n",
    "    Returns:\n",
    "        Probability tensor with shape (batch_size, num_prototypes) which sums to 1 along\n",
    "        the num_prototypes dimension.\n",
    "\n",
    "    \"\"\"                           \n",
    "    return F.softmax(torch.matmul(queries, prototypes.T) / temperature, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(probabilities: torch.tensor, \n",
    "            temperature: float\n",
    "           ) -> torch.tensor:\n",
    "    \"\"\"Sharpens the probabilities with the given temperature.\n",
    "\n",
    "    Args:\n",
    "        probabilities:\n",
    "            Tensor with shape (batch_size, dim)\n",
    "        temperature:\n",
    "            Temperature in (0, 1]. Lower temperature results in stronger sharpening (\n",
    "            output probabilities are less uniform).\n",
    "    Returns:\n",
    "        Probabilities tensor with shape (batch_size, num_prototypes).\n",
    "\n",
    "    \"\"\"\n",
    "    probabilities = probabilities ** (1.0 / temperature)\n",
    "    probabilities /= torch.sum(probabilities, dim=1, keepdim=True)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sinkhorn(probabilities: torch.tensor,\n",
    "             iterations: int = 3,\n",
    "             gather_distributed: bool = False,\n",
    "            ) -> torch.tensor:\n",
    "    \"\"\"Runs sinkhorn normalization on the probabilities as described in [0].\n",
    "\n",
    "    Code inspired by [1].\n",
    "\n",
    "    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141\n",
    "    - [1]: https://github.com/facebookresearch/msn\n",
    "\n",
    "    Args:\n",
    "        probabilities:\n",
    "            Probabilities tensor with shape (batch_size, num_prototypes).\n",
    "        iterations:\n",
    "            Number of iterations of the sinkhorn algorithms. Set to 0 to disable.\n",
    "        gather_distributed:\n",
    "            If True then features from all gpus are gathered during normalization.\n",
    "    Returns:\n",
    "        A normalized probabilities tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    if iterations <= 0:\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    num_targets, num_prototypes = probabilities.shape\n",
    "    probabilities = probabilities.T\n",
    "    sum_probabilities = torch.sum(probabilities)\n",
    "\n",
    "    probabilities = probabilities / sum_probabilities\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # normalize rows\n",
    "        row_sum = torch.sum(probabilities, dim=1, keepdim=True)\n",
    "\n",
    "        probabilities /= row_sum\n",
    "        probabilities /= num_prototypes\n",
    "\n",
    "        # normalize columns\n",
    "        probabilities /= torch.sum(probabilities, dim=0, keepdim=True)\n",
    "        probabilities /= num_targets\n",
    "\n",
    "    probabilities *= num_targets\n",
    "    return probabilities.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_loss(mean_anchor_probs: torch.tensor\n",
    "                       ) -> torch.tensor:\n",
    "    \"\"\"Calculates mean entropy regularization loss.\"\"\"\n",
    "    loss = -torch.sum(torch.log(mean_anchor_probs ** (-mean_anchor_probs)))\n",
    "    loss += math.log(float(len(mean_anchor_probs)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototypes = nn.ModuleList([nn.Linear(512,1024),\n",
    "                           nn.Linear(512,1024),\n",
    "                           nn.Linear(512,1024)]\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "FGozTVpeYsrm"
   },
   "outputs": [],
   "source": [
    "class MSNLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 temperature: float = 0.1,\n",
    "                 sinkhorn_iterations: int = 3,\n",
    "                 similarity_weight: float = 1.0,\n",
    "                 age_weight: float = 1.0,\n",
    "                 gender_weight: float = 1.0,\n",
    "                 regularization_weight: float = 1.0,\n",
    "               ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.sinkhorn_iterations = sinkhorn_iterations\n",
    "        self.similarity_weight = similarity_weight\n",
    "        self.age_weight = age_weight\n",
    "        self.gender_weight = gender_weight\n",
    "        self.regularization_weight = regularization_weight\n",
    "    \n",
    "    \n",
    "    def forward(self,\n",
    "                anchors: torch.tensor,\n",
    "                targets: torch.tensor,\n",
    "                prototypes: torch.tensor,\n",
    "                target_sharpen_temperature: float = 0.25,\n",
    "                focal: bool = True\n",
    "                      ) -> torch.tensor:\n",
    "        \n",
    "        similarity_loss = self.similarity_weight * self._forward_loss(anchors=anchors[0] if focal else anchors[:,0],\n",
    "                                                                      targets=targets[:,0],\n",
    "                                                                      prototypes=prototypes[0].weight\n",
    "                                                                     )\n",
    "        age_loss = self.age_weight * self._forward_loss(anchors=anchors[1] if focal else anchors[:,1],\n",
    "                                                           targets=targets[:,1],\n",
    "                                                           prototypes=prototypes[1].weight\n",
    "                                                          )\n",
    "        gender_loss = self.gender_weight * self._forward_loss(anchors=anchors[2] if focal else anchors[:,2],\n",
    "                                                              targets=targets[:,2],\n",
    "                                                              prototypes=prototypes[2].weight\n",
    "                                                             )\n",
    "        \n",
    "        loss = similarity_loss + gender_loss + gender_loss\n",
    "        return loss\n",
    "    \n",
    "    def _forward_loss(self,\n",
    "                anchors: torch.tensor,\n",
    "                targets: torch.tensor,\n",
    "                prototypes: torch.tensor,\n",
    "                target_sharpen_temperature: float = 0.25,\n",
    "               ) -> torch.tensor:\n",
    "\n",
    "        num_views = anchors.shape[0] // targets.shape[0]\n",
    "        anchors = F.normalize(anchors, dim=1)\n",
    "        targets = F.normalize(targets, dim=1)\n",
    "        prototypes = F.normalize(prototypes, dim=1)\n",
    "\n",
    "        anchor_probs = prototype_probabilities(anchors, \n",
    "                                               prototypes, \n",
    "                                               temperature=self.temperature\n",
    "                                              )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_probs = prototype_probabilities(targets, \n",
    "                                                   prototypes, \n",
    "                                                   temperature=self.temperature\n",
    "                                                   )\n",
    "            target_probs = sharpen(target_probs, temperature=target_sharpen_temperature)\n",
    "            if self.sinkhorn_iterations > 0:\n",
    "                target_probs = sinkhorn(probabilities=target_probs,\n",
    "                                        iterations=self.sinkhorn_iterations,\n",
    "                                        )\n",
    "            target_probs = torch.repeat_interleave(target_probs, repeats=num_views, dim=0)\n",
    "\n",
    "        loss = torch.mean(torch.sum(torch.log(anchor_probs ** (-target_probs)), dim=1))\n",
    "\n",
    "        # regularization loss\n",
    "        if self.regularization_weight > 0:\n",
    "            mean_anchor_probs = torch.mean(anchor_probs, dim=0)\n",
    "            reg_loss = regularization_loss(mean_anchor_probs=mean_anchor_probs)\n",
    "            loss += self.regularization_weight * reg_loss\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "aoccG0BJsr0X"
   },
   "outputs": [],
   "source": [
    "focal = False\n",
    "a,b = MSN(backbone)(x, focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "-LOPY1ONsyjk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.3521, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSNLoss()(a, b,prototypes,focal=focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gt5sjygmYsmp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SnRJFH1FYsjf",
    "outputId": "c53c206f-a597-4c18-b4ab-b54890ff3d37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA6Fn_RDYsg5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZN3QbyoxYseC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqlbSVIiYsYj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEA6uT82YsV7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NAR0JgkYsTU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SICuTJZYsQ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRORG9nnYsOF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ci_Rz4amYsLh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXFghtHbYsGB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L69HyAS6YsDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuiXBby1YsA1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGA-UEYTYr9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jqX4qsmYr4k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFl78FRPYruq",
    "outputId": "3ac39185-f35c-4074-bcad-96c04c2e830a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb045fec710>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo2kXn4xa8Uu"
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                 images_size: int =  224,\n",
    "                 patch_size: int = 16,\n",
    "                 num_channels: int = 3,\n",
    "                 embedding_dim: int = 512\n",
    "                ) -> None:\n",
    "      super().__init__()\n",
    "      \"\"\"\n",
    "        Role: convert the input image of size = (b,c,h,w) into patches sequence\n",
    "              of size = (b,p,embedding_dim) where\n",
    "              b = batch size\n",
    "              c = number of channels\n",
    "              h = height\n",
    "              w = width\n",
    "\n",
    "        Inputs:\n",
    "            image_size - Dimensionality of the input image (width or height)\n",
    "            patch_size - Dimensionality of the image patch (width or height)\n",
    "            embedding_dim - Dimensionality of the patch embedding\n",
    "            num_channels - Number of channels in the input images (usually 3)\n",
    "      \"\"\"\n",
    "      self.image_size = images_size\n",
    "      self.patch_size = patch_size\n",
    "      self.num_channels = num_channels\n",
    "      self.hidden_size = embedding_dim\n",
    "\n",
    "      self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "      self.projection_layer = nn.Conv2d(in_channels= self.num_channels,\n",
    "                                        out_channels= self.hidden_size,\n",
    "                                        kernel_size= self.patch_size,\n",
    "                                        stride= self.patch_size\n",
    "                                       )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "      x = self.projection_layer(x)\n",
    "      x = x.flatten(2).transpose(1,2)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgIQjWfMbCad"
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                 images_size: int =  224,\n",
    "                 num_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embedding_dim: int = 512,\n",
    "                 num_cls_tokens: int = 3,\n",
    "                 dropout_p: float = 0.0,\n",
    "                 learnable_pos_encoding: bool = True\n",
    "                ) -> None:\n",
    "      super().__init__()\n",
    "      \"\"\"\n",
    "        Role: convert the input image into patches as well as adding CLS token\n",
    "        and positional encoding\n",
    "\n",
    "        Inputs:\n",
    "            image_size - Dimensionality of the input image (width or height)\n",
    "            num_channels - Number of channels in the input images (usually 3)\n",
    "            patch_size - Dimensionality of the image patch (width or height)\n",
    "            embedding_dim - Dimensionality of the patch embedding\n",
    "            num_cls_tokens - number of classification tokens to add to the sequence\n",
    "            dropout_p -  percentage of applied dropout\n",
    "            learnable_pos_encoding - specify typ of positional encoding (fixed or learnable)\n",
    "      \"\"\"\n",
    "      self.image_size = images_size\n",
    "      self.patch_size = patch_size\n",
    "      self.num_channels = num_channels\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.num_cls_tokens = num_cls_tokens\n",
    "      self.dropout_p = dropout_p\n",
    "      self.learnable_pos_encoding = learnable_pos_encoding\n",
    "\n",
    "      self.patch_embeddings = PatchEmbeddings(images_size= self.image_size,\n",
    "                                              patch_size= self.patch_size,\n",
    "                                              num_channels= self.num_channels,\n",
    "                                              embedding_dim= self.embedding_dim\n",
    "                                              )\n",
    "\n",
    "      self.cls_token = nn.Parameter(torch.randn(size= (1, self.num_cls_tokens, self.embedding_dim)))\n",
    "\n",
    "      num_patches = self.patch_embeddings.num_patches\n",
    "      if self.learnable_pos_encoding:\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.positional_embeddings = nn.Parameter(torch.empty(1, num_patches , self.embedding_dim).normal_(std=0.02))\n",
    "\n",
    "      else:\n",
    "        self.positional_embeddings = self._get_positional_embeddings(num_patches=num_patches,\n",
    "                                                                     embedding_dim=self.embedding_dim\n",
    "                                                                    )\n",
    "\n",
    "      self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "\n",
    "      x = self.patch_embeddings(x)\n",
    "      batch_size, _, _ = x.size()\n",
    "      cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "      x = torch.cat([cls_token,x], dim=1)\n",
    "      x = x + self.positional_embeddings\n",
    "      x = self.dropout(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "    # def _get_positional_embeddings(self,\n",
    "    #                              num_patches: int,\n",
    "    #                              embedding_dim: int\n",
    "    #                              ) -> torch.Tensor:\n",
    "    #   poaitional_embeddings  = torch.ones(num_patches + self.num_cls_tokens, embedding_dim)\n",
    "    #   for i in range(num_patches + self.num_cls_tokens):\n",
    "    #       for j in range(embedding_dim):\n",
    "    #           poaitional_embeddings [i][j] = np.sin(i / (10000 ** (j / embedding_dim))) \\\n",
    "    #           if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / embedding_dim)))\n",
    "    #   return poaitional_embeddings.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RB8v9TUbHOV"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self,\n",
    "               embedding_dim: int = 512,\n",
    "               hidden_dim = 192,\n",
    "               dropout_p: float = 0.0\n",
    "               ) -> None:\n",
    "      super().__init__()\n",
    "      \"\"\"\n",
    "        Role: Feed forward netork of attention head\n",
    "\n",
    "        Inputs:\n",
    "            embedding_dim - Dimensionality of the patch embedding\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "            dropout_p -  percentage of applied dropout\n",
    "            learnable_pos_encoding - specify typ of positional encoding (fixed or learnable)\n",
    "      \"\"\"\n",
    "      self.feed_forward = nn.Sequential(nn.Linear(in_features= embedding_dim,\n",
    "                                                  out_features= hidden_dim\n",
    "                                                 ),\n",
    "                                        nn.GELU(),\n",
    "                                        nn.Linear(in_features=hidden_dim,\n",
    "                                                  out_features=embedding_dim\n",
    "                                                 ),\n",
    "                                        nn.Dropout(dropout_p)\n",
    "                                       )\n",
    "    def forward(self,\n",
    "              x: torch.tensor\n",
    "              )-> torch.Tensor:\n",
    "      x = self.feed_forward(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWWhv6RubWr3"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 512,\n",
    "                 hidden_dim: int = 192,\n",
    "                 num_heads: int = 4,\n",
    "                 dropout_p=0.0,\n",
    "                 pre_layer_norm: bool= True\n",
    "                 ) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "      \"\"\" Role: capture the interaction between\n",
    "          Inputs:\n",
    "              embedding_dim - Dimensionality of input and attention feature vectors\n",
    "              hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "              num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "              dropout - Amount of dropout to apply in the feed-forward network\n",
    "              pre_layer_norm - Specifies where to apply the layer norm (before or after)\n",
    "      \"\"\"\n",
    "      self.pre_layer_norm = pre_layer_norm\n",
    "      self.layer_norm_1 = nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "\n",
    "      self.attention_head = nn.MultiheadAttention(embed_dim= embedding_dim,\n",
    "                                                  num_heads= num_heads,\n",
    "                                                  dropout= dropout_p)\n",
    "\n",
    "      self.layer_norm_2 = nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "\n",
    "      self.feed_forward = FeedForwardNet(embedding_dim= embedding_dim,\n",
    "                                         hidden_dim= hidden_dim,\n",
    "                                         dropout_p= dropout_p\n",
    "                                        )\n",
    "\n",
    "    #attention weights from here\n",
    "    def forward(self,\n",
    "                x: torch.tensor\n",
    "                )-> torch.Tensor:\n",
    "\n",
    "        if self.pre_layer_norm:\n",
    "          normalization_out1 = self.layer_norm_1(x)\n",
    "          attention_out = self.attention_head(normalization_out1, normalization_out1, normalization_out1)[0]\n",
    "          addition_out1 = x + attention_out\n",
    "          normalization_out2 = self.layer_norm_2(addition_out1)\n",
    "          feed_forward_out = self.feed_forward(normalization_out2)\n",
    "          layer_out = addition_out1 + feed_forward_out\n",
    "\n",
    "        else:\n",
    "          attention_out = self.attention_head(x,x,x)[0]\n",
    "          addition_out1 = x + attention_out\n",
    "          normalization_out = self.layer_norm_1(addition_out1)\n",
    "          feed_forward_out = self.feed_forward(normalization_out)\n",
    "          addition_out2 = normalization_out + feed_forward_out\n",
    "          layer_out = self.layer_norm_2(addition_out2)\n",
    "\n",
    "        return attention_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGe0sAbCbcSF"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers: int = 6,\n",
    "                 num_heads: int = 4,\n",
    "                 hidden_dim: int = 192,\n",
    "                 pre_layer_norm: bool= True,\n",
    "                 images_size: int =  224,\n",
    "                 num_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embedding_dim: int = 512,\n",
    "                 num_cls_tokens: int = 3,\n",
    "                 dropout_p: float = 0.0,\n",
    "                 learnable_pos_encoding: bool = False\n",
    "                ) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "\n",
    "      self.image_size = images_size\n",
    "      self.patch_size = patch_size\n",
    "      self.num_channels = num_channels\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.num_cls_tokens = num_cls_tokens\n",
    "      self.dropout_p = dropout_p\n",
    "      self.learnable_pos_encoding = learnable_pos_encoding\n",
    "\n",
    "      self.input_embeddings = PatchEmbeddings(images_size= self.image_size,\n",
    "                                              num_channels= self.num_channels,\n",
    "                                              patch_size= self.patch_size,\n",
    "                                              embedding_dim= self.embedding_dim,\n",
    "                                              )\n",
    "                                              # num_cls_tokens= self.num_cls_tokens,\n",
    "                                              # dropout_p=self.dropout_p,\n",
    "                                              # learnable_pos_encoding=self.learnable_pos_encoding)\n",
    "\n",
    "\n",
    "\n",
    "      self.num_layers = num_layers\n",
    "      self.num_heads = num_heads\n",
    "      self.hidden_dim = hidden_dim\n",
    "      self.pre_layer_norm = pre_layer_norm\n",
    "\n",
    "      self.attention_block = AttentionBlock(embedding_dim= self.embedding_dim,\n",
    "                                            hidden_dim= self.hidden_dim,\n",
    "                                            num_heads= self.num_heads,\n",
    "                                            dropout_p=self.dropout_p,\n",
    "                                            pre_layer_norm=self.pre_layer_norm)\n",
    "\n",
    "\n",
    "      self.cls_token = nn.Parameter(torch.randn(size= (1, self.num_cls_tokens, self.embedding_dim)))\n",
    "      num_patches = self.input_embeddings.num_patches\n",
    "      self.positional_embeddings = nn.Parameter(torch.empty(1, num_patches + self.num_cls_tokens, self.embedding_dim).normal_(std=0.02))\n",
    "\n",
    "      self.encoder = nn.Sequential(*[self.attention_block for layer in range(num_layers)])\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.tensor,\n",
    "                branch: str = 'target'\n",
    "                )-> torch.Tensor:\n",
    "\n",
    "      if branch == 'target':\n",
    "        x = self.input_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "        x = torch.cat([cls_token,x], dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.encoder(x)\n",
    "        return x#[:,0:self.num_cls_tokens]\n",
    "\n",
    "      elif branch == 'anchor':\n",
    "        x = self.input_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "        x = torch.cat([cls_token,x], dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = torch.cat((x[:,:1],x[:,self.num_cls_tokens:]),dim = 1)\n",
    "        x = self.encoder(x)\n",
    "        return x#[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "IEXComnnbk7u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tensor([[1,2,3],[4,5,6]])\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.repeat((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.repeat(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([h for _ in range(5)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chexmsn-env",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
