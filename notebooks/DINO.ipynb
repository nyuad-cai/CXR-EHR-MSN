{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e07a0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple, Optional\n",
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "from lightly.models.modules.masked_autoencoder import MAEBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181b900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = DINOTransform(cj_prob=0,random_gray_scale=0,gaussian_blur=(0,0,0),sigmas=(0,0),solarization_prob=0)\n",
    "\n",
    "class BaselinesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str, \n",
    "\n",
    "                 ) -> None:\n",
    "      \n",
    "        self.data_dir = data_dir\n",
    "        self.all_images = os.listdir(self.data_dir)\n",
    "        for image in self.all_images:\n",
    "            if image.startswith('._'):\n",
    "                self.all_images.remove(image)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self,index: int\n",
    "                    ) -> Tuple[torch.Tensor]:\n",
    "        name = self.all_images[index]\n",
    "        path = os.path.join(self.data_dir, name)\n",
    "        img = Image.open(fp=path).convert('RGB')\n",
    "        img = transform(img)\n",
    "        return img, index, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7d71150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 learning_rate: float= 0.0001,\n",
    "                 weight_decay:float= 0.000,\n",
    "                 max_epochs: int = 100,\n",
    "                 \n",
    "                )-> None:\n",
    "        super().__init__()\n",
    "#         resnet = torchvision.models.resnet18()\n",
    "#         backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "#         input_dim = 512\n",
    "        # instead of a resnet you can also use a vision transformer backbone as in the\n",
    "        # original paper (you might have to reduce the batch size in this case):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        vit = torchvision.models.VisionTransformer(image_size=224,\n",
    "                                                   patch_size=16,\n",
    "                                                   num_layers=12,\n",
    "                                                   num_heads=6,\n",
    "                                                   hidden_dim=192,\n",
    "                                                   mlp_dim=192 * 4,\n",
    "                                                   )\n",
    "        backbone =MAEBackbone.from_vit(vit)\n",
    "        input_dim = backbone.hidden_dim\n",
    "\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 768, 256, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 768, 256, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = DINOLoss(output_dim=2048, warmup_teacher_temp_epochs=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        momentum = cosine_schedule(self.current_epoch, 10, 0.996, 1)\n",
    "        update_momentum(self.student_backbone, self.teacher_backbone, m=momentum)\n",
    "        update_momentum(self.student_head, self.teacher_head, m=momentum)\n",
    "        views = batch[0]\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=True , logger=True,prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        self.student_head.cancel_last_layer_gradients(current_epoch=self.current_epoch)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = optim.AdamW(params=self.parameters(), \n",
    "                                   lr=self.learning_rate, \n",
    "                                   weight_decay=self.weight_decay\n",
    "                                   )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                         eta_min=0,\n",
    "                                                         T_max=self.max_epochs\n",
    "                                                         )\n",
    "        \n",
    "        return {'optimizer': optimizer,\n",
    "                'lr_scheduler': scheduler\n",
    "               }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0fc4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DINO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c805511",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = DINOTransform()\n",
    "# we ignore object detection annotations by setting target_transform to return 0\n",
    "data_dir = '/scratch/fs999/shamoutlab/data/physionet.org/files/mimic-cxr-jpg/2.0.0/resized/'\n",
    "dataset = BaselinesDataset(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d8735b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=24,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0 | student_backbone | MAEBackbone        | 5.7 M \n",
      "1 | student_head     | DINOProjectionHead | 1.5 M \n",
      "2 | teacher_backbone | MAEBackbone        | 5.7 M \n",
      "3 | teacher_head     | DINOProjectionHead | 1.5 M \n",
      "4 | criterion        | DINOLoss           | 0     \n",
      "--------------------------------------------------------\n",
      "7.2 M     Trainable params\n",
      "7.2 M     Non-trainable params\n",
      "14.4 M    Total params\n",
      "57.435    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  75%|███████▌  | 4446/5892 [22:23<07:17,  3.31it/s, v_num=4, train_loss_step=1.940, train_loss_epoch=1.800]"
     ]
    }
   ],
   "source": [
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer = pl.Trainer(max_epochs=100, \n",
    "                     devices='auto', \n",
    "                     accelerator='auto',\n",
    "                     precision='16-mixed',\n",
    "                     log_every_n_steps=1,)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041792c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834bd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chexmsn-env\n",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
