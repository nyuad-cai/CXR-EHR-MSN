{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2e52e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import lightly.models.utils as utils\n",
    "from typing import List\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from lightly.utils.benchmarking import BenchmarkModule\n",
    "import torch.distributed as dist\n",
    "from lightly.loss import MSNLoss\n",
    "from lightly.models.modules.heads import MSNProjectionHead\n",
    "from lightly.models.modules.masked_autoencoder import MAEBackbone\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d2ca0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tensor(tensor: torch.tensor):\n",
    "    rows,cols = tensor.size()[0], tensor.size()[1]\n",
    "    for i in range(rows):\n",
    "        rand_idx = random.randint(0,cols-1) \n",
    "        flag = True\n",
    "        while flag:\n",
    "            if tensor[i,rand_idx] == 0.0:\n",
    "                rand_idx = random.randint(0,cols-1)\n",
    "            else:\n",
    "                flag = False\n",
    "        tensor[i,rand_idx] = 0.0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e1fc1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSN2(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_ratio = 0.15\n",
    "        \n",
    "        # ehr embedding layer\n",
    "        self.ehr_embed = nn.Linear(in_features=3,out_features=64)\n",
    "        \n",
    "        # cxr-ehr project layer\n",
    "        self.ehr_cxr_project = nn.Linear(in_features=64+192,out_features=192)\n",
    "        \n",
    "        vit = torchvision.models.VisionTransformer(image_size=224,\n",
    "                                                   patch_size=16,\n",
    "                                                   num_layers=12,\n",
    "                                                   num_heads=6,\n",
    "                                                   hidden_dim=192,\n",
    "                                                   mlp_dim=192*4)\n",
    "        \n",
    "        self.backbone = MAEBackbone.from_vit(vit)\n",
    "        self.projection_head = MSNProjectionHead(192,2048,1024)\n",
    "\n",
    "        self.anchor_backbone = copy.deepcopy(self.backbone)\n",
    "        self.anchor_projection_head = copy.deepcopy(self.projection_head)\n",
    "        self.anchor_ehr_cxr_project = copy.deepcopy(self.ehr_cxr_project)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone)\n",
    "        utils.deactivate_requires_grad(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.ehr_cxr_project)\n",
    "        \n",
    "        self.prototypes = nn.Linear(1024, 1024, bias=False).weight\n",
    "        self.criterion = MSNLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.anchor_backbone, self.backbone, 0.996)\n",
    "        utils.update_momentum(self.anchor_projection_head, self.projection_head, 0.996)\n",
    "        utils.update_momentum(self.anchor_ehr_cxr_project, self.ehr_cxr_project, 0.996)\n",
    "        \n",
    "        views, ehr = batch[0], batch[1]\n",
    "        ehr_masked = mask_tensor(ehr)\n",
    "        views = [view.to(self.device, non_blocking=True) for view in views]\n",
    "        ehr = ehr.to(self.device)\n",
    "        ehr_masked = ehr_masked.to(self.device)\n",
    "        \n",
    "        targets = views[0]\n",
    "        anchors = views[1]\n",
    "        anchors_focal = torch.concat(views[2:], dim=0)\n",
    "        \n",
    "        ehr_embed = self.ehr_embed(ehr)\n",
    "        ehr_masked_embed = self.ehr_embed(ehr_masked)\n",
    "        ehr_masked_embed = ehr_embed.repeat((11,1))\n",
    "        \n",
    "\n",
    "        targets_out = self.backbone(targets)\n",
    "        targets_out = self.ehr_cxr_project(torch.cat((targets_out,ehr_embed),1))\n",
    "        targets_out = self.projection_head(targets_out)\n",
    "        \n",
    "        anchors_out = self.encode_masked(anchors)\n",
    "        anchors_focal_out = self.encode_masked(anchors_focal)\n",
    "        anchors_out = torch.cat([anchors_out, anchors_focal_out], dim=0)\n",
    "        anchors_out = self.anchor_ehr_cxr_project(torch.cat((anchors_out,ehr_masked_embed),1))\n",
    "        anchors_out = self.anchor_projection_head(anchors_out)\n",
    "        \n",
    "        loss = self.criterion(anchors_out, targets_out, self.prototypes.data) \n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def encode_masked(self, anchors):\n",
    "        batch_size, _, _, width = anchors.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, _ = utils.random_token_mask(\n",
    "            size=(batch_size, seq_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=self.device,\n",
    "        )\n",
    "        out = self.anchor_backbone(anchors, idx_keep=idx_keep)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr=0.0001,\n",
    "                                      weight_decay=0.001)\n",
    "        \n",
    "#         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "#                                                                eta_min=0.0000001,\n",
    "#                                                                T_max=100)\n",
    "        return {'optimizer': optimizer,\n",
    "               #'lr_scheduler': scheduler\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9e8b795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/pytorch_lightning/core/module.py:420: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.9527, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MSN2()\n",
    "model.training_step(next(iter(dataloader)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e6885703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional,Dict,Union\n",
    "from torch.utils.data import Dataset\n",
    "from lightly.transforms.msn_transform import MSNTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1d269f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL.Image import Image\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "IMAGENET_STAT = {\"mean\":torch.tensor([0.4884, 0.4550, 0.4171]),\n",
    "                 \"std\":torch.tensor([0.2596, 0.2530, 0.2556])}\n",
    "\n",
    "# MIMIC_NORMALIZE ={\"mean\":torch.tensor([0.4723, 0.4723, 0.4723]), \n",
    "#                   \"std\":torch.tensor([0.3023, 0.3023, 0.3023])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c59dc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "class MSNTransform(MultiViewTransform):\n",
    "    \"\"\"Implements the transformations for MSN [0].\n",
    "\n",
    "    Input to this transform:\n",
    "        PIL Image or Tensor.\n",
    "\n",
    "    Output of this transform:\n",
    "        List of Tensor of length 2 * random_views + focal_views. (12 by default)\n",
    "\n",
    "    Applies the following augmentations by default:\n",
    "        - Random resized crop\n",
    "        - Random horizontal flip\n",
    "        - ImageNet normalization\n",
    "\n",
    "    Generates a set of random and focal views for each input image. The generated output\n",
    "    is (views, target, filenames) where views is list with the following entries:\n",
    "    [random_views_0, random_views_1, ..., focal_views_0, focal_views_1, ...].\n",
    "\n",
    "    - [0]: Masked Siamese Networks, 2022: https://arxiv.org/abs/2204.07141\n",
    "\n",
    "    Attributes:\n",
    "        random_size:\n",
    "            Size of the random image views in pixels.\n",
    "        focal_size:\n",
    "            Size of the focal image views in pixels.\n",
    "        random_views:\n",
    "            Number of random views to generate.\n",
    "        focal_views:\n",
    "            Number of focal views to generate.\n",
    "        random_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops for the relative to random_size.\n",
    "        focal_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops relative to focal_size.\n",
    "        hf_prob:\n",
    "            Probability that horizontal flip is applied.\n",
    "        vf_prob:\n",
    "            Probability that vertical flip is applied.\n",
    "        normalize:\n",
    "            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_size: int = 224,\n",
    "        focal_size: int = 96,\n",
    "        random_views: int = 2,\n",
    "        focal_views: int = 10,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        random_crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        focal_crop_scale: Tuple[float, float] = (0.05, 0.3),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.0,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_STAT,\n",
    "    ):\n",
    "        random_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=random_size,\n",
    "            crop_scale=random_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        focal_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=focal_size,\n",
    "            crop_scale=focal_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        transforms = [random_view_transform] * random_views\n",
    "        transforms += [focal_view_transform] * focal_views\n",
    "        super().__init__(transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "312397f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNViewTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        crop_size: int = 224,\n",
    "        crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.0,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_STAT,\n",
    "    ):\n",
    "\n",
    "        transform = [\n",
    "            T.RandomAffine(degrees=affine_dgrees, \n",
    "                          scale=affine_scale, \n",
    "                          shear=affine_shear, \n",
    "                          translate=affine_translate),\n",
    "            T.RandomResizedCrop(size=crop_size, scale=crop_scale),\n",
    "            T.RandomHorizontalFlip(p=hf_prob),\n",
    "            T.RandomVerticalFlip(p=vf_prob),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=normalize[\"mean\"], std=normalize[\"std\"]),\n",
    "        ]\n",
    "\n",
    "        self.transform = T.Compose(transform)\n",
    "\n",
    "    def __call__(self, image: Union[torch.Tensor, Image]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies the transforms to the input image.\n",
    "\n",
    "        Args:\n",
    "            image:\n",
    "                The input image to apply the transforms to.\n",
    "\n",
    "        Returns:\n",
    "            The transformed image.\n",
    "\n",
    "        \"\"\"\n",
    "        transformed = self.transform(image)\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a08be9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import Tuple, Optional\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch import Tensor\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0bfd14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexMSNDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str,\n",
    "                 transforms: nn.Module,\n",
    "                 ) -> None:\n",
    "      \n",
    "        self.meta = pd.read_csv(data_dir)\n",
    "        self.transforms = transforms\n",
    "        self.ehr = self.meta.to_numpy()[:,8:].astype('float32')\n",
    "\n",
    "        \n",
    "    def __len__(self\n",
    "                ) -> int:\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self,\n",
    "                    index: int\n",
    "                    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        \n",
    "        img = self.meta['path'][index]\n",
    "        img = Image.open(fp=img).convert('RGB')\n",
    "        img = self.transforms(img)\n",
    "        ehr = torch.from_numpy(self.ehr[index])\n",
    "\n",
    "        \n",
    "        return img, ehr\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "52eefbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/meta-age-norm.csv'\n",
    "# pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b482bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChexMSNDataset(data_dir,transforms=MSNTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "38ae21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=24,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f0b0ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d98d41cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d4c91b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MSN2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408d51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "64ccfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/sas10092/.conda/envs/chexmsn-env/lib/python3.9 ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=100,\n",
    "                     log_every_n_steps=1,\n",
    "                     precision='16-mixed'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b0432",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type              | Params\n",
      "-------------------------------------------------------------\n",
      "0 | ehr_embed              | Linear            | 256   \n",
      "1 | ehr_cxr_project        | Linear            | 49.3 K\n",
      "2 | backbone               | MAEBackbone       | 5.7 M \n",
      "3 | projection_head        | MSNProjectionHead | 6.7 M \n",
      "4 | anchor_backbone        | MAEBackbone       | 5.7 M \n",
      "5 | anchor_projection_head | MSNProjectionHead | 6.7 M \n",
      "6 | anchor_ehr_cxr_project | Linear            | 49.3 K\n",
      "7 | criterion              | MSNLoss           | 0     \n",
      "  | other params           | n/a               | 1.0 M \n",
      "-------------------------------------------------------------\n",
      "13.5 M    Trainable params\n",
      "12.5 M    Non-trainable params\n",
      "26.0 M    Total params\n",
      "103.881   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  17%|█▋        | 992/5879 [02:00<09:52,  8.25it/s, v_num=1, train_loss_step=4.970, train_loss_epoch=4.970] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:   4%|▍         | 247/5879 [00:34<13:02,  7.20it/s, v_num=1, train_loss_step=4.840, train_loss_epoch=4.960] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  91%|█████████▏| 5365/5879 [10:22<00:59,  8.62it/s, v_num=1, train_loss_step=4.900, train_loss_epoch=4.960]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  73%|███████▎  | 4276/5879 [08:17<03:06,  8.59it/s, v_num=1, train_loss_step=5.030, train_loss_epoch=4.940]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  60%|██████    | 3537/5879 [06:51<04:32,  8.59it/s, v_num=1, train_loss_step=4.930, train_loss_epoch=4.930]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:  51%|█████     | 3011/5879 [05:48<05:31,  8.65it/s, v_num=1, train_loss_step=4.820, train_loss_epoch=4.840]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869987d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ba8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4f45b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chexmsn-env\n",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
