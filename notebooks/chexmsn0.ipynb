{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed2e52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import lightly.models.utils as utils\n",
    "from typing import List\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from lightly.utils.benchmarking import BenchmarkModule\n",
    "import torch.distributed as dist\n",
    "from lightly.loss import MSNLoss\n",
    "from lightly.models.modules.heads import MSNProjectionHead\n",
    "from lightly.models.modules.masked_autoencoder import MAEBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0459685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSN(BenchmarkModule):\n",
    "    def __init__(self,\n",
    "               dataloader_kNN: DataLoader, \n",
    "               num_classes: int = 2,\n",
    "               knn_k: int = 5,\n",
    "               knn_t: float = 0.1,\n",
    "               mask_ratio: float = 0.15,\n",
    "               lr : float = 0.1,\n",
    "               prototypes_num: int = 1024,\n",
    "               weight_decay: float = 0.0,\n",
    "               max_epochs: int = 100\n",
    "               ) -> None:\n",
    "        \n",
    "        super().__init__(dataloader_kNN, num_classes,knn_k,knn_t)\n",
    "        #self.save_hyperparameters() \n",
    "        self.weight_decay = weight_decay\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.lr = lr\n",
    "        self.prototypes_num = prototypes_num\n",
    "        self.max_epochs = max_epochs\n",
    "       \n",
    "        vit = torchvision.models.vit_b_16(weights=None)\n",
    "        self.backbone = MAEBackbone.from_vit(vit)\n",
    "        self.projection_head = MSNProjectionHead(768)\n",
    "\n",
    "        self.anchor_backbone = copy.deepcopy(self.backbone)\n",
    "        self.anchor_projection_head = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone)\n",
    "        utils.deactivate_requires_grad(self.projection_head)\n",
    "\n",
    "        self.prototypes = nn.Linear(256, self.prototypes_num, bias=False).weight\n",
    "        self.criterion = MSNLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.anchor_backbone, self.backbone, 0.996)\n",
    "        utils.update_momentum(self.anchor_projection_head, self.projection_head, 0.996)\n",
    "\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device, non_blocking=True) for view in views]\n",
    "        targets = views[0]\n",
    "        anchors = views[1]\n",
    "        anchors_focal = torch.concat(views[2:], dim=0)\n",
    "\n",
    "        targets_out = self.backbone(targets)\n",
    "        targets_out = self.projection_head(targets_out)\n",
    "        anchors_out = self.encode_masked(anchors)\n",
    "        anchors_focal_out = self.encode_masked(anchors_focal)\n",
    "        anchors_out = torch.cat([anchors_out, anchors_focal_out], dim=0)\n",
    "\n",
    "        loss = self.criterion(anchors_out, targets_out, self.prototypes.data)\n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "\n",
    "    def encode_masked(self, anchors):\n",
    "        batch_size, _, _, width = anchors.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, _ = utils.random_token_mask(size=(batch_size, seq_length),\n",
    "                                              mask_ratio=self.mask_ratio,\n",
    "                                              device=self.device,\n",
    "                                              )\n",
    "        out = self.anchor_backbone(anchors, idx_keep)\n",
    "        return self.anchor_projection_head(out)\n",
    "    \n",
    "    \n",
    "#     def on_validation_epoch_end(self, outputs):\n",
    "#         device = self.dummy_param.device\n",
    "#         if outputs:\n",
    "#             total_num = torch.Tensor([0]).to(device)\n",
    "#             total_top1 = torch.Tensor([0.]).to(device)\n",
    "#             for (num, top1) in outputs:\n",
    "#                 total_num += num[0]\n",
    "#                 total_top1 += top1\n",
    "             \n",
    "\n",
    "#             acc = float(total_top1.item() / total_num.item())\n",
    "#             if acc > self.max_accuracy:\n",
    "#                 self.max_accuracy = acc\n",
    "#             self.log('kNN_accuracy', acc * 100.0, prog_bar=True, on_epoch=True, logger=True,)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [\n",
    "            *list(self.anchor_backbone.parameters()),\n",
    "            *list(self.anchor_projection_head.parameters()),\n",
    "            self.prototypes,\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                               eta_min=0.00001,\n",
    "                                                               T_max=self.max_epochs\n",
    "                                                            )\n",
    "        return {'optimizer': optimizer,\n",
    "               'lr_scheduler': scheduler\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e6885703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional\n",
    "from torch.utils.data import Dataset\n",
    "from lightly.transforms.msn_transform import MSNTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6552aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_dir:str,\n",
    "               paths: list,\n",
    "               split:str,\n",
    "               ) -> Tuple[List]:\n",
    "    CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "                'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "                'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "                'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "        \n",
    "    filenames_to_path = {path.split('/')[-1].split('.')[0]: path for path in paths}\n",
    "    metadata = pd.read_csv(os.path.join(data_dir,'mimic-cxr-2.0.0-metadata.csv'))\n",
    "    labels = pd.read_csv(os.path.join(data_dir,'mimic-cxr-2.0.0-chexpert.csv'))\n",
    "    labels[CLASSES] = labels[CLASSES].fillna(0)\n",
    "    labels = labels.replace(-1.0, 0.0)\n",
    "    splits = pd.read_csv(os.path.join(data_dir,'mimic-cxr-ehr-split.csv'))\n",
    "    metadata_with_labels = metadata.merge(labels[CLASSES+['study_id'] ], how='inner', on='study_id')\n",
    "    filesnames_to_labels = dict(zip(metadata_with_labels['dicom_id'].values, metadata_with_labels[CLASSES].values))\n",
    "    filenames_loaded = splits.loc[splits.split==split]['dicom_id'].values\n",
    "    \n",
    "    filenames_loaded = [filename  for filename in filenames_loaded if filename in filesnames_to_labels]\n",
    "\n",
    "    return filenames_to_path, filenames_loaded, filesnames_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d269f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICCXR(Dataset):\n",
    "    def __init__(self, \n",
    "                 paths: str,\n",
    "                 data_dir: str, \n",
    "                 transform: Optional[T.Compose] = None, \n",
    "                 split: str = 'validate',\n",
    "                 percentage:float = 1.0\n",
    "                 ) -> None:\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.filenames_to_path, \\\n",
    "        self.filenames_loaded, \\\n",
    "        self.filesnames_to_labels = preprocess(data_dir=self.data_dir,\n",
    "                                               paths=paths,\n",
    "                                               split=split\n",
    "                                              )\n",
    "        limit = (round(len(self.filenames_loaded) * percentage))\n",
    "        self.filenames_loaded = self.filenames_loaded[0:limit]\n",
    " \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, str):\n",
    "            img = Image.open(self.filenames_to_path[index]).convert('RGB')\n",
    "            labels = torch.tensor(self.filesnames_to_labels[index]).float()\n",
    "\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, labels\n",
    "        \n",
    "        filename = self.filenames_loaded[index]\n",
    "        \n",
    "        img = Image.open(self.filenames_to_path[filename]).convert('RGB')\n",
    "\n",
    "        labels = torch.tensor(self.filesnames_to_labels[filename]).float()\n",
    "        \n",
    "            \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames_loaded)\n",
    "    \n",
    "\n",
    "\n",
    "transform=MSNTransform(cj_prob=0,gaussian_blur=0,random_gray_scale=0)\n",
    "\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str, \n",
    "\n",
    "                 ) -> None:\n",
    "      \n",
    "        self.data_dir = data_dir\n",
    "        self.all_images = os.listdir(self.data_dir)\n",
    "        for image in self.all_images:\n",
    "            if image.startswith('._'):\n",
    "                self.all_images.remove(image)\n",
    "        \n",
    "    def __len__(self\n",
    "                ) -> int:\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self,\n",
    "                    index: int\n",
    "                    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        \n",
    "        name = self.all_images[index]\n",
    "        path = os.path.join(self.data_dir, name)\n",
    "        img = Image.open(fp=path).convert('RGB')\n",
    "\n",
    "        img = transform(img)\n",
    "\n",
    "\n",
    "\n",
    "        return img, index, name\n",
    "\n",
    "\n",
    "\n",
    "class MIMICVal(Dataset):\n",
    "    def __init__(self, \n",
    "                 paths: str,\n",
    "                 data_dir: str, \n",
    "                 transform: Optional[T.Compose] = None, \n",
    "                 split: str = 'val'\n",
    "                 ) -> None:\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.filenames_to_path, self.filenames_loaded, self.filesnames_to_labels = preprocess(data_dir=self.data_dir,\n",
    "                                                                                              paths=paths,\n",
    "                                                                                              split=split\n",
    "                                                                                              )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, str):\n",
    "            img = Image.open(self.filenames_to_path[index]).convert('RGB')\n",
    "            labels = torch.tensor(self.filesnames_to_labels[index]).float()\n",
    "            if labels[8] == 1.0:\n",
    "                label = torch.tensor(0)\n",
    "            else:\n",
    "                label = torch.tensor(1)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, label, index\n",
    "        \n",
    "        filename = self.filenames_loaded[index]\n",
    "        \n",
    "        img = Image.open(self.filenames_to_path[filename]).convert('RGB')\n",
    "\n",
    "        labels = torch.tensor(self.filesnames_to_labels[filename]).float()\n",
    "        \n",
    "        if labels[8] == 1.0:\n",
    "            label = torch.tensor(0)\n",
    "        else:\n",
    "            label = torch.tensor(1)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "52eefbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/scratch/fs999/shamoutlab/data/physionet.org/files/mimic-cxr-jpg/2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b482bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PretrainDataset(data_dir=os.path.join(data_dir,'resized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38ae21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e210f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_STAT = {\"mean\":torch.tensor([0.4884, 0.4550, 0.4171]),\n",
    "                 \"std\":torch.tensor([0.2596, 0.2530, 0.2556])}\n",
    "\n",
    "val_test_transforms = T.Compose([T.Resize(256),\n",
    "                                 T.CenterCrop(224),\n",
    "                                 T.ToTensor(),\n",
    "                                 T.Normalize(mean=IMAGENET_STAT[\"mean\"],\n",
    "                                             std=IMAGENET_STAT[\"std\"])                                                                \n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a10f98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = paths = glob.glob(os.path.join(data_dir,'resized','**','*.jpg'), recursive=True)\n",
    "val_dataset = MIMICVal(paths=paths,\n",
    "                       data_dir=data_dir,\n",
    "                       split='validate',\n",
    "                       transform = val_test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8132ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(dataset= val_dataset,\n",
    "                            batch_size=64,\n",
    "                            drop_last=True,\n",
    "                            num_workers=16,\n",
    "                            pin_memory=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d4c91b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MSN(dataloader_kNN=val_dataloader,\n",
    "           num_classes=2,\n",
    "           knn_k=20,\n",
    "           knn_t=0.2,\n",
    "           mask_ratio=0.15,\n",
    "           lr=0.0001,\n",
    "           prototypes_num=1024,\n",
    "           weight_decay=0.001,\n",
    "           max_epochs=100\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "64ccfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/sas10092/.conda/envs/chexmsn-env/lib/python3.9 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=100,\n",
    "                     log_every_n_steps=1,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "437b0432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Missing logger folder: /scratch/sas10092/ChexMSN/notebooks/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type              | Params\n",
      "-------------------------------------------------------------\n",
      "0 | backbone               | MAEBackbone       | 86.6 M\n",
      "1 | projection_head        | MSNProjectionHead | 6.3 M \n",
      "2 | anchor_backbone        | MAEBackbone       | 86.6 M\n",
      "3 | anchor_projection_head | MSNProjectionHead | 6.3 M \n",
      "4 | criterion              | MSNLoss           | 0     \n",
      "  | other params           | n/a               | 262 K \n",
      "-------------------------------------------------------------\n",
      "93.1 M    Trainable params\n",
      "92.9 M    Non-trainable params\n",
      "185 M     Total params\n",
      "743.989   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 5892/5893 [1:59:18<00:01,  0.82it/s, v_num=0, train_loss_step=5.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 22. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  16%|█▌        | 938/5893 [19:01<1:40:32,  0.82it/s, v_num=0, train_loss_step=5.420, train_loss_epoch=5.330] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011e537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940d299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chexmsn-env\n",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
