{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2e52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import lightly.models.utils as utils\n",
    "from typing import List\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from lightly.utils.benchmarking import BenchmarkModule\n",
    "import torch.distributed as dist\n",
    "from lightly.loss import MSNLoss\n",
    "from lightly.models.modules.heads import MSNProjectionHead\n",
    "from lightly.models.modules.masked_autoencoder import MAEBackbone\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "333cc1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VICRegLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 #beta:float =1.0,\n",
    "                 lamda:float =25.0,\n",
    "                 mu: float = 25.0,\n",
    "                 nu: float = 1.0,\n",
    "                 gamma:float = 1.0,\n",
    "                 epsilon: float = 0.001\n",
    "                ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        #self.beta = beta\n",
    "        self.lamda = lamda\n",
    "        self.mu = mu\n",
    "        self.nu = nu\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        #self.msn_loss = MSNLoss(regularization_weight=0)\n",
    "    \n",
    "    def _invar_loss(self,\n",
    "                  Z1: torch.Tensor,\n",
    "                  Z2: torch.Tensor,\n",
    "                  ) -> torch.Tensor:\n",
    "        Z2 = Z2.repeat((11,1))\n",
    "        invar_loss = F.mse_loss(Z1,Z2)\n",
    "\n",
    "        return invar_loss\n",
    "\n",
    "\n",
    "    def _var_loss(self,\n",
    "                Z1: torch.Tensor,\n",
    "                Z2: torch.Tensor,\n",
    "               ) -> torch.Tensor:\n",
    "        Z2 = Z2.repeat((11,1))\n",
    "        std_Z_1 = torch.sqrt(Z1.var(dim=0) + self.epsilon)\n",
    "        std_Z_2 = torch.sqrt(Z2.var(dim=0) + self.epsilon)\n",
    "        var_loss = torch.mean(torch.relu(self.gamma - std_Z_1)) + \\\n",
    "                   torch.mean(torch.relu(self.gamma - std_Z_2))\n",
    "\n",
    "        return var_loss\n",
    "\n",
    "\n",
    "    def _covar_loss(self,\n",
    "                  Z1: torch.Tensor,\n",
    "                  Z2: torch.Tensor\n",
    "                  ) -> torch.Tensor:\n",
    "        Z2 = Z2.repeat((11,1))\n",
    "        n, d = Z1.shape\n",
    "        Z1 = Z1-Z1.mean(dim=0)\n",
    "        Z2 = Z2-Z2.mean(dim=0)\n",
    "        cov_Z1 = torch.mm(Z1.T,Z1)/(n-1)\n",
    "        cov_Z2 = torch.mm(Z2.T,Z2)/(n-1)\n",
    "        covar_Z1_loss = (cov_Z1.sum() - cov_Z1.diagonal().sum()).pow(2) / d\n",
    "        covar_Z2_loss = (cov_Z2.sum() - cov_Z2.diagonal().sum()).pow(2) / d\n",
    "        covar_loss = covar_Z1_loss + covar_Z2_loss\n",
    "        return covar_loss\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "              Z1: torch.Tensor,\n",
    "              Z2: torch.Tensor,\n",
    "              Z3:torch.Tensor\n",
    "              ) -> torch.Tensor:\n",
    "        #msn_loss = self.beta * self.msn_loss(Z1,Z2,Z3)\n",
    "        var_loss = self.mu * self._var_loss(Z1,Z2)\n",
    "        invar_loss = self.lamda * self._invar_loss(Z1,Z2)\n",
    "        covar_loss = self.nu * self._covar_loss(Z1,Z2)\n",
    "\n",
    "        return invar_loss,var_loss,covar_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65a76e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSN1(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ViT small configuration (ViT-S/16)\n",
    "        self.mask_ratio = 0.15\n",
    "        # self.backbone = MAEBackbone(\n",
    "        #     image_size=224,\n",
    "        #     patch_size=16,\n",
    "        #     num_layers=12,\n",
    "        #     num_heads=6,\n",
    "        #     hidden_dim=384,\n",
    "        #     mlp_dim=384 * 4,\n",
    "        # )\n",
    "        # # or use a torchvision ViT backbone:\n",
    "        vit = torchvision.models.VisionTransformer(image_size=224,patch_size=16,num_layers=12,num_heads=6,hidden_dim=192,mlp_dim=192*4)\n",
    "        self.backbone = MAEBackbone.from_vit(vit)\n",
    "        self.projection_head = MSNProjectionHead(192,768,192)\n",
    "\n",
    "        self.anchor_backbone = copy.deepcopy(self.backbone)\n",
    "        self.anchor_projection_head = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone)\n",
    "        utils.deactivate_requires_grad(self.projection_head)\n",
    "\n",
    "        self.prototypes = nn.Linear(192, 1024, bias=False).weight\n",
    "        self.criterion = VICRegLoss(lamda=2)#VICRegLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.anchor_backbone, self.backbone, 0.996)\n",
    "        utils.update_momentum(self.anchor_projection_head, self.projection_head, 0.996)\n",
    "\n",
    "        views1 = batch[0]\n",
    "        views2 = batch[1]\n",
    "        views1 = [view.to(self.device, non_blocking=True) for view in views1]\n",
    "        views2 = [view.to(self.device, non_blocking=True) for view in views2]\n",
    "        targets = views1[0]\n",
    "        anchors = views2[1]\n",
    "        anchors_focal = torch.concat(views2[2:], dim=0)\n",
    "\n",
    "        targets_out = self.backbone(targets)\n",
    "        targets_out = self.projection_head(targets_out)\n",
    "        anchors_out = self.encode_masked(anchors)\n",
    "        anchors_focal_out = self.encode_masked(anchors_focal)\n",
    "        anchors_out = torch.cat([anchors_out, anchors_focal_out], dim=0)\n",
    "\n",
    "        inv,var,cov = self.criterion(anchors_out, targets_out, self.prototypes.data)\n",
    "        #self.log(\"msn_loss\", msn, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        self.log(\"inv_loss\", inv, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        self.log(\"var_loss\", var, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        self.log(\"covar_loss\", cov, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        loss = inv + var + cov \n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def encode_masked(self, anchors):\n",
    "        batch_size, _, _, width = anchors.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, _ = utils.random_token_mask(\n",
    "            size=(batch_size, seq_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=self.device,\n",
    "        )\n",
    "        out = self.anchor_backbone(anchors, idx_keep=idx_keep)\n",
    "        return self.anchor_projection_head(out)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [\n",
    "            *list(self.anchor_backbone.parameters()),\n",
    "            *list(self.anchor_projection_head.parameters()),\n",
    "            self.prototypes,\n",
    "        ]\n",
    "        optim = torch.optim.AdamW(params, lr=0.0001,weight_decay=0.001)\n",
    "        return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6885703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional,Dict,Union\n",
    "from torch.utils.data import Dataset\n",
    "from lightly.transforms.msn_transform import MSNTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d269f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL.Image import Image\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "IMAGENET_STAT = {\"mean\":torch.tensor([0.4884, 0.4550, 0.4171]),\n",
    "                 \"std\":torch.tensor([0.2596, 0.2530, 0.2556])}\n",
    "\n",
    "MIMIC_NORMALIZE ={\"mean\":torch.tensor([0.4723, 0.4723, 0.4723]), \n",
    "                  \"std\":torch.tensor([0.3023, 0.3023, 0.3023])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c59dc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "class MSNTransform(MultiViewTransform):\n",
    "    \"\"\"Implements the transformations for MSN [0].\n",
    "\n",
    "    Input to this transform:\n",
    "        PIL Image or Tensor.\n",
    "\n",
    "    Output of this transform:\n",
    "        List of Tensor of length 2 * random_views + focal_views. (12 by default)\n",
    "\n",
    "    Applies the following augmentations by default:\n",
    "        - Random resized crop\n",
    "        - Random horizontal flip\n",
    "        - ImageNet normalization\n",
    "\n",
    "    Generates a set of random and focal views for each input image. The generated output\n",
    "    is (views, target, filenames) where views is list with the following entries:\n",
    "    [random_views_0, random_views_1, ..., focal_views_0, focal_views_1, ...].\n",
    "\n",
    "    - [0]: Masked Siamese Networks, 2022: https://arxiv.org/abs/2204.07141\n",
    "\n",
    "    Attributes:\n",
    "        random_size:\n",
    "            Size of the random image views in pixels.\n",
    "        focal_size:\n",
    "            Size of the focal image views in pixels.\n",
    "        random_views:\n",
    "            Number of random views to generate.\n",
    "        focal_views:\n",
    "            Number of focal views to generate.\n",
    "        random_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops for the relative to random_size.\n",
    "        focal_crop_scale:\n",
    "            Minimum and maximum size of the randomized crops relative to focal_size.\n",
    "        hf_prob:\n",
    "            Probability that horizontal flip is applied.\n",
    "        vf_prob:\n",
    "            Probability that vertical flip is applied.\n",
    "        normalize:\n",
    "            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_size: int = 224,\n",
    "        focal_size: int = 96,\n",
    "        random_views: int = 2,\n",
    "        focal_views: int = 10,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        random_crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        focal_crop_scale: Tuple[float, float] = (0.05, 0.3),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.0,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_STAT,\n",
    "    ):\n",
    "        random_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=random_size,\n",
    "            crop_scale=random_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        focal_view_transform = MSNViewTransform(\n",
    "            affine_dgrees=affine_dgrees,\n",
    "            affine_scale=affine_scale,\n",
    "            affine_shear=affine_shear,\n",
    "            affine_translate=affine_translate,\n",
    "            crop_size=focal_size,\n",
    "            crop_scale=focal_crop_scale,\n",
    "            hf_prob=hf_prob,\n",
    "            vf_prob=vf_prob,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        transforms = [random_view_transform] * random_views\n",
    "        transforms += [focal_view_transform] * focal_views\n",
    "        super().__init__(transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "312397f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNViewTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        affine_dgrees: int = 15,\n",
    "        affine_scale: Tuple[float,float]= (.9, 1.1),\n",
    "        affine_shear: int = 0,\n",
    "        affine_translate: Tuple[float,float] = (0.1, 0.1),\n",
    "        crop_size: int = 224,\n",
    "        crop_scale: Tuple[float, float] = (0.3, 1.0),\n",
    "        hf_prob: float = 0.5,\n",
    "        vf_prob: float = 0.0,\n",
    "        normalize: Dict[str, List[float]] = IMAGENET_STAT,\n",
    "    ):\n",
    "\n",
    "        transform = [\n",
    "            T.RandomAffine(degrees=affine_dgrees, \n",
    "                          scale=affine_scale, \n",
    "                          shear=affine_shear, \n",
    "                          translate=affine_translate),\n",
    "            T.RandomResizedCrop(size=crop_size, scale=crop_scale),\n",
    "            T.RandomHorizontalFlip(p=hf_prob),\n",
    "            T.RandomVerticalFlip(p=vf_prob),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=normalize[\"mean\"], std=normalize[\"std\"]),\n",
    "        ]\n",
    "\n",
    "        self.transform = T.Compose(transform)\n",
    "\n",
    "    def __call__(self, image: Union[torch.Tensor, Image]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies the transforms to the input image.\n",
    "\n",
    "        Args:\n",
    "            image:\n",
    "                The input image to apply the transforms to.\n",
    "\n",
    "        Returns:\n",
    "            The transformed image.\n",
    "\n",
    "        \"\"\"\n",
    "        transformed = self.transform(image)\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a08be9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import Tuple, Optional\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bfd14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexMSNDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str,\n",
    "                 transforms: nn.Module,\n",
    "                 same = True\n",
    "                 ) -> None:\n",
    "      \n",
    "        self.meta = pd.read_csv(data_dir)\n",
    "        self.all_images = list(self.meta.path)\n",
    "        self.transforms = transforms\n",
    "        self.same = same\n",
    "        \n",
    "    def __len__(self\n",
    "                ) -> int:\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self,\n",
    "                    index: int\n",
    "                    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        \n",
    "        target_path = self.all_images[index]\n",
    "        image_id = target_path.split('/')[-1][:-4]\n",
    "        img_gender_path = self._retrieve_anchors(image_id=image_id,\n",
    "                                              meta = self.meta,\n",
    "                                              same=self.same)\n",
    "\n",
    "        img_target = Image.open(fp=target_path).convert('RGB')\n",
    "        img_target = self.transforms(img_target)\n",
    "        \n",
    "#         img_age = Image.open(fp=img_age_path).convert('RGB')\n",
    "#         img_age = self.transforms(img_age)\n",
    "\n",
    "        img_gender = Image.open(fp=img_gender_path).convert('RGB')\n",
    "        img_gender = self.transforms(img_gender)\n",
    "\n",
    "        return (img_target,img_gender)#,img_gender)\n",
    "    \n",
    "    \n",
    "    def _retrieve_anchors(self,\n",
    "                          image_id: str,\n",
    "                          meta: pd.DataFrame,\n",
    "                          same: bool = False) -> Tuple[str]:\n",
    "        record = meta[meta.dicom_id == image_id]\n",
    "    \n",
    "        subject_id = list(record.subject_id)[0]\n",
    "        age_groub =list(record.ageR10)[0] \n",
    "        gender = list(record.gender)[0]\n",
    "    \n",
    "        group = meta[meta.ageR10 == age_groub]\n",
    "    \n",
    "        if same:\n",
    "            candidate_anchors = group[group.gender == gender]\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.subject_id != subject_id]\n",
    "            images= list(candidate_anchors.path)\n",
    "            sampled_images = random.sample(images,k=2)\n",
    "            image_age, image_gender = sampled_images[0],sampled_images[1]\n",
    "            image_age = sampled_images[0]\n",
    "            return image_gender #,image_age\n",
    "        else:\n",
    "            candidate_anchors = group\n",
    "            candidate_anchors = candidate_anchors[candidate_anchors.subject_id != subject_id]\n",
    "            images= list(candidate_anchors.path)\n",
    "            image_age = random.sample(images,k=1)[0]\n",
    "            #candidate_anchors = candidate_anchors[candidate_anchors.gender == gender]\n",
    "            #images= list(candidate_anchors.path)\n",
    "            #image_gender = random.sample(images,k=1)[0]\n",
    "            return image_age#, image_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2353b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61290df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52eefbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/meta.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b482bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChexMSNDataset(data_dir,transforms=MSNTransform(focal_views=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38ae21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e210f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(next(iter(dataloader))[0][0][0][0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a10f98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(next(iter(dataloader))[1][0][0][0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132ec8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4c91b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MSN1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64ccfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/sas10092/.conda/envs/chexmsn-env/lib/python3.9 ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=100,\n",
    "                     log_every_n_steps=1,\n",
    "                     precision='16-mixed'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b0432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type              | Params\n",
      "-------------------------------------------------------------\n",
      "0 | backbone               | MAEBackbone       | 5.7 M \n",
      "1 | projection_head        | MSNProjectionHead | 888 K \n",
      "2 | anchor_backbone        | MAEBackbone       | 5.7 M \n",
      "3 | anchor_projection_head | MSNProjectionHead | 888 K \n",
      "4 | criterion              | VICRegLoss        | 0     \n",
      "  | other params           | n/a               | 196 K \n",
      "-------------------------------------------------------------\n",
      "6.8 M     Trainable params\n",
      "6.6 M     Non-trainable params\n",
      "13.4 M    Total params\n",
      "53.630    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  81%|████████▏ | 4791/5879 [51:49<11:46,  1.54it/s, v_num=13, inv_loss_step=3.600, var_loss_step=3.460, covar_loss_step=1.490, train_loss_step=8.550]   "
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011e537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940d299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f244d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0635c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chexmsn-env\n",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
