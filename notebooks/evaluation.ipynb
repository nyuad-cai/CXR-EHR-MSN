{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527e7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from lightly.models import utils\n",
    "from typing import Optional, List,Tuple, Dict\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torchvision.models.vision_transformer import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6aa134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_new(df):\n",
    "    yt =np.array([np.array(x) for x in df['y_truth'].values])\n",
    "    yp =np.array([np.array(x) for x in df['y_pred'].values])\n",
    "    auroc = roc_auc_score(yt, yp)\n",
    "    auprc = average_precision_score(yt, yp)\n",
    "    return auprc, auroc\n",
    "\n",
    "def bootstraping_eval(df, num_iter):\n",
    "    \"\"\"This function samples from the testing dataset to generate a list of performance metrics using bootstraping method\"\"\"\n",
    "    auroc_list = []\n",
    "    auprc_list = []\n",
    "    for _ in range(num_iter):\n",
    "        sample = df.sample(frac=1, replace=True)\n",
    "        auprc, auroc = evaluate_new(sample)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "    return auprc_list, auroc_list\n",
    "\n",
    "def computing_confidence_intervals(list_,true_value):\n",
    "\n",
    "    \"\"\"This function calcualts the 95% Confidence Intervals\"\"\"\n",
    "    delta = (true_value - list_)\n",
    "    list(np.sort(delta))\n",
    "    delta_lower = np.percentile(delta, 97.5)\n",
    "    delta_upper = np.percentile(delta, 2.5)\n",
    "\n",
    "    upper = true_value - delta_upper\n",
    "    lower = true_value - delta_lower\n",
    "    return (upper,lower)\n",
    "\n",
    "def get_model_performance(df):\n",
    "    test_auprc, test_auroc = evaluate_new(df)\n",
    "    auprc_list, auroc_list = bootstraping_eval(df, num_iter=10)\n",
    "    upper_auprc, lower_auprc = computing_confidence_intervals(auprc_list, test_auprc)\n",
    "    upper_auroc, lower_auroc = computing_confidence_intervals(auroc_list, test_auroc)\n",
    "    print(\"\\n--------------\")\n",
    "    text_a=str(f\"AUROC {round(test_auroc, 3)} ({round(lower_auroc, 3)}, {round(upper_auroc, 3)}) CI 95%\")\n",
    "    text_b=str(f\"AUPRC {round(test_auprc, 3)} ({round(lower_auprc, 3)}, {round(upper_auprc, 3)}) CI 95% \")\n",
    "    print(text_a,'\\n')\n",
    "    print(text_b,'\\n')\n",
    "    summary = {'test_auroc':np.round(test_auroc,3),\n",
    "               'lower_auroc':np.round(lower_auroc,3),\n",
    "               'upper_auroc':np.round(upper_auroc,3),\n",
    "               'test_auprc':np.round(test_auprc,3),\n",
    "               'lower_auprc':np.round(lower_auprc,3),\n",
    "               'upper_auprc':np.round(upper_auprc,3),\n",
    "               'auroc_text':text_a,\n",
    "               'auprc_text':text_b}\n",
    "    \n",
    "#     final = pd.DataFrame(summary,index=[0])\n",
    "#     final.to_csv(os.path.join(summary_path,'summary.csv'),index=False)\n",
    "    #return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c7e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                backbone:nn.Module,\n",
    "                learning_rate: float =  1e-3,\n",
    "                weight_decay: float = 0.0,\n",
    "                output_dim: int = 14,\n",
    "                freeze: int = 0,\n",
    "                max_epochs: int = 50,\n",
    "                mask_ratio: float = 0.15,\n",
    "                scheduler: str = 'cosine',\n",
    "                summary_path: int  = './models'\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # self.save_hyperparameters() \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.output_dim = output_dim\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.summary_path = summary_path\n",
    "        \n",
    "        self.train_step_preds = []\n",
    "        self.train_step_label = []\n",
    "        \n",
    "        self.val_step_preds = []\n",
    "        self.val_step_label = []\n",
    "        \n",
    "        self.test_step_preds = []\n",
    "        self.test_step_label = []\n",
    "        \n",
    "     \n",
    "        if freeze == 1:\n",
    "            utils.deactivate_requires_grad(self.backbone)\n",
    "        elif freeze == 0:\n",
    "            utils.activate_requires_grad(self.backbone)\n",
    "               \n",
    "    def forward(self,\n",
    "                x: Tensor\n",
    "               ) -> Tensor:\n",
    "        x = self.backbone(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "    def training_step(self, \n",
    "                      batch: List[Tensor], \n",
    "                      batch_idx: int\n",
    "                     ) -> float:\n",
    "        \n",
    "        input, label = batch\n",
    "        prediction = self.forward(input)\n",
    "        \n",
    "        self.train_step_label.append(label)\n",
    "        self.train_step_preds.append(prediction)\n",
    "\n",
    "        loss = nn.BCELoss()(prediction, label)            \n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=False , logger=True, prog_bar=True)\n",
    "        \n",
    "        return {'loss':loss,\n",
    "                'pred':prediction,\n",
    "                'label':label}\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "\n",
    "\n",
    "        \n",
    "        y = torch.cat(self.train_step_label).detach().cpu()\n",
    "        pred = torch.cat(self.train_step_preds).detach().cpu()\n",
    "\n",
    "        auroc = np.round(roc_auc_score(y, pred), 4)\n",
    "        auprc = np.round(average_precision_score(y, pred), 4)   \n",
    "        self.log('train_auroc',auroc, on_epoch=True, on_step=False,logger=True, prog_bar=True)\n",
    "        self.log('train_auprc',auprc, on_epoch=True, on_step=False,logger=True, prog_bar=True)      \n",
    "        self.train_step_label.clear()\n",
    "        self.train_step_preds.clear()\n",
    "        \n",
    "    def validation_step (self, \n",
    "                      batch: List[Tensor], \n",
    "                      batch_idx: int\n",
    "                     ) -> float:\n",
    "        \n",
    "        input,label = batch\n",
    "        prediction = self.forward(input) \n",
    "        \n",
    "        self.val_step_label.append(label)\n",
    "        self.val_step_preds.append(prediction)\n",
    "\n",
    "        loss = self._bce_loss(prediction, label,mode='val')       \n",
    "        self.log(\"val_loss\", loss, on_epoch= True,on_step=False,logger=True, prog_bar=True)\n",
    "\n",
    "        return {'loss':loss,\n",
    "                'pred':prediction,\n",
    "                'label':label}\n",
    "\n",
    "    def on_validation_epoch_end(self,*arg, **kwargs) -> None:\n",
    "        \n",
    "        y = torch.cat(self.val_step_label).detach().cpu()\n",
    "        pred = torch.cat(self.val_step_preds).detach().cpu()\n",
    "\n",
    "        auroc = np.round(roc_auc_score(y, pred), 4)\n",
    "        auprc = np.round(average_precision_score(y, pred), 4)   \n",
    "        self.log('val_auroc',auroc, on_epoch=True, on_step=False, logger=True, prog_bar=True)\n",
    "        self.log('val_auprc',auprc, on_epoch=True, on_step=False, logger=True, prog_bar=True)    \n",
    "        self.val_step_label.clear()\n",
    "        self.val_step_preds.clear()\n",
    "        \n",
    "    def test_step(self, \n",
    "                  batch: List[Tensor], \n",
    "                  batch_idx: int\n",
    "                 ) -> float:\n",
    "        input, label = batch\n",
    "        prediction = self.forward(input)\n",
    "        \n",
    "        self.test_step_label.append(label)\n",
    "        self.test_step_preds.append(prediction)\n",
    "        loss = self._bce_loss(prediction, label,mode='test')\n",
    "        self.log(\"test_loss\", loss, on_epoch= True,on_step=False , logger=True, prog_bar=True)\n",
    "\n",
    "        \n",
    "        return {'loss':loss,\n",
    "                'pred':prediction,\n",
    "                'label':label}\n",
    "\n",
    "    def on_test_epoch_end(self,*arg, **kwargs) -> None:\n",
    "        y = torch.cat(self.test_step_label).detach().cpu()\n",
    "        pred = torch.cat(self.test_step_preds).detach().cpu()\n",
    "\n",
    "\n",
    "        auroc = np.round(roc_auc_score(y, pred), 4)\n",
    "        auprc = np.round(average_precision_score(y, pred), 4)   \n",
    "        self.log('test_auroc',auroc, on_epoch=True, on_step=False, logger=True)\n",
    "        self.log('test_auprc',auprc, on_epoch=True, on_step=False, logger=True) \n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df['y_truth'] = y.tolist()\n",
    "        df['y_pred'] = pred.tolist()\n",
    "        get_model_performance(df)\n",
    "        \n",
    "        \n",
    "        self.test_step_label.clear()\n",
    "        self.test_step_preds.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = optim.Adam(params=self.parameters(), \n",
    "                                   lr=self.learning_rate, \n",
    "                                   weight_decay=self.weight_decay\n",
    "                                   )\n",
    "        if self.scheduler == 'cosine':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                         eta_min=0,\n",
    "                                                         T_max=self.max_epochs\n",
    "                                                         )\n",
    "            return {'optimizer': optimizer,\n",
    "                    'lr_scheduler': scheduler,\n",
    "                   }\n",
    "        \n",
    "        elif self.scheduler == 'reduce':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                         mode='max',\n",
    "                                                         factor=0.5,\n",
    "                                                         patience=3,\n",
    "                                                         )   \n",
    "            return {'optimizer': optimizer,\n",
    "                    'lr_scheduler': scheduler,\n",
    "                    'monitor': \"val_auroc\"\n",
    "                   }\n",
    "    \n",
    "\n",
    "    \n",
    "    def _bce_loss(self, preds, y,mode='train'):\n",
    "        loss = nn.BCELoss()(preds, y)\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.detach().cpu().numpy()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225f3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_dir:str,\n",
    "               paths: list,\n",
    "               split:str,\n",
    "               ) -> Tuple[List]:\n",
    "    CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "                'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "                'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "                'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "    filenames_to_path = {path.split('/')[-1].split('.')[0]: path for path in paths}\n",
    "    metadata = pd.read_csv(os.path.join(data_dir,'mimic-cxr-2.0.0-metadata.csv'))\n",
    "    labels = pd.read_csv(os.path.join(data_dir,'mimic-cxr-2.0.0-chexpert.csv'))\n",
    "    labels[CLASSES] = labels[CLASSES].fillna(0)\n",
    "    labels = labels.replace(-1.0, 0.0)\n",
    "    splits = pd.read_csv(os.path.join(data_dir,'mimic-cxr-ehr-split.csv'))\n",
    "    metadata_with_labels = metadata.merge(labels[CLASSES+['study_id'] ], how='inner', on='study_id')\n",
    "    filesnames_to_labels = dict(zip(metadata_with_labels['dicom_id'].values, metadata_with_labels[CLASSES].values))\n",
    "    filenames_loaded = splits.loc[splits.split==split]['dicom_id'].values\n",
    "    \n",
    "    filenames_loaded = [filename  for filename in filenames_loaded if filename in filesnames_to_labels]\n",
    "\n",
    "    return filenames_to_path, filenames_loaded, filesnames_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b462b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_STAT = {\"mean\":torch.tensor([0.4884, 0.4550, 0.4171]),\n",
    "                 \"std\":torch.tensor([0.2596, 0.2530, 0.2556])}\n",
    "train_transforms = T.Compose([T.Resize(256),\n",
    "                              T.RandomHorizontalFlip(),\n",
    "                              T.RandomAffine(degrees=45, scale=(.85, 1.15), shear=0, translate=(0.15, 0.15)),\n",
    "                              T.CenterCrop(224),\n",
    "                              T.ToTensor(),\n",
    "                              T.Normalize(mean=IMAGENET_STAT[\"mean\"],\n",
    "                                          std=IMAGENET_STAT[\"std\"])                                                                   \n",
    "                            ])\n",
    "\n",
    "\n",
    "val_test_transforms = T.Compose([T.Resize(256),\n",
    "                                 T.CenterCrop(224),\n",
    "                                 T.ToTensor(),\n",
    "                                 T.Normalize(mean=IMAGENET_STAT[\"mean\"],\n",
    "                                             std=IMAGENET_STAT[\"std\"])                                                                \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0dc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICCXR(Dataset):\n",
    "    def __init__(self, \n",
    "                 paths: str,\n",
    "                 data_dir: str, \n",
    "                 transform: Optional[T.Compose] = None, \n",
    "                 split: str = 'validate',\n",
    "                 percentage:float = 1.0\n",
    "                 ) -> None:\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.filenames_to_path, \\\n",
    "        self.filenames_loaded, \\\n",
    "        self.filesnames_to_labels = preprocess(data_dir=self.data_dir,\n",
    "                                               paths=paths,\n",
    "                                               split=split\n",
    "                                              )\n",
    "        limit = (round(len(self.filenames_loaded) * percentage))\n",
    "        self.filenames_loaded = random.sample(self.filenames_loaded,limit)\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, str):\n",
    "            img = Image.open(self.filenames_to_path[index]).convert('RGB')\n",
    "            labels = torch.tensor(self.filesnames_to_labels[index]).float()\n",
    "\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, labels\n",
    "        \n",
    "        filename = self.filenames_loaded[index]\n",
    "        img = Image.open(self.filenames_to_path[filename]).convert('RGB')\n",
    "        labels = torch.tensor(self.filesnames_to_labels[filename]).float()\n",
    "        \n",
    "            \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199f3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpert(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_path: str,\n",
    "                 transform: Optional[T.Compose] = None, \n",
    "                 split: str = 'test',\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.data = pd.read_csv(data_path)\n",
    "\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.data['Path'][index]).convert('RGB')\n",
    "        labels = self.data.iloc[index,1:].to_numpy().astype('float32')\n",
    "        img = self.transform(img)\n",
    "        return img, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba43ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIHDataset(Dataset):\n",
    "    def __init__(self, root, data_path, transform=None):\n",
    "        self.root = root\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "        file = open(self.root)\n",
    "        images = file.read().splitlines()\n",
    "        \n",
    "        ids = []\n",
    "        \n",
    "        for idx, path in enumerate(self.df['Image']):\n",
    "            if path.split('/')[-1] in images:\n",
    "                ids.append(idx)\n",
    "        \n",
    "        self.df = self.df.iloc[ids, :].reset_index(drop=True)\n",
    "        self.images = self.df['Image'].values\n",
    "        self.labels = self.df.iloc[:, 1:].values\n",
    "        labels = list(map(lambda x: x.lower(), self.df.columns[1:]))\n",
    "        self.classes = {v: k for k, v in enumerate(labels)}\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images[item]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, torch.tensor(self.labels[item], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b97725",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_data_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "1ebdd8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(os.path.join(mimic_data_dir,'resized','**','*.jpg'), recursive=True)\n",
    "\n",
    "mimic_train_dataset = MIMICCXR(paths=paths,\n",
    "                               data_dir= mimic_data_dir, \n",
    "                               split='train', \n",
    "                               transform=train_transforms,\n",
    "                               percentage=1.0\n",
    "                              )\n",
    "mimic_val_dataset = MIMICCXR(paths=paths,\n",
    "                             data_dir=mimic_data_dir, \n",
    "                             split='validate', \n",
    "                             transform=val_test_transforms,\n",
    "\n",
    "                             )\n",
    "mimic_test_dataset = MIMICCXR(paths=paths,\n",
    "                              data_dir=mimic_data_dir, \n",
    "                              split='test', \n",
    "                              transform=val_test_transforms,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ddb97fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_train_dataloader = DataLoader(dataset=mimic_train_dataset,\n",
    "                                    batch_size=64,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=24,\n",
    "                                    pin_memory=True,\n",
    "                                    drop_last=True\n",
    "                                  )\n",
    "mimic_val_dataloader = DataLoader(dataset=mimic_val_dataset,\n",
    "                                  batch_size=64,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=24,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "mimic_test_dataloader = DataLoader(dataset=mimic_test_dataset,\n",
    "                                   batch_size=64,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=24,\n",
    "                                   pin_memory=True,\n",
    "                                   drop_last=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af015ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_data_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6f473b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert = CheXpert(data_path=chexpert_data_dir,\n",
    "                    transform=val_test_transforms,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "d71a7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_dataloader = DataLoader(dataset=chexpert,\n",
    "                             batch_size=688,\n",
    "                             shuffle=False,\n",
    "                             num_workers=24,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=False\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b1c9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_data_root = ''\n",
    "nih_data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffdb9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_train_dataset = NIHDataset(root=nih_data_root,\n",
    "                               data_path=nih_data_path,\n",
    "                               transform=train_transforms\n",
    "                              )\n",
    "nih_val_dataset = NIHDataset(root=nih_data_root,\n",
    "                             data_path=nih_data_path,\n",
    "                             transform=val_test_transforms\n",
    "                            )\n",
    "nih_test_dataset = NIHDataset(root=nih_data_root,\n",
    "                              data_path=nih_data_path,\n",
    "                              transform=val_test_transforms\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d36826c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nih_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c4b3217",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = VisionTransformer(image_size=224,\n",
    "                             patch_size=16,\n",
    "                             num_layers=12,\n",
    "                             num_heads=6,\n",
    "                             hidden_dim=192,\n",
    "                             mlp_dim=192*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "7da7ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = ''\n",
    "all_weights = torch.load(checkpoint_dir,map_location='cpu')['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "faf4ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weights(weights: Dict[str,torch.Tensor]) -> Dict[str,torch.Tensor]:\n",
    "    \n",
    "    for k in list(weights.keys()):\n",
    "\n",
    "        if k.startswith('backbone.'):\n",
    "            \n",
    "            if k.startswith('backbone.') and not k.startswith('backbone.heads'):\n",
    "                \n",
    "                weights[k[len(\"backbone.\"):]] = weights[k]\n",
    "                \n",
    "        del weights[k]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "d7949e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = parse_weights(all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1196ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.load_state_dict(weight,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c486f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvaluationModel(backbone=backbone,\n",
    "                        learning_rate=0.0001,\n",
    "                        freeze=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26d2da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.heads.head = nn.Linear(in_features=192,\n",
    "                                      out_features=model.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf1a56e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/sas10092/.conda/envs/chexmsn-env/lib/python3.9 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sas10092/.conda/envs/chexmsn-env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=50,\n",
    "                     num_sanity_val_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_dataloader,val_dataloaders=val_dataloader)\n",
    "trainer.test(model=model,\n",
    "             dataloaders=chexpert_dataloader,\n",
    "             ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220442b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chexmsn-env\n",
   "language": "python",
   "name": "chexmsn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
